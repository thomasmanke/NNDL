[
  {
    "objectID": "help/03_Pytorch.html",
    "href": "help/03_Pytorch.html",
    "title": "Pytorch Starter",
    "section": "",
    "text": "…using conda environments (also mamba/micromamba)\n# first define dedicated environment\nconda create -n pytorch_env python=3.10\nconda activate pytorch_env\n# from pytorch channel\nconda install pytorch torchvision torchaudio -c pytorch\n… or using pip\npip install pytorch"
  },
  {
    "objectID": "help/03_Pytorch.html#install",
    "href": "help/03_Pytorch.html#install",
    "title": "Pytorch Starter",
    "section": "",
    "text": "…using conda environments (also mamba/micromamba)\n# first define dedicated environment\nconda create -n pytorch_env python=3.10\nconda activate pytorch_env\n# from pytorch channel\nconda install pytorch torchvision torchaudio -c pytorch\n… or using pip\npip install pytorch"
  },
  {
    "objectID": "help/03_Pytorch.html#import",
    "href": "help/03_Pytorch.html#import",
    "title": "Pytorch Starter",
    "section": "Import",
    "text": "Import\n\n\nCode\nimport torch\nprint(f'torch version: {torch.__version__}')\n\n# new functionality/vocabulary (nouns and verbs): dir(torch) \n\n\ntorch version: 2.7.1"
  },
  {
    "objectID": "help/03_Pytorch.html#tensors",
    "href": "help/03_Pytorch.html#tensors",
    "title": "Pytorch Starter",
    "section": "Tensors",
    "text": "Tensors\n\nGeneration\n\n\nCode\nX = [[0,1,2], [3,4,5]] # list\nXt = torch.tensor(X)   # tensor\n\nprint(f'type(X): {type(X)}')\nprint(f'type(Xt): {type(Xt)}')\n\n#help(torch.tensor) # first aid\n#print(\"Xt address:\", Xt.data_ptr()) \n\n\ntype(X): &lt;class 'list'&gt;\ntype(Xt): &lt;class 'torch.Tensor'&gt;\n\n\n\n\n\n\n\n\nCreate from numpy\n\n\n\nnumpy array refer to contiguous data buffers in memory. This allows for efficient data sharing with torch tensors.\n\n\nCode\nimport numpy as np\nimport torch\n\nXn = np.array( [0,1,2] )    # numpy array\nXt = torch.tensor(Xn)       # tensor with copy of data\nXs = torch.from_numpy(Xn)   # tensor with shared data\n\n# different Python objects (wrappers to data) have different addresses\nprint(\"Xn id:\", id(Xn)) \nprint(\"Xs id:\", id(Xs))\nprint(\"Xt id:\", id(Xt))\n\n# data buffers address: points to first element of contiguous data\nprint(\"Xn address:\", Xn.ctypes.data) \nprint(\"Xs address:\", Xs.data_ptr()) \nprint(\"Xt address:\", Xt.data_ptr()) \n\n# Example\nXn[1]=42\nprint(\"Xn: \", Xn) \nprint(\"Xs:\", Xs) \nprint(\"Xt:\", Xt)\n\n\n\n\n\n\nData Type and Type conversion\n\n\nCode\nprint(f'Xt.dtype): {Xt.dtype}') # single data type for elements of Xt\nYt = Xt.to(dtype=torch.float32) # shortcut: Xt.float()\nprint(f'Yt.dtype): {Yt.dtype}') \n\n\nXt.dtype): torch.int64\nYt.dtype): torch.float32\n\n\n\n\nAccess\n\n\nCode\nprint(Xt)\nprint(Xt[:,2]) # simple index, e.g. column=2\nind_select = torch.tensor([[1],[0]])      # complex index (along)\ntorch.gather(Xt, dim=1, index=ind_select) # pick [0,1] and [1,0]\n\n\ntensor([[0, 1, 2],\n        [3, 4, 5]])\ntensor([2, 5])\n\n\ntensor([[1],\n        [3]])\n\n\n\n\nAttributes\n\n\nCode\nprint(f'Xt.shape: {Xt.shape}')\nprint(f'Xt.dtype: {Xt.dtype}')\nprint(f'Xt.device: {Xt.device}')\n\n\nXt.shape: torch.Size([2, 3])\nXt.dtype: torch.int64\nXt.device: cpu\n\n\n\n\nMethods\n\n\nCode\nprint(f'Xt.numpy: {Xt.numpy()}') # converting\nprint(f'Xt.sum: {Xt.sum(dim=0)}') # summarizing along dimensions\nprint(f'Xt.argmax: {Xt.argmax(dim=1)}') # indices with largest value\n\n# many more attributes and methods: dir(Xt)\n\n\nXt.numpy: [[0 1 2]\n [3 4 5]]\nXt.sum: tensor([3, 5, 7])\nXt.argmax: tensor([2, 2])\n\n\n\n\nGeneration by shape\n\n\nCode\nshape=(3,4) # rows, columns\nprint('zeros: ', torch.zeros(shape))\nprint('rand: ', torch.rand(shape))\nprint('randn: ', 1 + 0.01*torch.randn(shape))\n\n\nzeros:  tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\nrand:  tensor([[0.1436, 0.3273, 0.8961, 0.0874],\n        [0.2248, 0.6668, 0.1766, 0.7784],\n        [0.5971, 0.5654, 0.6214, 0.2057]])\nrandn:  tensor([[1.0138, 1.0015, 1.0110, 0.9944],\n        [1.0080, 1.0082, 0.9991, 1.0032],\n        [1.0037, 1.0031, 0.9889, 1.0096]])\n\n\n\n\nReshaping\n\n\nCode\nXt = torch.tensor([[0,1,2], [3,4,5]] )\n\n# a special reshape = flatten\nprint(f'Xt.reshape(6): {Xt.reshape(6)}') \nprint(f'Xt.flatten: {Xt.flatten(start_dim=0)}') \n\n# more complicated reshapes\nYt = Xt.reshape(3,2)\nprint(f'Xt: {Xt}') \nprint(f'Yt: {Yt}') # \n\n\n# Warning: reshape() may not create a copy of Xt\n# Yt shares the same storage with Xt !!\nYt[0,0] = 42\nprint(f'Yt: {Yt}') #\nprint(f'Xt: {Xt}') # \n\n# to copy explicitly use clone()\n#Yt = Xt.clone().reshape(3, 2)\n\n\nXt.reshape(6): tensor([0, 1, 2, 3, 4, 5])\nXt.flatten: tensor([0, 1, 2, 3, 4, 5])\nXt: tensor([[0, 1, 2],\n        [3, 4, 5]])\nYt: tensor([[0, 1],\n        [2, 3],\n        [4, 5]])\nYt: tensor([[42,  1],\n        [ 2,  3],\n        [ 4,  5]])\nXt: tensor([[42,  1,  2],\n        [ 3,  4,  5]])\n\n\n\n\nPermuting\n\n\nCode\nXt = torch.arange(6).reshape(1,2,3) # 3D tensor with first direction 1-D\nprint(f'Xt: {Xt}')  \n\nprint(f'Xt: {Xt.shape}') # \nprint(f'Xt.squeeze: {Xt.squeeze().shape}') # squeezing\nprint(f'Xt.permute: {Xt.permute(2,0,1).shape}') # permuting\n#print(f'Xt.permute: {Xt.permute(2,0,1)}')\n\n\nXt: tensor([[[0, 1, 2],\n         [3, 4, 5]]])\nXt: torch.Size([1, 2, 3])\nXt.squeeze: torch.Size([2, 3])\nXt.permute: torch.Size([3, 1, 2])\n\n\n\n\nCombining\n\n\nCode\nXt = torch.tensor([[0,1,2], [3,4,5]] )\nprint(f'element_wise add {Xt + Xt}') \nprint(f'element_wise mult {Xt * Xt}') \nprint(f'matrix multiply {Xt.T @ Xt}') \n\n\nelement_wise add tensor([[ 0,  2,  4],\n        [ 6,  8, 10]])\nelement_wise mult tensor([[ 0,  1,  4],\n        [ 9, 16, 25]])\nmatrix multiply tensor([[ 9, 12, 15],\n        [12, 17, 22],\n        [15, 22, 29]])"
  },
  {
    "objectID": "help/03_Pytorch.html#devices",
    "href": "help/03_Pytorch.html#devices",
    "title": "Pytorch Starter",
    "section": "Devices",
    "text": "Devices\nPyTorch objects are associated with a device. If possible use GPU for speed.\n@GoogleColab or Kaggle: You may have to change the Runtime/Accelerator\n\n\nCode\ndef detect_device():\n    import torch\n    if torch.backends.mps.is_available():\n        # Mac\n        return \"mps\"\n    elif torch.cuda.is_available():\n        # NVIDIA\n        return \"cuda\"\n    else:\n        return \"cpu\"\n\ndevice = detect_device()\nprint('device: ', device)\n\n\ndevice:  mps\n\n\n\n\nCode\n# Initalize on device\nx = torch.tensor(\n    [3.0, 6.0, 9.0],\n    dtype=torch.float32,\n    device=device)\n\nprint(x)\n# transfer to CPU\nx=x.cpu()\nprint(x)\n\n# transfer to device\nx = x.to(device)\nprint(x)\n\n\ntensor([3., 6., 9.], device='mps:0')\ntensor([3., 6., 9.])\ntensor([3., 6., 9.], device='mps:0')"
  },
  {
    "objectID": "help/03_Pytorch.html#parameters",
    "href": "help/03_Pytorch.html#parameters",
    "title": "Pytorch Starter",
    "section": "Parameters",
    "text": "Parameters\n\n\nCode\nshape=(2,3)\nW = torch.rand(shape, requires_grad=True)\nprint(W.grad_fn)\n\n\nNone"
  },
  {
    "objectID": "help/03_Pytorch.html#autograd",
    "href": "help/03_Pytorch.html#autograd",
    "title": "Pytorch Starter",
    "section": "Autograd",
    "text": "Autograd\n\n\nCode\n# Lead nodes; define by used with\na = torch.tensor(2.0, requires_grad=True)\nb = torch.tensor(3.0, requires_grad=True)\nx = torch.tensor(1.0, requires_grad=True)\n\n# operations in computational graph: nodes z and L\nz = a + 2*b\nL = x*z  # L = x*(a+2b);  dL/dx = z; dL/dz = x\n\n# endows inner nodes (z and L) with gradient functions\nprint(f'z_grad = {z.grad_fn}')\nprint(f'L_grad = {L.grad_fn}')\n\n\nz_grad = &lt;AddBackward0 object at 0x30fa41ff0&gt;\nL_grad = &lt;MulBackward0 object at 0x330ec1a80&gt;\n\n\n\n\nCode\na.grad = b.grad = x.grad = None  # clear old grads if any\nz.retain_grad() # per default pytorch keeps gradients only for leave nodes\nL.backward() # fill gradients with respect to L\n\nprint('x.grad:', x.grad) # dL/dx = z  = a + b = 5\nprint('a.grad:', a.grad) # dL/da = (dL/dz) (dz/da) = x * 1\nprint('b.grad:', b.grad) # dL/db = (dL/dz) (dz/db) = x * 1\nprint('z.grad:', z.grad) # dL/dz = x\n\n\nx.grad: tensor(8.)\na.grad: tensor(1.)\nb.grad: tensor(2.)\nz.grad: tensor(1.)"
  },
  {
    "objectID": "help/03_Pytorch.html#legoblocks-torch.nn-module",
    "href": "help/03_Pytorch.html#legoblocks-torch.nn-module",
    "title": "Pytorch Starter",
    "section": "Legoblocks: torch.nn module",
    "text": "Legoblocks: torch.nn module\n\nSimple Functions\n\n\nCode\nX = torch.tensor([-2.0, 0.0, 0.5, 1.0])\nprint('X           ', X)\nprint('sigmoid(X): ', torch.nn.functional.sigmoid(X)) # sigmoid function\nprint('softmax(X): ', torch.nn.functional.softmax(X, dim=0))   # X --&gt; p, \\sum X = 1\nprint('relu(X):    ', torch.nn.functional.relu(X))    # ReLU: max(0, X)\n\n# similar with classes\nrelu = torch.nn.ReLU() # create instance of relu class \nprint('relu(X):    ', relu(X))\n\n\nX            tensor([-2.0000,  0.0000,  0.5000,  1.0000])\nsigmoid(X):  tensor([0.1192, 0.5000, 0.6225, 0.7311])\nsoftmax(X):  tensor([0.0246, 0.1817, 0.2996, 0.4940])\nrelu(X):     tensor([0.0000, 0.0000, 0.5000, 1.0000])\nrelu(X):     tensor([0.0000, 0.0000, 0.5000, 1.0000])\n\n\n\n\nLinear Layers\n\n\nCode\nD_in = 1\nD_out = 3\n# create D_out x D_in matrix of parameters (randomly initialized)\nlinear_layer = torch.nn.Linear(D_in, D_out)\nprint('weight.shape: ', linear_layer.weight.shape)\nprint('weight matrix:', linear_layer.weight)\n\n# fake data\nX = torch.arange(6, dtype=torch.float32).reshape((-1,D_in))\nprint('X.shape: ', X.shape)\n\n# linear transform: y_hat = f(X) = X @ W.T + bias\ny_hat = linear_layer(X)\n\nprint('After linear transformation = matrix multiplication')\nprint('y_hat.shape: ', y_hat.shape)\n\n\nweight.shape:  torch.Size([3, 1])\nweight matrix: Parameter containing:\ntensor([[-0.7548],\n        [-0.6746],\n        [ 0.7900]], requires_grad=True)\nX.shape:  torch.Size([6, 1])\nAfter linear transformation = matrix multiplication\ny_hat.shape:  torch.Size([6, 3])\n\n\n\n\nEmbeddings\n\n\nCode\nnw = 10 # number of words\nk  = 3  # embedding dimension\n\nembedding_layer = torch.nn.Embedding(nw, k)\nwords_ids = torch.tensor([1, 3, 3, 0, 9]) # integer representation\nwords_vec = embedding_layer(words_ids)    # better representation ?\n\nprint('word vectors: ', words_vec)\n\n\nword vectors:  tensor([[ 0.1987, -1.9730,  0.4795],\n        [-0.6995,  0.9854,  0.5563],\n        [-0.6995,  0.9854,  0.5563],\n        [ 2.3458,  0.8227,  0.9022],\n        [-1.1950,  0.6150,  1.3377]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\n\nDropout Layer\n\n\nCode\nX = torch.ones(1,8)\n\ndropout_layer = torch.nn.Dropout(p=0.3) # set 30% to 0\ndropout_layer.train() # only use during training for robustness\n\nprint(X)\n# illustrate random drops and scaling\nfor i in range(6):\n    X_drop = dropout_layer(X)\n    print(X_drop)\n\n\ntensor([[1., 1., 1., 1., 1., 1., 1., 1.]])\ntensor([[1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286]])\ntensor([[1.4286, 1.4286, 0.0000, 1.4286, 0.0000, 1.4286, 1.4286, 1.4286]])\ntensor([[1.4286, 1.4286, 1.4286, 0.0000, 0.0000, 1.4286, 0.0000, 1.4286]])\ntensor([[0.0000, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 0.0000, 1.4286]])\ntensor([[1.4286, 0.0000, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286]])\ntensor([[0.0000, 1.4286, 0.0000, 1.4286, 1.4286, 0.0000, 1.4286, 0.0000]])\n\n\n\n\nBuilding Larger Models\nWe can define our own models from building blocks. Ultimately this will help to abstract from individual layers, their initialization, normalization, dropouts etc.\n\n\nCode\nimport torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self, n_in, n_out):\n        # inherit stuff from nn.Module\n        super().__init__()\n        # define layers: just some random examples\n        self.linear = nn.Linear(n_in, n_out)\n        self.norm = nn.Dropout(p=0.3)\n\n    # conncect layers defined above\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.norm(x)\n        x = nn.functional.sigmoid(x)\n        return x\n\nmodel = MyModel(10, 1)\nprint(model)\n\n\nMyModel(\n  (linear): Linear(in_features=10, out_features=1, bias=True)\n  (norm): Dropout(p=0.3, inplace=False)\n)"
  },
  {
    "objectID": "help/03_Pytorch.html#optimizer-and-loss-function",
    "href": "help/03_Pytorch.html#optimizer-and-loss-function",
    "title": "Pytorch Starter",
    "section": "Optimizer and Loss Function",
    "text": "Optimizer and Loss Function\n\n\nCode\nimport torch.optim as optim\n\nlearning_rate = 0.01 # hyperparameter\n\noptimizer = optim.Adam(model.parameters(), lr = learning_rate)\n\nloss_function = nn.MSELoss()"
  },
  {
    "objectID": "help/03_Pytorch.html#model.train-vs.-model.eval",
    "href": "help/03_Pytorch.html#model.train-vs.-model.eval",
    "title": "Pytorch Starter",
    "section": "model.train() vs. model.eval()",
    "text": "model.train() vs. model.eval()\ntwo different modes for training and evaluation\n\n\n\n\n\n\n\n\nFeature\nmodel.train() (Training Mode)\nmodel.eval() (Evaluation Mode)\n\n\n\n\nDropout\nRandomly disables neurons (adds noise)\nAll neurons active (no dropout)\n\n\nBatchNorm\nUses current batch stats (mean/var)\nUses running (saved) stats\n\n\nGradient Tracking\nYes\nUse with torch.no_grad() to disable\n\n\nWhen to use\nDuring training loop\nDuring validation/testing/inference\n\n\n\nTypical use:\n\n\nCode\n# train data\nn_samples, m_features = 100, 10\nx = torch.rand((n_samples, m_features))\ny = torch.bernoulli(x[:,0], 0.5).reshape(-1,1)\n\n# Training\nn_epochs = 10\nmodel.train()\nfor epoch in range(n_epochs):\n\n    output = model(x)           # forward path x -&gt; output\n\n    loss = loss_function(output, y) # loss calculation\n\n    optimizer.zero_grad() # reset gradients (do not accumulate)\n    loss.backward()       # backward path L -&gt; x (gradient calc)\n    optimizer.step()      # update parm = parm - learning_rate * grad\n\n    # print loss\n    if(epoch % 20 == 0):\n        print('epoch {}, loss {}'.format(epoch, loss.data))\n\n# Evaluation (Validation or Test or Prediction)\nx_val = torch.rand((n_samples, m_features))\ny_val = torch.bernoulli(x_val[:,0], 0.5).reshape(-1,1)\n\nmodel.eval()\n# disable grad calculation for speed\nwith torch.no_grad():\n    val_output = model(x_val)\n    val_loss = loss_function(val_output, y_val)\n\n\nepoch 0, loss 0.3031926155090332"
  },
  {
    "objectID": "help/03_Pytorch.html#saving-models",
    "href": "help/03_Pytorch.html#saving-models",
    "title": "Pytorch Starter",
    "section": "Saving models",
    "text": "Saving models\nYou may want to save (and share) your final model. But for long-running jobs it may also be good to save occasional checkpoints during training\n\n\nCode\nimport os\nsave_dir = \"checkpoints\"\nos.makedirs(save_dir, exist_ok=True)\n\n# 1. only saves state_dict, need to know model class\ntorch.save(model.state_dict(), os.path.join(save_dir, \"model_dict.pt\"))\nmodel.load_state_dict(torch.load(\"checkpoints/model_dict.pt\"))\n\n# 2. save full model. Warning: may not port across pytorch versions!!! \ntorch.save(model, os.path.join(save_dir, \"model_full.pth\"))\nmodel = torch.load(\"checkpoints/model_full.pth\", weights_only=False)"
  },
  {
    "objectID": "help/03_Pytorch.html#saving-histories",
    "href": "help/03_Pytorch.html#saving-histories",
    "title": "Pytorch Starter",
    "section": "Saving histories",
    "text": "Saving histories\n\n\nCode\n# 1. npz format (most general and robust)\nnp.savez(os.path.join(save_dir, \"loss_history.npz\"),\n         train=train_losses, val=val_losses)\n\ndata = np.load(\"checkpoints/loss_history.npz\")\ntrain_losses = data[\"train\"]\nval_losses = data[\"val\"]\n\n# 2. pickle format (more flexible less portable)\nwith open(os.path.join(save_dir, \"loss_history.pkl\"), \"wb\") as f:\n    pickle.dump({\"train\": train_losses, \"val\": val_losses}, f)\n\nwith open(\"checkpoints/loss_history.pkl\", \"rb\") as f:\n    losses = pickle.load(f)\ntrain_losses = losses[\"train\"]\nval_losses = losses[\"val\"]"
  },
  {
    "objectID": "help/03_Pytorch.html#saving-everything",
    "href": "help/03_Pytorch.html#saving-everything",
    "title": "Pytorch Starter",
    "section": "Saving Everything",
    "text": "Saving Everything\n\n\nCode\ntorch.save({\n    'model_state': model.state_dict(),\n    'optimizer_state': optimizer.state_dict(),\n    'train_losses': train_losses,\n    'val_losses': val_losses,\n}, os.path.join(save_dir, \"checkpoint_all.pt\"))\n\ncheckpoint = torch.load(\"checkpoints/checkpoint_all.pt\")\nmodel.load_state_dict(checkpoint['model_state'])\noptimizer.load_state_dict(checkpoint['optimizer_state'])\ntrain_losses = checkpoint['train_losses']\nval_losses = checkpoint['val_losses']"
  },
  {
    "objectID": "help/03_Pytorch.html#tensorboard",
    "href": "help/03_Pytorch.html#tensorboard",
    "title": "Pytorch Starter",
    "section": "Tensorboard",
    "text": "Tensorboard\ntensorboard is a popular tool to inspect neural networks and training progress\n\n\nCode\nfrom torch.utils.tensorboard import SummaryWriter\n\n# default `log_dir` is \"runs\" - we'll be more specific here\nwriter = SummaryWriter('log_dir/mnist_experiment')\n\n# typical uses\nwriter.add_graph(model)\nfor epoch in range(100):\n    # training loop\n    # ...\n    # train_loss = loss_function(logits, y)\n    writer.add_scalar(\"Loss\", train_loss, epoch)\n\nwriter.close()"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html",
    "href": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html",
    "title": "Loss and Gradients",
    "section": "",
    "text": "Find loss minimum numerically and efficiently.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n# define truth (possibly simple)\ndef y_true_fn(theta, x):\n    y_true = ( theta[0] + \n        theta[1]*np.sin(theta[2] * x + theta[3]) +\n        theta[4]*np.cos(theta[4] * x - theta[6]) +\n        theta[7]*np.sin(theta[8] * x + theta[9])\n    )\n    # complexity of true model is irrelevant for shape of loss surface\n    y_true = theta[0] + theta[1]*x \n\n#   optional: turn into binary\n#    y_true = (y_true &gt; np.median(y_true)).astype(int) \n    return y_true\n\n# define model predictions (complex)\ndef y_pred_fn(w1, w2, x):\n#    y_pred = sigmoid(w1 + w2 * x)\n    y_pred = np.sin(np.pi*x + np.sin(w1*np.pi*x)) * np.cos(w2*x)\n    return y_pred\n\ndef MSEloss(w1, w2):\n    y_pred = y_pred_fn(w1, w2, x[:, None, None])\n    return np.mean((y_true[:, None, None] - y_pred)**2, axis=0)\n\ndef CEloss(w1, w2):\n    y_pred = y_pred_fn(w1, w2, x[:, None, None])\n    eps = 1e-12 # avoid log(0)\n    y_pred = np.clip(y_pred, eps, 1 - eps)\n    # binary cross entropy\n    bce = -(y_true[:, None, None] * np.log(y_pred) +\n            (1 - y_true[:, None, None]) * np.log(1 - y_pred))\n    return np.mean(bce, axis=0)\n\nn = 240\nx = np.linspace(-np.pi, np.pi, n)\ntheta = [9, 1.2, 3.0, 0.3, -0.8, 5.0, -0.1, 0.6, 7.0, 0.7, 0.2]\ny_true = y_true_fn(theta,x)\n\n#plt.figure(figsize=(6, 5))\n#plt.scatter(x,y_true)\n#plt.show()\n\n# construct 2D parameter grid\nw1 = np.linspace(-6, 6, 200)\nw2 = np.linspace(-6, 6, 200)\nW1, W2 = np.meshgrid(w1, w2)\n\nZ = MSEloss(W1, W2)\n\nplt.figure(figsize=(6,5))\nplt.contourf(W1, W2, Z, levels=100, cmap='viridis')\nplt.colorbar(label='Loss')\nplt.xlabel('w1')\nplt.ylabel('w2')\nplt.title('Non-convex Loss Surface')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nChallenge: find (global) minimum for non-convex landscapes\nNon-convexity is problem of (non-linear) model, not the complex data distribution\nneed efficient numerical algorithm to navigate in complex loss landscapes"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html#goal",
    "href": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html#goal",
    "title": "Loss and Gradients",
    "section": "",
    "text": "Find loss minimum numerically and efficiently.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n# define truth (possibly simple)\ndef y_true_fn(theta, x):\n    y_true = ( theta[0] + \n        theta[1]*np.sin(theta[2] * x + theta[3]) +\n        theta[4]*np.cos(theta[4] * x - theta[6]) +\n        theta[7]*np.sin(theta[8] * x + theta[9])\n    )\n    # complexity of true model is irrelevant for shape of loss surface\n    y_true = theta[0] + theta[1]*x \n\n#   optional: turn into binary\n#    y_true = (y_true &gt; np.median(y_true)).astype(int) \n    return y_true\n\n# define model predictions (complex)\ndef y_pred_fn(w1, w2, x):\n#    y_pred = sigmoid(w1 + w2 * x)\n    y_pred = np.sin(np.pi*x + np.sin(w1*np.pi*x)) * np.cos(w2*x)\n    return y_pred\n\ndef MSEloss(w1, w2):\n    y_pred = y_pred_fn(w1, w2, x[:, None, None])\n    return np.mean((y_true[:, None, None] - y_pred)**2, axis=0)\n\ndef CEloss(w1, w2):\n    y_pred = y_pred_fn(w1, w2, x[:, None, None])\n    eps = 1e-12 # avoid log(0)\n    y_pred = np.clip(y_pred, eps, 1 - eps)\n    # binary cross entropy\n    bce = -(y_true[:, None, None] * np.log(y_pred) +\n            (1 - y_true[:, None, None]) * np.log(1 - y_pred))\n    return np.mean(bce, axis=0)\n\nn = 240\nx = np.linspace(-np.pi, np.pi, n)\ntheta = [9, 1.2, 3.0, 0.3, -0.8, 5.0, -0.1, 0.6, 7.0, 0.7, 0.2]\ny_true = y_true_fn(theta,x)\n\n#plt.figure(figsize=(6, 5))\n#plt.scatter(x,y_true)\n#plt.show()\n\n# construct 2D parameter grid\nw1 = np.linspace(-6, 6, 200)\nw2 = np.linspace(-6, 6, 200)\nW1, W2 = np.meshgrid(w1, w2)\n\nZ = MSEloss(W1, W2)\n\nplt.figure(figsize=(6,5))\nplt.contourf(W1, W2, Z, levels=100, cmap='viridis')\nplt.colorbar(label='Loss')\nplt.xlabel('w1')\nplt.ylabel('w2')\nplt.title('Non-convex Loss Surface')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nChallenge: find (global) minimum for non-convex landscapes\nNon-convexity is problem of (non-linear) model, not the complex data distribution\nneed efficient numerical algorithm to navigate in complex loss landscapes"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html#gradients",
    "href": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html#gradients",
    "title": "Loss and Gradients",
    "section": "Gradients",
    "text": "Gradients\n\n\n\n\n\n\nIdea & Hope\n\n\n\nuse local properties of loss lanscape to update parameters\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the loss function and its derivative (gradient)\ndef L(w):\n    return (w - 2)**2 + 1   # Parabola with minimum at w=2\n\ndef dL(w):\n    return 2 * (w - 2)\n\n# Choose a point away from the minimum\nw0 = 0.5\nL0 = L(w0)\ngrad = dL(w0)\n\n# Define tangent line at w0\ndef tangent(w):\n    return L0 + grad * (w - w0)\n\n# Create range for plotting\nw = np.linspace(-1, 3, 200)\n\n# Plot\nplt.figure(figsize=(6, 6))\nplt.plot(w, L(w), label='Loss $L(w)$', color='blue')\nplt.plot(w, tangent(w), '--', color='red', label='Tangent at $w_0$')\nplt.scatter([w0], [L0], color='red', zorder=5, label=f'initial $w_0 = {w0}$')\nplt.scatter([2], [L(2)], color='green', zorder=5, label='Minimum')\n\n# Annotate gradient direction\nplt.arrow(w0, L0, 0.6, grad*0.6, head_width=0.1, head_length=0.3, fc='red', ec='red')\n\n# Labels and style\nplt.xlabel('Weight $w$')\nplt.ylabel('Loss $L(w)$')\nplt.title('Simple Loss Function')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nA more realistic loss function will depend on many weights and have non-convex shape\n\n\n\n\n\n\n\n\n\n\nGradient Descent\n\n\n\n\nchanges in parameter \\(w\\) will propagates through network to affect \\(L\\)\nParameter Update \\[\nw = w - \\alpha \\frac{dL}{dw}\n\\]\nsimilar for bias term \\(b\\)\nlocal derivative: \\(\\frac{dL}{dw} = f(w)\\)\nvectorization: \\(w\\) and \\(\\frac{dL}{dw}\\) are vectors of same dimension. Derivatives \\(\\to\\) Gradients\n\\(\\alpha\\): learning rate (hyperparameter)\n\n\n\n\nImportant Derivatives\nHow to calculate local slopes\n\n\n\n\n\n\n\nFunction \\(f(x)\\)\nDerivative \\(f'(x)=\\frac{df}{dx}\\)\n\n\n\n\n\\(5x\\)\n\\(5\\)\n\n\n\\(5x^2\\)\n\\(10x\\)\n\n\n\\(e^x\\)\n\\(e^x\\)\n\n\n\\(e^{-x}\\)\n\\(-e^{-x}\\)\n\n\nSigmoid: \\(\\frac{1}{1 + e^{-x}}\\)\n\\(\\frac{e^{-x}}{(1 + e^{-x})^2} = f(x)(1 - f(x))\\)\n\n\n\\(\\tanh(x)\\)\n\\(1 - f(x)^2\\)\n\n\nReLU: \\(max(0, x)\\)\n\\((0,1)\\)\n\n\n\\(f(g(x))\\)\n\\(f'(g(x)) \\cdot g'(x) \\longrightarrow\\) chain rule"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html#gradient-descent-1",
    "href": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html#gradient-descent-1",
    "title": "Loss and Gradients",
    "section": "Gradient Descent",
    "text": "Gradient Descent\neach sample \\(i\\) contributes to overall loss and overall gradient \\[\n\\begin{aligned}\nL(w) &= \\frac{1}{N}\\sum_i L_i(w) \\\\\n\\frac{dL}{dw} &= \\frac{1}{N}\\sum_i \\frac{dL_i}{dw} \\\\\nw &= w - \\alpha \\frac{d L}{dw}\n\\end{aligned}\n\\]\n\nInitialization (random): \\(w_0\\)\nstep-wise update is expensive (sum over all data)\nin practice this is done for **batches* of training samples"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html#forward-calculation",
    "href": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html#forward-calculation",
    "title": "Loss and Gradients",
    "section": "Forward Calculation",
    "text": "Forward Calculation\n\nExample: Logistic regression\n\n\n\nLogistic regression with two input \\((x_1, x_2\\)) and one output activation \\(a \\in [0,1]\\). This network encodes a computation that is applied to each input sample \\((x_1, x_2)\\).\n\n\nThe loss function (binary cross entropy): \\[\nL(a,y) = -y \\log(a) - (1-y)\\log(1-a)\n\\]\n\n\n\n\n\n\nMinimal Loss = Maximal Likelihood\n\n\n\n\n\n\nobserved data: \\(x\\)\ntrue label: \\(y\\)\npredicted label probability: \\(p =f(x, \\theta)\\)\n\nConditional Probability: \\[\nPr(\\mbox{Label}=y|x, \\theta) = p^y (1-p)^{(1-y)} \\to \\mbox{maximize !}\n\\]\nLoss for one sample \\(x\\): \\[\nL(p, y) = - \\log Pr(y|x, \\theta) =  - y \\log p - (1-y) \\log (1 - p) \\to \\mbox{minimize !}\n\\]\nLoss for all \\(N\\) samples \\((x_1,y_1), (x_2,y_2) \\ldots (x_N, y_N)\\) and predicted probabilities \\(p_i = f(x_i, \\theta)\\): \\[\nL(x,y) = \\frac{1}{N} \\sum_i L(p_i, y_i)\n\\]\n\n\n\n\n\nComputational Graph\n\n\n\nComputational Graph shows how data flows through a graph of basic operations (nodes)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html#backpropagation",
    "href": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html#backpropagation",
    "title": "Loss and Gradients",
    "section": "Backpropagation",
    "text": "Backpropagation\n\n\n\n\n\n\nChain Rule\n\n\n\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial w} &= \\left(\\frac{\\partial L}{\\partial a}\\right)\\left(\\frac{\\partial a}{\\partial z}\\right) \\left(\\frac{\\partial z}{\\partial w}\\right)\n\\end{aligned}\n\\]\n\n\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial a} &= -\\frac{y}{a} + \\frac{1-y}{1-a}\\\\\n\\frac{\\partial a}{\\partial z} &= a(1-a)~~~\\mbox{for sigmoid}\\\\\n\\frac{\\partial z}{\\partial w_j} &= x_j~~~; ~~~\\frac{\\partial z}{\\partial b} = 1\n\\end{aligned}\n\\]\n\n\n\n\n\n\nBackward Calculation\n\n\n\n\nderivatives = lookup and multiplication\ndecompose larger calculation into smaller local parts\nstrict computational flow ensures that all required contributions are known\nimportance of differentiable activation functions: \\(\\frac{\\partial a}{\\partial z}\\)\nattaches a gradient to each forward value: \\(a \\to da \\equiv \\frac{\\partial L}{\\partial a}\\)\nflexibility: other loss functions, other inputs, more layers\nthis is for one sample \\((\\mathbf{x},y)\\) only"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html#summary-forward-and-backward-flows",
    "href": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html#summary-forward-and-backward-flows",
    "title": "Loss and Gradients",
    "section": "Summary: Forward and Backward Flows",
    "text": "Summary: Forward and Backward Flows\nThe simple example (1 layer logistic regression) generalizes\n\nto deeper neural network with multiple layers\nparameters (weights and biases) at each layer \\(l\\)\nactivities \\(a\\) are calculated forward\ngradients \\(dL/da \\equiv da\\) are calculated backward\n\n\n\n\nForward and Backward Propagation through layer \\(l\\)\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\ngradients are byproduct of backpropagation\nshapes of gradients agree with shape of tensors: \\([\\frac{\\partial L}{\\partial a}] = [da] = [a]\\)\nmatrix notation for book-keeping/simplification and vectorized speedup\nlookup and matrix multiplication"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html#further-reading",
    "href": "lectures/02_NeuralNetworks/02_LossAndGradientDescent.html#further-reading",
    "title": "Loss and Gradients",
    "section": "Further reading",
    "text": "Further reading\n\n3Blue1Brown. Backpropagation.\nAutomatic Differentiation.\nAndrew Ng @ 6:23h"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Non-Linearities.html",
    "href": "lectures/02_NeuralNetworks/01_Non-Linearities.html",
    "title": "Non-linearities",
    "section": "",
    "text": "… for multinomial classification with \\(K\\) classes (in output layer)\n\\[\nz_k = w_k \\cdot x + b_k \\longrightarrow p_k = \\frac{e^{z_k}}{\\sum_{l=0}^{K-1} e^{z_l}}~~~~k \\in [0, ..., K-1]\n\\]\n\nshift invariance (for probabilities):\n\\(p_k = f(z_k) = f(z_k + c)\\)\nscale invariance (for class predictions): \\(\\hat y_k = \\arg \\max_k f(z_k) = \\arg \\max_k f(z_k/T) =\\arg \\max_k z_k\\)\n\n\n\nCode\nimport torch\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom itertools import product\n\nz = torch.tensor([-0.35, 1.5])\nC = [0.0, 33.3, 0.0, 0.0]\nT = [1.0, 1.0, 5.0, 0.50]\nlabels = [f\"c={c}, T={t}\" for c, t in zip(C,T)]\nprobs = torch.stack([F.softmax((z + c)/t, dim=0) for c, t in zip(C,T)])\nx = torch.arange(len(labels)).numpy()\np0 = probs[:, 0].numpy()\np1 = probs[:, 1].numpy()\n\n# --- stacked barplot ---\nplt.figure(figsize=(6, 4))\nplt.bar(x, p0, label=\"class 0\", color=\"steelblue\")\nplt.bar(x, p1, bottom=p0, label=\"class 1\", color=\"orange\")\nplt.axhline(0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\n\nplt.xticks(x, labels, rotation=30, ha=\"right\")\nplt.ylim(0, 1)\nplt.ylabel(\"Probability\")\nplt.title(\"Softmax probabilities (with Shifts and Scales)\")\nplt.legend(loc=\"upper right\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nShift invariance of probabilities, Scale invariance of predictions, Confidence and Over-Confidence. Here for 2 categories with z = [-0.35, +1.5].\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ninterprete output of NN as probabilities with \\(\\sum p_k = 1\\)\nsoftmax uses all neurons (of output layer)\nno hyper-parameter\n\n\n\n\n\nHow to avoid overflow \\(e^{z_k}\\) for large \\(|z_k|\\) ?\nDefine maximal \\(z_k\\)\n\\[\n\\begin{aligned}\nz^\\ast &= \\max_k z_k \\\\\n\\log\\sum_l e^{z_l} &= z^\\ast + \\log \\sum_l e^{z_l - z^\\ast}\n\\end{aligned}\n\\]\nThis trick is used by F.softmax(z) and other professional implementations of softmax."
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Non-Linearities.html#softmax",
    "href": "lectures/02_NeuralNetworks/01_Non-Linearities.html#softmax",
    "title": "Non-linearities",
    "section": "",
    "text": "… for multinomial classification with \\(K\\) classes (in output layer)\n\\[\nz_k = w_k \\cdot x + b_k \\longrightarrow p_k = \\frac{e^{z_k}}{\\sum_{l=0}^{K-1} e^{z_l}}~~~~k \\in [0, ..., K-1]\n\\]\n\nshift invariance (for probabilities):\n\\(p_k = f(z_k) = f(z_k + c)\\)\nscale invariance (for class predictions): \\(\\hat y_k = \\arg \\max_k f(z_k) = \\arg \\max_k f(z_k/T) =\\arg \\max_k z_k\\)\n\n\n\nCode\nimport torch\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom itertools import product\n\nz = torch.tensor([-0.35, 1.5])\nC = [0.0, 33.3, 0.0, 0.0]\nT = [1.0, 1.0, 5.0, 0.50]\nlabels = [f\"c={c}, T={t}\" for c, t in zip(C,T)]\nprobs = torch.stack([F.softmax((z + c)/t, dim=0) for c, t in zip(C,T)])\nx = torch.arange(len(labels)).numpy()\np0 = probs[:, 0].numpy()\np1 = probs[:, 1].numpy()\n\n# --- stacked barplot ---\nplt.figure(figsize=(6, 4))\nplt.bar(x, p0, label=\"class 0\", color=\"steelblue\")\nplt.bar(x, p1, bottom=p0, label=\"class 1\", color=\"orange\")\nplt.axhline(0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\n\nplt.xticks(x, labels, rotation=30, ha=\"right\")\nplt.ylim(0, 1)\nplt.ylabel(\"Probability\")\nplt.title(\"Softmax probabilities (with Shifts and Scales)\")\nplt.legend(loc=\"upper right\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nShift invariance of probabilities, Scale invariance of predictions, Confidence and Over-Confidence. Here for 2 categories with z = [-0.35, +1.5].\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ninterprete output of NN as probabilities with \\(\\sum p_k = 1\\)\nsoftmax uses all neurons (of output layer)\nno hyper-parameter\n\n\n\n\n\nHow to avoid overflow \\(e^{z_k}\\) for large \\(|z_k|\\) ?\nDefine maximal \\(z_k\\)\n\\[\n\\begin{aligned}\nz^\\ast &= \\max_k z_k \\\\\n\\log\\sum_l e^{z_l} &= z^\\ast + \\log \\sum_l e^{z_l - z^\\ast}\n\\end{aligned}\n\\]\nThis trick is used by F.softmax(z) and other professional implementations of softmax."
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Non-Linearities.html#sigmoid",
    "href": "lectures/02_NeuralNetworks/01_Non-Linearities.html#sigmoid",
    "title": "Non-linearities",
    "section": "Sigmoid",
    "text": "Sigmoid\n… a special case for two output variables: binary logistic regression\n\\[\n\\left[z_0, z_1\\right] \\to \\left[ p_0 = \\frac{e^{z_0}}{e^{z_0} + e^{z_1}}, p_1 = 1 - p_0 \\right] \\\\\n\\]\nBinary Classification can be realized with single output neuron\n\\[\np_0 = \\sigma(z_0) = \\frac{1}{1+\\exp(-z_0)}\n\\]\n\n\n\n\n\n\nBinary Cross Entropy\n\n\n\n\nspecial case of Cross Entropy 2 classes (binary output).\n\nLoss for a single sample \\((x,y) \\to (z, y) \\to (p, y) \\to (\\hat y, y)\\)\n\\[\nL(p, y) = - \\sum_l y_l \\log p_l = - y_0 \\log p_0 - (1-y_0) \\log( 1 - p_0)\n\\]\nTrue class label: \\(y_0 \\in \\{0, 1\\}\\) and \\(y_1 = 1 - y_0\\)\nTotal loss = average loss for all samples \\((x_i,y_i)\\)\n\\[\nL = \\frac{1}{N} \\sum_i L(p_i, y_i)\n\\]\n\n\n\n\nCode\nz = torch.linspace(-5, 5, steps=200)\ny_sigmoid = torch.sigmoid(z)\nplt.figure(figsize=(8, 5))\nplt.plot(z, y_sigmoid, label='Sigmoid', linewidth=2)\nplt.title(\"Sigmoid Function\")\nplt.xlabel(\"z\")\nplt.ylabel(\"σ(z)\")\nplt.grid(True) \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ninput: \\(-\\infty \\ldots +\\infty\\)\noutput: \\(0 \\ldots 1\\) (probability interpretation)\nsharp transitions, position determined by bias\nnarrow range of changes & asymptotically flat\nsigmoid can also be used for internal neurons\nsigmoid works independently for individual neurons\nno additional hyper-parameter"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Non-Linearities.html#other-activations-functions",
    "href": "lectures/02_NeuralNetworks/01_Non-Linearities.html#other-activations-functions",
    "title": "Non-linearities",
    "section": "Other Activations Functions",
    "text": "Other Activations Functions\nIntroduce non-linearities also for internal layers\n2-layer network without non-linearities\n\\[\n\\begin{aligned}\nz_1 & = W_1 \\cdot x + b_1 \\\\\nz_2 & = W_2 \\cdot z_1 + b_2 \\\\\n& = W_2 ( W_1 \\cdot x + b_1) + b_2 \\\\\n&= W^\\prime x + b^\\prime\n\\end{aligned}\n\\]\n… useless and corresponds to 1-layer network !\n\n\n\n\n\n\nUse (non-linear) activation functions \\(a(z)\\)\n\n\n\n\\[\n\\begin{aligned}\nz_1 & = W_1 \\cdot x + b_1 \\\\\na_1 & = a(z_1) \\\\\nz_2 & = W_2 \\cdot a_1 + b_2 \\\\\na_2 & = a(z_2)\n\\end{aligned}\n\\]\n\nmany activation functions \\(a(z)\\) are possible\nshould be simple and differentiable\n\n\n\n\ntanh\n\\[\n\\begin{aligned}\n\\tanh(z) &= \\frac{e^z - e^{-z}}{e^z + e^{-z}} = 2 \\sigma(2z) - 1\n\\end{aligned}\n\\]\n\n\nCode\ny_tanh = torch.tanh(z)\nplt.figure(figsize=(8, 5))\nplt.plot(z, y_tanh, label='tanh', linewidth=2)\nplt.title(\"tanh\")\nplt.xlabel(\"z\")\nplt.ylabel(\"tanh(z)\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ninput: \\(-\\infty \\ldots +\\infty\\)\noutput: \\(-1 \\ldots 1\\)\n“centered” and scaled sigmoid\nsharp transitions, position determined by bias\nsmall range of changes & asymptotically flat\nzero-centered: positive and negative output\n\n\n\n\n\nReLU: Rectified Linear Unit\n\\[\nReLU(z) = \\max(0,z)\n\\]\n\n\nCode\ny_relu = F.relu(z)\nplt.figure(figsize=(8, 5))\nplt.plot(z, y_relu, label='ReLU', linewidth=2)\nplt.title(\"ReLU\")\nplt.xlabel(\"z\")\nplt.ylabel(\"ReLU(z)\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ninput: \\(-\\infty \\ldots +\\infty\\)\noutput: \\(0 \\ldots \\infty\\)\nvery simple and cheap\nno saturation for \\(x&gt;0\\)\nonly discontinuity at \\(x=0\\)\n\n\n\n… and many more activation functions possible."
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Non-Linearities.html#summary",
    "href": "lectures/02_NeuralNetworks/01_Non-Linearities.html#summary",
    "title": "Non-linearities",
    "section": "Summary",
    "text": "Summary\n\n\nCode\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nz = torch.linspace(-5, 5, 400)\ny_relu = F.relu(z)\ny_silu = F.silu(z)\ny_sigmoid = torch.sigmoid(z)\n\nplt.plot(z, y_relu, label=\"ReLU\", linestyle=\"--\")\nplt.plot(z, y_silu, label=\"SiLU (Swish)\")\nplt.plot(z, y_sigmoid, label=\"Sigmoid\", linestyle=\":\")\nplt.legend()\nplt.title(\"Activation Functions\")\nplt.grid(True, linestyle=\":\")\nplt.xlabel(\"z\")\nplt.ylabel(\"a(z)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSigmoid\ntanh\nReLU\nSiLU\n\n\n\n\nDefinition\n\\(\\frac{1}{1 + e^{-z}}\\)\n\\(\\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\\)\n\\(\\max(0, z)\\)\n\\(\\frac{z}{1 + e^{-z}}\\)\n\n\nRange\n\\((0, 1)\\)\n\\((−1, 1)\\)\n\\(\\left[0, \\infty \\right)\\)\n\\((-1.28, \\infty)\\)\n\n\nAdvantages\n• Smooth and bounded• Probabilistic interpretation• Historically important\n• Zero-centered• Smooth and bounded• Symmetric around 0\n• Simple and fast• Avoids vanishing gradients for x&gt;0• Sparse activations\n• Smooth and differentiable• Avoids dead neurons• No hyperparameters\n\n\nDisadvantages\n• Not zero-centered• Saturates → vanishing gradients• Slow convergence in deep nets\n• saturates for large |x| → vanishing gradients• Slower than ReLU\n• Zero gradient for x&lt;0 (dead neurons)• Non-smooth at 0\n• Slightly higher compute cost (needs sigmoid)• Non-monotonic around −1• Less intuitive\n\n\nTypical use cases\noutput layer for binary classification\nRNNs, LSTMs\nDefault for many modern NN\nTransformers, ReLU replacement"
  },
  {
    "objectID": "lectures/01_MachineLearning/04_MultivariateRegression.html",
    "href": "lectures/01_MachineLearning/04_MultivariateRegression.html",
    "title": "Multivariate Linear Regression",
    "section": "",
    "text": "A simple neuron calculating a simple linear function.\n\n\n\n\n\n\n\n\nSome Lessons from iris\n\n\n\n\nThere is no perfect fit because real data is noisy. In general: MSE \\(\\ne 0\\)\nModels are ignorant: What is the Petal.Width for Petal.Length = -1cm ?\nModels are not necessarily causal\n“All models are wrong, some are useful”\nAssessments of predicted performance should invoke some left out data (test set)"
  },
  {
    "objectID": "lectures/01_MachineLearning/04_MultivariateRegression.html#summary",
    "href": "lectures/01_MachineLearning/04_MultivariateRegression.html#summary",
    "title": "Multivariate Linear Regression",
    "section": "",
    "text": "A simple neuron calculating a simple linear function.\n\n\n\n\n\n\n\n\nSome Lessons from iris\n\n\n\n\nThere is no perfect fit because real data is noisy. In general: MSE \\(\\ne 0\\)\nModels are ignorant: What is the Petal.Width for Petal.Length = -1cm ?\nModels are not necessarily causal\n“All models are wrong, some are useful”\nAssessments of predicted performance should invoke some left out data (test set)"
  },
  {
    "objectID": "lectures/01_MachineLearning/04_MultivariateRegression.html#generalization-more-input-variables",
    "href": "lectures/01_MachineLearning/04_MultivariateRegression.html#generalization-more-input-variables",
    "title": "Multivariate Linear Regression",
    "section": "Generalization: more input variables",
    "text": "Generalization: more input variables\n\n\n\nA simple neuron receiving multiple inputs - and calculating a linear function.\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport torch\nimport torch.nn as nn\n\n\n\n\nCode\n# get iris data from sklearn\niris = datasets.load_iris() \nX = iris.data[:,:3] # first three columns  \ny = iris.data[:,3]  # forth column\n\n# convert to tensors\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\nprint(f\"X.shape = {X.shape}, y.shape = {y.shape}\")\n\n\nX.shape = torch.Size([150, 3]), y.shape = torch.Size([150, 1])"
  },
  {
    "objectID": "lectures/01_MachineLearning/04_MultivariateRegression.html#model-training",
    "href": "lectures/01_MachineLearning/04_MultivariateRegression.html#model-training",
    "title": "Multivariate Linear Regression",
    "section": "Model Training",
    "text": "Model Training\n… requires minimal changes to pytorch code\n\n\nCode\nn_epochs = 1000\nmodel = nn.Linear(3, 1)  # only change: n_in = 3, n_out = 1\n\n# all below as before\nloss_func = nn.MSELoss()  # MSE!\noptimizer = torch.optim.SGD(model.parameters()) \n\nmodel.train()\nfor epoch in range(n_epochs):\n    y_pred = model(X) \n    loss = loss_func(y_pred, y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(\"Fitted parameters:\")\nprint(\"Intercept (bias):\", model.bias.item())\n\n## Notice more weights\nprint(\"Coefficients (weights):\", model.weight.detach().numpy())\nprint('Loss: ', loss)    \n\n\nFitted parameters:\nIntercept (bias): 0.2394721806049347\nCoefficients (weights): [[ 0.0421822  -0.21746331  0.36797115]]\nLoss:  tensor(0.0560, grad_fn=&lt;MseLossBackward0&gt;)"
  },
  {
    "objectID": "lectures/01_MachineLearning/04_MultivariateRegression.html#predict-and-plot",
    "href": "lectures/01_MachineLearning/04_MultivariateRegression.html#predict-and-plot",
    "title": "Multivariate Linear Regression",
    "section": "Predict and Plot",
    "text": "Predict and Plot\nNow the predicted values y_pred depend on multiple inputs high-dimensional inputs x. It’s convenient to plot the true value y_true against y_pred = f(x).\n\n\nCode\nmodel.eval()\ny_pred = model(X)\n\n# convert to numpy (only for plotting)\nyp = y_pred.detach().numpy()\nyt = y.numpy()\n\ntitle_str = f\"Truth vs Predictions. Loss = MSE = {loss:.3f}\"\nplt.figure(figsize=(6,4))\nplt.plot(yp, yt, 'o')\nplt.xlabel(\"y_pred = f(x)\")\nplt.ylabel(\"y_true\")\nplt.title(title_str)\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n/var/folders/jh/jwym2_j128v8nn1dwc0sc10m0000gn/T/ipykernel_47611/1653731775.py:14: UserWarning:\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument."
  },
  {
    "objectID": "lectures/01_MachineLearning/06_TrainingAndTesting.html",
    "href": "lectures/01_MachineLearning/06_TrainingAndTesting.html",
    "title": "Training And Testing Data",
    "section": "",
    "text": "Code\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# helper function\ndef poly_features(x, degree: int):\n    # built polynomial features from x (up to degree)\n    # x -&gt; [1, x, x^2, ..., x^degree]\n    cols = [torch.ones_like(x)]\n    for p in range(1, degree + 1):\n        cols.append(x ** p)\n    return torch.cat(cols, dim=1)\n\ndef model_train(model, X, y, epochs=2000, lr=0.05):\n    loss_func = torch.nn.MSELoss()\n    opt = torch.optim.Adam(model.parameters(), lr=lr)\n#    opt = torch.optim.SGD(model.parameters(), lr=lr)\n    model.train()\n    for _ in range(epochs):\n        opt.zero_grad()\n        loss = loss_func(model(X), y)\n        loss.backward()\n        opt.step()\n    return loss.item()\n\ndef model_predict(model, X):\n    model.eval()\n    with torch.no_grad():\n        return model(X)\n\ndef model_test(model, X, y):\n    loss_func = torch.nn.MSELoss()\n    with torch.no_grad():\n        return loss_func(model(X), y).item()"
  },
  {
    "objectID": "lectures/01_MachineLearning/06_TrainingAndTesting.html#generate-synthetic-data",
    "href": "lectures/01_MachineLearning/06_TrainingAndTesting.html#generate-synthetic-data",
    "title": "Training And Testing Data",
    "section": "Generate synthetic data",
    "text": "Generate synthetic data\n\n\nCode\nseed = 0\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nn = 10\nnoise_sigma = 0.75\n\n# data range\nx = torch.linspace(-2.0, 2.0, n).unsqueeze(1) \n# synthetic data. very simple y = x + noise\ny = x + noise_sigma * torch.randn_like(x)\n\n# define plotting range (for later convenience)\nx_plot = torch.linspace(-2.1, 2.1, 200).unsqueeze(1)"
  },
  {
    "objectID": "lectures/01_MachineLearning/06_TrainingAndTesting.html#train-and-test-data",
    "href": "lectures/01_MachineLearning/06_TrainingAndTesting.html#train-and-test-data",
    "title": "Training And Testing Data",
    "section": "Train and Test Data",
    "text": "Train and Test Data\n\n\nCode\n# Train-test split (80/20)\nn_train = int(0.8 * n) # number of training data \n\n# permute indices [0,1,...,n-1] --&gt; [3, 1, 6, ....]\nperm = torch.randperm(n)  \ntrain_idx, test_idx = perm[:n_train], perm[n_train:]\nx_train, y_train = x[train_idx], y[train_idx]\nx_test, y_test = x[test_idx], y[test_idx]\n\n# plot orginal data\nplt.figure(figsize=(7,5))\nplt.scatter(x_train, y_train, color='black', label='train data', s=50)\nplt.scatter(x_test, y_test, color='red', label='test data', s=50)\nplt.title(\"Train and Test Data\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()"
  },
  {
    "objectID": "lectures/01_MachineLearning/06_TrainingAndTesting.html#fitting-under-and-over",
    "href": "lectures/01_MachineLearning/06_TrainingAndTesting.html#fitting-under-and-over",
    "title": "Training And Testing Data",
    "section": "Fitting: Under and Over",
    "text": "Fitting: Under and Over\n\n\n\n\n\n\nPolynomial Fit\n\n\n\n\n\n\n\nCode\npf = poly_features(x, 3)\nprint(pf[:5])\n\n\ntensor([[ 1.0000, -2.0000,  4.0000, -8.0000],\n        [ 1.0000, -1.5556,  2.4198, -3.7641],\n        [ 1.0000, -1.1111,  1.2346, -1.3717],\n        [ 1.0000, -0.6667,  0.4444, -0.2963],\n        [ 1.0000, -0.2222,  0.0494, -0.0110]])\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(7,5))\nplt.scatter(x_train, y_train, color='black', label='train data', s=50)\nplt.scatter(x_test, y_test, color='red', label='test data', s=50)\n\n# plot fits of different polynomial degrees\ndegrees = [0, 1, 7]\ncolors = ['blue', 'green', 'orange']\nfor d, col in zip(degrees, colors):\n    Xtr = poly_features(x_train, d)\n    Xpl = poly_features(x_plot, d)\n    model = torch.nn.Linear(d + 1, 1, bias=False)\n    model_train(model, Xtr, y_train)\n    y_pred = model_predict(model, Xpl)\n    plt.plot(x_plot, y_pred, color=col, lw=3, label=f\"d={d}\")\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Polynomial fits and overfits\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nhigh degree polynomials \\(\\to\\) complex models, many parameters, high flexibilty\ncomplex models can fit training data (black) points with high accuracy\nerror tends to be large on left-out data (test data, red)\nevaluate models on test data !\n\n\n\n\n\nCode\ndegrees = range(0, 9)\ntrain_mse, test_mse = [], []\nfor d in degrees:\n    Xtr = poly_features(x_train, d)\n    Xte = poly_features(x_test, d)\n    model = torch.nn.Linear(d + 1, 1, bias=False)\n    train_loss = model_train(model, Xtr, y_train)\n    test_loss = model_test(model, Xte, y_test)\n    train_mse.append(train_loss)\n    test_mse.append(test_loss)\n\nplt.figure(figsize=(7,5))\nplt.plot(degrees, train_mse, marker='o', label='Train MSE')\nplt.plot(degrees, test_mse, marker='o', label='Test MSE')\nplt.xlabel('Polynomial degree (model complexity)')\nplt.ylabel('Mean Squared Error')\nplt.title('Train vs Test error')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\noverfitted models do not generalize well to unseen data\nmany regularization methods"
  },
  {
    "objectID": "lectures/01_MachineLearning/06_TrainingAndTesting.html#appendix-more-splitting",
    "href": "lectures/01_MachineLearning/06_TrainingAndTesting.html#appendix-more-splitting",
    "title": "Training And Testing Data",
    "section": "Appendix: More splitting",
    "text": "Appendix: More splitting\nThere is also a powerful (and frequently used) method to conduct the train-test split with sklearn functionality. But this requires changing from torch tensors to numpy arrays (and convert back to tensors).\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\nprint(f'x.shape: {x.shape} y.shape {y.shape}')\nx_train, x_test, y_train, y_test = train_test_split(\n    x.numpy(), y.numpy(), test_size=0.2, random_state=42\n)\n\n# Convert back to torch\nx_train = torch.tensor(x_train, dtype=torch.float32)\nx_test  = torch.tensor(x_test, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32)\ny_test  = torch.tensor(y_test, dtype=torch.float32)\n\nprint(f'x_train.shape: {x_train.shape} y_train.shape {y_train.shape}')\nprint(f'x_test.shape: {x_test.shape} y_test.shape {y_test.shape}')\n\n\nx.shape: torch.Size([10, 1]) y.shape torch.Size([10, 1])\nx_train.shape: torch.Size([8, 1]) y_train.shape torch.Size([8, 1])\nx_test.shape: torch.Size([2, 1]) y_test.shape torch.Size([2, 1])"
  },
  {
    "objectID": "lectures/01_MachineLearning/index.html",
    "href": "lectures/01_MachineLearning/index.html",
    "title": "01 Machine Learning",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nData Stories\n\n\n\nData Formats\n\nData Types\n\nData Analysis Workflow\n\nData Challenges\n\nMetadata\n\n\n\nData is everywhere and in all shapes\n\n\n\nThomas Manke\n\n\nOct 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Machine Learning ?\n\n\n\nsklearn\n\npytorch\n\nmachine learning\n\nloss function\n\nepochs\n\ntraining loop\n\n\n\nCan machines learn?\n\n\n\nThomas Manke\n\n\nOct 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nModels 101\n\n\n\nlinear model\n\nRSS\n\nparameters\n\nloss function\n\nmodel matrix\n\nmodel checking\n\nexplained variance\n\n\n\nSome models are useful\n\n\n\nThomas Manke\n\n\nOct 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Linear Regression\n\n\n\nmultivariate regression\n\niris\n\n\n\nHandle multiple inputs\n\n\n\nThomas Manke\n\n\nOct 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Regression\n\n\n\nmultinomial regression\n\niris\n\n\n\nHandle multiple outputs\n\n\n\nThomas Manke\n\n\nOct 28, 2025\n\n\n\n\n\n\n\n\n\n\n\nTraining And Testing Data\n\n\n\noverfitting\n\nunderfitting\n\n\n\nuse different data sets to test your model\n\n\n\nThomas Manke\n\n\nOct 29, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html",
    "title": "What is Machine Learning ?",
    "section": "",
    "text": "“The principal difficulty … lay in the fact of there being too much evidence. What was vital was overlaid and hidden by what was irrelevant. Of all the facts which were presented to us we had to pick just those which we deemed to be essential, and then piece them together in their order, so as to reconstruct this very remarkable chain of events.” Sherlock Holmes (The Naval treaty, 1893)\n\n\n\nCan we write a program to do the same?"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#how-do-we-recognize-label-images",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#how-do-we-recognize-label-images",
    "title": "What is Machine Learning ?",
    "section": "",
    "text": "“The principal difficulty … lay in the fact of there being too much evidence. What was vital was overlaid and hidden by what was irrelevant. Of all the facts which were presented to us we had to pick just those which we deemed to be essential, and then piece them together in their order, so as to reconstruct this very remarkable chain of events.” Sherlock Holmes (The Naval treaty, 1893)\n\n\n\nCan we write a program to do the same?"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#a-paradigm-shift",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#a-paradigm-shift",
    "title": "What is Machine Learning ?",
    "section": "A paradigm shift",
    "text": "A paradigm shift\n\nClassical Programming: Rules (f) + Data (X) –&gt; Answers (Y)\n\nMachine Learning: Answers (Y) + Data (X) –&gt; Rules (f)\n\nQuiz (5 min): A simpler challenge: Find the rule \\(y = f(x)\\) for x and y below\n\n\nCode\nimport numpy as np\n\nx = np.array([-1,  0, 1, 2, 3, 4])     # Data\ny = np.array([-3, -1, 1, 3, 5, 7])     # Answer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice: a more realistic scenario would allow for errors: \\(y = f(x) + \\epsilon\\)"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#the-math-way-gauss-1809",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#the-math-way-gauss-1809",
    "title": "What is Machine Learning ?",
    "section": "The Math Way (Gauss: 1809)",
    "text": "The Math Way (Gauss: 1809)\n\nModel Definition\n\\[\n\\begin{align}\n\\mbox{Linear model:} ~~~& y=f(x_i; a,b) = a  + b x_i \\\\ \\\\\n\\mbox{2 Paramters:} ~~~& \\theta = (a,b) ~~~~\\mbox{same for all  } x_i\n\\end{align}\n\\]\n\n\nA goal\n\\[\n\\begin{align}\nL(\\theta) = \\sum_i (y_i - f(x_i;\\theta))^2  \\to \\mbox{minimize!}\\\\\n\\underset{\\theta}{\\mbox{argmin}} ~L(\\theta) \\longrightarrow \\hat{\\theta}\n\\end{align}\n\\]\n\nlinear regression = ordinary least squares (OLS)\nsynonymous terms: goal = loss function = cost function = objective function = …\nsolving the goal = fitting = training = learning = parameter estimation = (linear) regression\nfor linear models: fast and analytical solution !!\n\n\n\nOptimal Paramters\n\\[\n\\hat{\\theta} = (a,b) = (-1,2)\n\\]\n\n\nPredictions\nrun best model for new values, e.g. \\(x= (10, -40, \\ldots)\\)\n\n\nCode\nx_new = np.array([10, -40])\ny_new = -1 + 2.0*x_new\nprint('predictions: ', x_new, \"-&gt;\", y_new)\n\n\npredictions:  [ 10 -40] -&gt; [ 19. -81.]\n\n\n\n\n\n\n\n\nModeling Steps\n\n\n\n\ndefine the data\ndefine the model\nfit parameters\nevaluate model\nuse model (predict)"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#the-python-way-sklearn-2013",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#the-python-way-sklearn-2013",
    "title": "What is Machine Learning ?",
    "section": "The Python way (sklearn: 2013)",
    "text": "The Python way (sklearn: 2013)\n\n\nCode\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\n\nlm = linear_model.LinearRegression()  # define model\nxr = x.reshape(-1,1)                  # define data (+restructuring for specific tool) \n\nlm.fit(xr, y)             \n\n# report fit\nprint('Fitted Parameters       ', lm.intercept_, lm.coef_)\n\nyp = lm.predict(xr)                   # prediction\nMSE= mean_squared_error(y, yp)        # evaluate fit. other scores: R2=lm.score(xr, y)\nprint('Mean Squared Error:     ', MSE)\n\n# predict y for some new x\nx_new = np.linspace(-10, 10, 20)\ny_new = lm.predict(x_new.reshape(-1,1))\n\n\nFitted Parameters        -1.0 [2.]\nMean Squared Error:      0.0\n\n\n\n\nCode\ntitle_str = f\"Loss = {MSE:.3f}; Intercept = {lm.intercept_.item():.3f}, Slope = {lm.coef_.item():.3f}\"\nplt.figure(figsize=(6,4))\nplt.plot(x, y, 'o', label=\"Data\")\nplt.plot(x_new, y_new, label=\"Model prediction\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(title_str)\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#the-pytorch-way-2025",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#the-pytorch-way-2025",
    "title": "What is Machine Learning ?",
    "section": "The PyTorch Way (2025)",
    "text": "The PyTorch Way (2025)\nLearn parameters by iterative improvements\n\\[\n\\begin{array}{ll}\n\\textbf{Input:} & \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\} \\\\\n\\textbf{Initialize:} & \\theta \\text{ randomly} \\\\\n\\textbf{Repeat:} & \\\\\n& y_{\\mathrm{pred}} \\gets f(x; \\theta) \\\\\n& L \\gets L(y_{\\mathrm{pred}}, y) \\\\\n& \\theta \\gets \\text{update}(\\theta, L) \\\\\n\\textbf{Until:} & \\text{stopping criterion is met}\n\\end{array}\n\\]\n\n\nCode\nimport numpy as np\nimport torch\nprint('torch version:', torch.__version__)\n\n# convert to tensor, type and reshape to column vector\nx_t = torch.as_tensor(x, dtype=torch.float32).view(-1, 1)\ny_t = torch.as_tensor(y, dtype=torch.float32).view(-1, 1)\n\nmodel = torch.nn.Linear(1,1) # define black box model (1 input and 1 output)\nloss_func = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# training loop\nlosses = []\nmodel.train()\nfor epoch in range(200):\n    y_pred = model(x_t)             # model prediction\n    loss = loss_func(y_pred, y_t)   # model loss\n\n    optimizer.zero_grad()           # initalizations\n    loss.backward()                 # calculate new parameters\n    optimizer.step()                # update parameters\n\n    losses.append(loss.item())      # track loss history\n\n# finished training. inspect model \nprint(f'Mean Squared Error (loss) = {loss.item():.5f}')\nprint('Trained Parameters')\nprint(f'PyTorch: intercept =  {model.bias.item():.3f},  slope={model.weight.item():.3f}')\nprint(f'sklearn: intercept =  {lm.intercept_.item():.3f},  slope={lm.coef_.item():.3f}')\n\n\ntorch version: 2.7.1\nMean Squared Error (loss) = 0.01165\nTrained Parameters\nPyTorch: intercept =  -0.859,  slope=1.954\nsklearn: intercept =  -1.000,  slope=2.000\n\n\n\n\n\n\n\n\nDon’t Panic !!!\n\n\n\n\nlooks more complicated than sklearn, but is much more flexible and powerful\niterative approach is generic and extends to much more complex models, far beyond linear.\nPyTorch supports all modeling steps: define model, define loss function, define optimizer, fit model, predict.\nThere are alternative frameworks: TensorFlow/Keras, … \\(\\to\\) Tutorial\nI will use PyTorch, but it should be possible to transfer everything also to TF/Keras\nPyTorch has new data structures that need to get used to: torch.Tensor\npredictions less accurate (and slower) than with sklearn \\(\\to\\) iterations not yet converged\nloss (MSE) can be monitored to assess convergence"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#monitor-training",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#monitor-training",
    "title": "What is Machine Learning ?",
    "section": "Monitor Training",
    "text": "Monitor Training\n\n\nCode\nplt.figure()\nplt.plot(losses)\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.title('Loss history')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nloss \\(L\\) should decrease for sensible parameter updates\nloss histories are the most important tools to assess learning progress"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#making-predictions",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#making-predictions",
    "title": "What is Machine Learning ?",
    "section": "Making predictions",
    "text": "Making predictions\nEven though our current best model is not very good, we already have a blackbox that allows us to make prediction for any new data \\(x\\). Like so:\n\n\nCode\n# define some new input data  x_new\nx_new = np.linspace(-10, 10, 20)\nx_t = torch.as_tensor(x_new, dtype=torch.float32).view(-1, 1)\n\n# blackbox predcition\ny_new = model(x_t) \n\n\n\n\nCode\nintercept = model.bias.item()\nslope = model.weight.item()\ntitle_str = f\"Loss = {loss.item():.3f}; Intercept = {intercept:.3f}, Slope = {slope:.3f}\"\nplt.figure(figsize=(6,4))\nplt.plot(x, y, 'o', label=\"Data\")\nplt.plot(x_new, y_new.detach(), label=\"Model prediction\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(title_str)\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#summary",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#summary",
    "title": "What is Machine Learning ?",
    "section": "Summary",
    "text": "Summary\n\n3 ways to do linear regression - 1 common goal: minimize loss (MSE)\nGauss (linear algebra), scikit-learn (Gauss + Python), PyTorch (iterations & optimization)\n\n\n\n\nA simple neuron calculating a linear function."
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html",
    "href": "lectures/01_MachineLearning/01_Data.html",
    "title": "Data Stories",
    "section": "",
    "text": "Domain\nOpen Formats\nProprietary Formats\nTypical Sizes\n\n\n\n\nUnstructured text\nTXT, Markdown, LaTeX\nDOCX (Microsoft), Pages (Apple)\nkB - MB\n\n\ntabular data\nCSV, TSV\nXLSX (Microsoft), Numbers (Apple)\nkB - MB\n\n\nkey-value data (nested)\nJSON YAML, XML\n???\nkB - MB\n\n\nImages\nPNG, JPEG, TIFF, SVG\nPSD (Photoshop), HEIF/HEIC (Apple)\nkB - GB\n\n\nHuman Genome\nFASTA\n-\n1 GB\n\n\nUbuntu 25.04\n*.c\n-\n5.8 GB\n\n\nWindows 11\n\n.EXE, .DLL (Microsoft)\n7 GB\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nsizes = [7, 1]   # sizes in GB\nlabels = [\"Windows 11\", \"Human Genome\"]\nexplode = [0, 0.075]\n\nfig, ax = plt.subplots()\nwedges, texts = ax.pie(sizes, \n  labels=labels,\n  explode=explode,\n  wedgeprops = dict(width=0.75),\n  startangle=45\n)  \n\nfor t in texts:\n    t.set_fontsize(16)\n\nplt.show()\n\n\n\n\n\nYour Genome Is Not Big !"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-sizes",
    "href": "lectures/01_MachineLearning/01_Data.html#data-sizes",
    "title": "Data Stories",
    "section": "",
    "text": "Domain\nOpen Formats\nProprietary Formats\nTypical Sizes\n\n\n\n\nUnstructured text\nTXT, Markdown, LaTeX\nDOCX (Microsoft), Pages (Apple)\nkB - MB\n\n\ntabular data\nCSV, TSV\nXLSX (Microsoft), Numbers (Apple)\nkB - MB\n\n\nkey-value data (nested)\nJSON YAML, XML\n???\nkB - MB\n\n\nImages\nPNG, JPEG, TIFF, SVG\nPSD (Photoshop), HEIF/HEIC (Apple)\nkB - GB\n\n\nHuman Genome\nFASTA\n-\n1 GB\n\n\nUbuntu 25.04\n*.c\n-\n5.8 GB\n\n\nWindows 11\n\n.EXE, .DLL (Microsoft)\n7 GB\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nsizes = [7, 1]   # sizes in GB\nlabels = [\"Windows 11\", \"Human Genome\"]\nexplode = [0, 0.075]\n\nfig, ax = plt.subplots()\nwedges, texts = ax.pie(sizes, \n  labels=labels,\n  explode=explode,\n  wedgeprops = dict(width=0.75),\n  startangle=45\n)  \n\nfor t in texts:\n    t.set_fontsize(16)\n\nplt.show()\n\n\n\n\n\nYour Genome Is Not Big !"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-rates",
    "href": "lectures/01_MachineLearning/01_Data.html#data-rates",
    "title": "Data Stories",
    "section": "Data Rates",
    "text": "Data Rates\n\n\n\n\n\n\n\n\n\nDomain\nOpen Formats\nProprietary Formats\nTypical Rates\n\n\n\n\nAudio\nFLAC, WAV\n(MP3), M4A (Apple), WMA (Microsoft)\n1–10 MB/min\n\n\nVideo\nWebM/VP9\n(MP4/H.264), MOV (Apple), WMV (Microsoft)\n10 MB/min (HD) → GBs/min (4K+)\n\n\nInternet (Common Crawl)\nWARC (HTTP)\n-\n100 TB/ month ~ 2GB/min\n\n\nGenomics (sequencing machines)\nFASTQ, VCF, BAM, …\nBCL (Illumina), pod5 (ONT)\n34 GB/min → 2 GB/min\n\n\nEarth Observation System (NASA)\nHDF5\n-\n?? → 2 GB/min\n\n\nParticle physics (CERN LHC)\nROOT\n-\n1 PB/s → 2000 GB/min\n\n\nSquare Kilometer Array\nHDF5, FITS\n-\n75 TB/min → 1 TB/min\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\non-the-fly processing for large data\nship metadata with data\navoid data duplication\ncompression"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-io",
    "href": "lectures/01_MachineLearning/01_Data.html#data-io",
    "title": "Data Stories",
    "section": "Data I/O",
    "text": "Data I/O\nThere are several ways to get data into RAM for analysis\n\ntabular data\n\n\nCode\nimport pandas as pd\n\n# from local paths or URL\nurl=\"https://github.com/thomasmanke/DataSets/raw/refs/heads/main/iris.csv.gz\"\ndf = pd.read_csv(url, compression=\"gzip\")\n\nprint(df.head()) # A glimpse\n\n\n   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n0           5.1          3.5           1.4          0.2  setosa\n1           4.9          3.0           1.4          0.2  setosa\n2           4.7          3.2           1.3          0.2  setosa\n3           4.6          3.1           1.5          0.2  setosa\n4           5.0          3.6           1.4          0.2  setosa"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-integrity",
    "href": "lectures/01_MachineLearning/01_Data.html#data-integrity",
    "title": "Data Stories",
    "section": "Data Integrity",
    "text": "Data Integrity\nSometimes you may have to check data integrity after downloads and transfers\n\n\nCode\nimport requests\nimport hashlib\nimport pandas as pd\nfrom io import BytesIO\n\nurl=\"https://github.com/thomasmanke/DataSets/raw/refs/heads/main/iris.csv.gz\"\nresponse = requests.get(url)\ndata = response.content\nchecksum = hashlib.sha256(data).hexdigest()\nprint(f\"Checksum (SHA256) of compressed data: {checksum}\")\n\n# treat data as compressed file and load as before\n#data_io = BytesIO(data) \n#df = pd.read_csv(data_io, compression=\"gzip\")\n\n\nChecksum (SHA256) of compressed data: cc954f855bffd5fa2a5c80194f4c527b951a69e35b8ddb88661b9abb1cf84911"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-summaries",
    "href": "lectures/01_MachineLearning/01_Data.html#data-summaries",
    "title": "Data Stories",
    "section": "Data Summaries",
    "text": "Data Summaries\n\n\nCode\nprint(\"== Iris - Missing Values ==\")\nprint(df.isna().sum())     # column-wise      \nprint(\"== Iris - Statistical Summmary ==\")\nprint(df.describe(include=\"all\"))       \n\n\n== Iris - Missing Values ==\nSepal.Length    0\nSepal.Width     0\nPetal.Length    0\nPetal.Width     0\nSpecies         0\ndtype: int64\n== Iris - Statistical Summmary ==\n        Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\ncount     150.000000   150.000000    150.000000   150.000000     150\nunique           NaN          NaN           NaN          NaN       3\ntop              NaN          NaN           NaN          NaN  setosa\nfreq             NaN          NaN           NaN          NaN      50\nmean        5.843333     3.057333      3.758000     1.199333     NaN\nstd         0.828066     0.435866      1.765298     0.762238     NaN\nmin         4.300000     2.000000      1.000000     0.100000     NaN\n25%         5.100000     2.800000      1.600000     0.300000     NaN\n50%         5.800000     3.000000      4.350000     1.300000     NaN\n75%         6.400000     3.300000      5.100000     1.800000     NaN\nmax         7.900000     4.400000      6.900000     2.500000     NaN\n\n\n\n\n\n\n\n\nData \\(\\gg\\) Numbers\n\n\n\n\nsubject experise required \\(\\to\\) information on iris can be found here\ndata have metadata: when, who, where, why\nhave a question\nhave a model\n\n\n\n\nimage data\n\n\nCode\nfrom PIL import Image\nimport numpy as np\nimport requests\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\n\nurl = 'https://img-datasets.s3.amazonaws.com/elephant.jpg'\nresponse = requests.get(url)\nfo = BytesIO(response.content) # content2fileobject\nimg_obj = Image.open(fo)       # image object \nprint(f\"img_obj type: {type(img_obj)} size={img_obj.size} mode={img_obj.mode}\")\n\n# transformations (optional)\n#img_obj = img_obj.resize((200, 200))\n#img_obj = img_obj.rotate(45) \n\n# conversion: numpy array\nimg = np.array(img_obj, dtype=np.uint8)\nprint(f\"img type: {type(img)} shape: {img.shape}\")\n\n\nimg_obj type: &lt;class 'PIL.JpegImagePlugin.JpegImageFile'&gt; size=(1440, 1440) mode=RGB\nimg type: &lt;class 'numpy.ndarray'&gt; shape: (1440, 1440, 3)\n\n\n\n\nCode\nplt.imshow(img) # imshow accepts both pillow img and np\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\npackaged data\nSome datasets are very famous and they have been packaged for easy access.\n\n\nCode\nimport pandas as pd\nfrom seaborn import load_dataset\nfrom plotly.express import data\nfrom torchvision import datasets\n\n# iris from seaborn\niris = load_dataset(\"iris\")\nprint(f\"{type(iris)} {iris.shape}\")  \n\n# gapminder from px\ngapminder = data.gapminder()\nprint(f\"{type(gapminder)} {gapminder.shape}\")\n\n# mnist from torchvision\nmnist = datasets.MNIST(root=\"./data\", train=True, download=True)\nprint(f\"{type(mnist.data)} {mnist.data.shape}\")  \n\n# Reshaping and data type conversions\nmnist_np = mnist.data.flatten(start_dim=1).numpy()\nprint(f\"{type(mnist_np)} {mnist_np.shape}\")  \n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt; (150, 5)\n&lt;class 'pandas.core.frame.DataFrame'&gt; (1704, 8)\n&lt;class 'torch.Tensor'&gt; torch.Size([60000, 28, 28])\n&lt;class 'numpy.ndarray'&gt; (60000, 784)\n\n\n\n\n\n\n\n\nTask (10 min)\n\n\n\nSearch online for more detailed descriptions of those famous datasets (Iris, Gapminder, MNIST)\nDiscuss with your neighbour:\n\nWhat is in the data?\nHow many samples?\nHow many variables/features?\nWhich questions could we ask about the data?\n\nReport back to class.\n\n\n\n\nCode\n# Sometimes the data comes with descriptions\nfrom sklearn.datasets import load_iris\niris = load_iris()\nprint(iris.DESCR)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nclean data from packages\ndata types: package-dependent, may need conversion/reshaping\npay attention to shapes and dimensions\nwatch out for metadata and descriptions"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-analysis",
    "href": "lectures/01_MachineLearning/01_Data.html#data-analysis",
    "title": "Data Stories",
    "section": "Data Analysis",
    "text": "Data Analysis\n\n\n\nData Analysis Workflow. Iterations and Repetitions (from Hadley Wickham).\n\n\n\n\n\n\n\n\nData Analysis \\(\\gg\\) Modeling\n\n\n\n\n\n\n\n\n\n\n\nStage\nExample tasks\nTypical time share\n\n\n\n\nData Acquisition\ngathering data and metadata, checking consistency, negotiating access\n10–20 %\n\n\nData Cleaning\nHandling missing values, reshaping tables, fixing types, deduplication\n20–60 %\n\n\nData Transformation\nNormalization, scaling, filtering, creating derived variables\n20–30 %\n\n\nData Exploration\nVisualization, summary statistics, feature selection, sanity checks\n10–20 %\n\n\nData Modeling\nModel selection, Model training/(deep)learning, evaluation\n5–20 %\n\n\nData Communication\nReports, dashboards, storytelling\n5–10 %\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMost of this course will focus on “Data Modeling”"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-science-challenges",
    "href": "lectures/01_MachineLearning/01_Data.html#data-science-challenges",
    "title": "Data Stories",
    "section": "Data Science Challenges",
    "text": "Data Science Challenges\n\nLanguages and Jargon\nDomain Expertise\nDirty Data\nFAIR data\nStatistics & Maths\n(Programming)\n\n\n\n\n“Many safe roads at land; countless doomed routes at sea.”"
  },
  {
    "objectID": "homework/02_MNIST.html",
    "href": "homework/02_MNIST.html",
    "title": "01 Homework",
    "section": "",
    "text": "Reference:\n\nhttps://www.kaggle.com/code/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n\nRun a classification on the MNIST data with\n\nA simple logistic (10 classes)\nA three layer NN (128, 128, 128, 10) with ReLU\nA CNN\n\ncompare the accuracies on a test data"
  },
  {
    "objectID": "homework/02_MNIST.html#task",
    "href": "homework/02_MNIST.html#task",
    "title": "01 Homework",
    "section": "",
    "text": "Reference:\n\nhttps://www.kaggle.com/code/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n\nRun a classification on the MNIST data with\n\nA simple logistic (10 classes)\nA three layer NN (128, 128, 128, 10) with ReLU\nA CNN\n\ncompare the accuracies on a test data"
  },
  {
    "objectID": "homework/02_MNIST.html#packages",
    "href": "homework/02_MNIST.html#packages",
    "title": "01 Homework",
    "section": "Packages",
    "text": "Packages"
  },
  {
    "objectID": "homework/02_MNIST.html#data",
    "href": "homework/02_MNIST.html#data",
    "title": "01 Homework",
    "section": "Data",
    "text": "Data\nVerify mean and std"
  },
  {
    "objectID": "homework/02_MNIST.html#model-definition",
    "href": "homework/02_MNIST.html#model-definition",
    "title": "01 Homework",
    "section": "Model Definition",
    "text": "Model Definition"
  },
  {
    "objectID": "homework/02_MNIST.html#parameters",
    "href": "homework/02_MNIST.html#parameters",
    "title": "01 Homework",
    "section": "Parameters",
    "text": "Parameters"
  },
  {
    "objectID": "homework/02_MNIST.html#define-traing-loop",
    "href": "homework/02_MNIST.html#define-traing-loop",
    "title": "01 Homework",
    "section": "Define Traing Loop",
    "text": "Define Traing Loop"
  },
  {
    "objectID": "homework/02_MNIST.html#run-training",
    "href": "homework/02_MNIST.html#run-training",
    "title": "01 Homework",
    "section": "Run Training",
    "text": "Run Training"
  },
  {
    "objectID": "projects/02_DeepNeuralNetworks.html",
    "href": "projects/02_DeepNeuralNetworks.html",
    "title": "02 Deep Neural Networks",
    "section": "",
    "text": "In this project, each team will apply multi-layer perceptrons (MLPs) to learn and approximate nonlinear relationships from data. By constructing, training, and evaluating neural networks, you will explore how MLPs can approximate complex relationships and how architecture, hyperparameters and optimization affect performance."
  },
  {
    "objectID": "projects/02_DeepNeuralNetworks.html#overview",
    "href": "projects/02_DeepNeuralNetworks.html#overview",
    "title": "02 Deep Neural Networks",
    "section": "",
    "text": "In this project, each team will apply multi-layer perceptrons (MLPs) to learn and approximate nonlinear relationships from data. By constructing, training, and evaluating neural networks, you will explore how MLPs can approximate complex relationships and how architecture, hyperparameters and optimization affect performance."
  },
  {
    "objectID": "projects/02_DeepNeuralNetworks.html#project-assignment-table",
    "href": "projects/02_DeepNeuralNetworks.html#project-assignment-table",
    "title": "02 Deep Neural Networks",
    "section": "Project Assignment Table",
    "text": "Project Assignment Table\nTeams will receive different subtasks based on their assigned dataset (Project ID). See more details below.\n\n\n\n\n\n\n\n\n\nProject ID\nData Type\nTask Description\nTarget Variable (\\(y\\))\n\n\n\n\n2.1\nNon-linear function\nNon-linear regression\ncontinuous values\n\n\n2.2\nNon-linear function\nNon-linear classification\ndiscrete classes\n\n\n2.3\nComplex shapes\nPixel classification complex boundaries\nbinary classes (inside/outside)\n\n\n2.4\nDNA sequences\nSequence Classification (DNA)\nsequence labels"
  },
  {
    "objectID": "projects/02_DeepNeuralNetworks.html#tasks-and-deliverables",
    "href": "projects/02_DeepNeuralNetworks.html#tasks-and-deliverables",
    "title": "02 Deep Neural Networks",
    "section": "Tasks and Deliverables",
    "text": "Tasks and Deliverables\n\n\n\n\n\n\n\n\n#\nTask\nDescription\n\n\n\n\n1\nData loading and inspection\nLoad the dataset, inspect its structure and dimensions, and briefly describe its content.\n\n\n2\nData preprocessing\nTransform the data into a form suitable for MLP input:– Convert to numpy / torch tensors– Flatten or reshape inputs as needed– Split into training and test sets\n\n\n3\nModel definition\nDefine an appropriate MLP architecture (e.g with PyTorch nn.Sequential). Include number of layers, activation functions, and output dimensions. Report structure and number of parameters\n\n\n4\nModel training\nImplement a full training loop including:– Loss function and optimizer– Iterative training with loss updates– Plot the loss history\n\n\n5\nModel evaluation\nGenerate predictions on the test set and compute suitable metrics:– Regression: MSE or R²– Classification: overall accuracy and class-wise metrices\n\n\n6\nVisualization\nPlot training and validation loss curves, and visualize model results e.g. predicted vs. true values, or confusion matrices.\n\n\n7\nDiscussion\nProvide a short reflection on model behavior and performance."
  },
  {
    "objectID": "projects/02_DeepNeuralNetworks.html#submission-guidelines",
    "href": "projects/02_DeepNeuralNetworks.html#submission-guidelines",
    "title": "02 Deep Neural Networks",
    "section": "Submission Guidelines",
    "text": "Submission Guidelines\nPlease submit a single notebook per team. It will be the basis for your presentation and it will be shared with the class.\nThe notebook must\n\ninclude a statement of orginality and contributions at the beginning\nbe clean, well-structured, and reproducible.\nrun from start to finish without modification (ideally in less than 5 minutes)\ninclude all necessary installation instructions, imports and random seeds for reproducibility.\ninclude short text cells and clear comments describing what you do in each section.\nshould use concise and readable visualizations.\nbe submitted before the deadline.\nfollow filename convention: Project1.3_Team02.ipynb (adjust your Project-ID and Team-ID)."
  },
  {
    "objectID": "projects/02_DeepNeuralNetworks.html#data-loads",
    "href": "projects/02_DeepNeuralNetworks.html#data-loads",
    "title": "02 Deep Neural Networks",
    "section": "Data Loads",
    "text": "Data Loads\n\nProject 2.1\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef f_composed(x):\n    return np.sin(10*np.pi*x + np.sin(15*np.pi*x)) * np.cos(3*x) \n\ndef f_logistic(u=0.321,steps=1000):\n    y=np.empty(steps)\n    for i in range(steps):\n        u = 4*u*(1 - u)\n        y[i] = u\n    y = np.array(y)\n    return y\n\nn = 2500\nn_train = int(0.8 * n)\n\nx = np.linspace(-1, 1, n)\ny = f_composed(x) # try f_logistic() for a challenge\n\n\n\n\nProject 2.2\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\n\ndef make_spirals(n_samples=1000, noise=0.2, n_classes=2, random_state=None):\n    \"\"\"\n    This function creates n_samples x n_classes data points in 2D\n    where each data points belongs to one of n_classes spirals (+ noise)\n    Output: \n        - X: X.shape [n_samples,2]  (coordinates)\n        - y: y.shape [n_samples,]    (class ID)\n    \"\"\"\n    rng = np.random.default_rng(random_state)\n    X = np.empty((n_samples * n_classes, 2), dtype=float)\n    y = np.repeat(np.arange(n_classes), n_samples)\n\n    r = np.linspace(0.0, 1.0, n_samples)\n\n    for k in range(n_classes):\n        t = np.linspace(4*k, 4*(k+1), n_samples) + rng.normal(scale=noise, size=n_samples)\n        start = k * n_samples\n        X[start:start+n_samples] = np.column_stack((r * np.sin(t), r * np.cos(t)))\n\n    return X, y\n\nn, nc = 1000, 3 \nsig = 0.1\nseed = 42\n\n# choose one or both\nX, y = make_moons(n_samples=n, noise=sig, random_state = seed) \nX, y = make_spirals(n_samples=n, n_classes=nc, noise=sig, random_state=seed)\n\n\n\n\nProject 2.3\n\n\nCode\nimport requests\nfrom io import BytesIO\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# some suggestions - feel free to try your own shapes\nurl=\"https://github.com/thomasmanke/DataSets/blob/main/Berlin_50x50.png?raw=true\"\nurl=\"https://github.com/thomasmanke/DataSets/blob/main/Mandelbrot.png?raw=true\"\n\nwidth=50\nheight=50\nthreshold=100 # adjust grey-BW threshold if neccessary\nimg = Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\")\nimg = img.resize((width, height), Image.LANCZOS) # resize for speed\nimg = img.convert(\"L\")                          # 8-bit grayscale\nimg_np = np.array(img, dtype=np.uint8)          # convert to numpy\nimg_np = (img_np &gt;= threshold).astype(np.uint8) # convert to BW\n\n\n\n\nProject 2.4\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nurl=\"https://github.com/thomasmanke/DataSets/raw/refs/heads/main/labeled_sequences.csv.gz\"\n\ndf = pd.read_csv(url)\nseqs = df[\"sequence\"].tolist()\ny    = df[\"label\"].values\n\nN = len(seqs)    # number of sequences\nL = len(seqs[0]) # same length for all (!!!) sequences\n\n\n\n\nHow to convert sequences to numbers\nimport torch\nimport torch.nn.functional as F\n\n# Some hints to convert: DNA seqs -&gt; X\n# There are other ways, but feel free to follow the recipe below\n# chr -&gt; int -&gt; bool (one-hot encoding)\n# but please make sure you understand the steps\n\n# maps to switch chr &lt;-&gt; int\nmapping = dict(zip(\"ACGT\", range(4)))\nidx2base = {v: k for k, v in mapping.items()}\n\n# integer encoding of sequence: \"AAGTCCA\" -&gt; [0,0,2,3,1,1,0]\nseqs_int = torch.tensor([[mapping[ch] for ch in s] for s in seqs], dtype=torch.long)\n\n# one-hot encoding of seqs_int: \nX = F.one_hot(seqs_int, num_classes=4).float() \n\n# flatten one-hot encoding\nX = X.view(N, 4*L)\nprint(f\"X.shape: {X.shape}, y.shape: {y.shape}, L = {L}\")"
  },
  {
    "objectID": "projects/01_MachineLearning.html",
    "href": "projects/01_MachineLearning.html",
    "title": "01 Machine Learning",
    "section": "",
    "text": "This project serves a introduction to the key challenges in machine learning and data analysis. Upon successful completion you should be able to conduct such analysis tasks independently and apply them to other problems.\n\nSetup Jupyter Notebook which includes\n\ndetail environment in which this nodebook was developed and tested\ninstallation of versioned packages (if necessary)\nload required software packages and report their version\nother custom functions (if necessary)\nproject-wide variables\n\nLoad Data (see examples below) and describe in detail their origin, content, structures and data types.\nData Exploration. Prepare statistical and visual data summaries. How many variables/features does the data contain, how many samples? Are there missing values? Explore correlations among variables and conduct at least one unsupervised analysis. Consider variable transformations and normalizations where applicable.\nData Model. Define what you want to model (following the table below) and choose an appropriate model framework. Define a simple baseline model and a loss function - theoretically and practically. Explain the model in terms of input-output data structures and dimensions.\nTrain Model. Define a Train Define Train and Test Data and run the training loop. Monitor the behaviour of the loss function.\nEvaluate Model. Run model predictions on the test data set. Calculate and visualize the appropriate metrics to evaluate model performance. Explain why a separate test data set should be used for such evaluations.\nImprove Model. Suggest extensions to your baseline model and repeat steps 4. - 6. Do these extensions improve the model?\n\n\n\n\n\n\n\nSummary\n\n\n\nProvide a concise summary and critical reflection of this project and all analysis steps. What are the assumptions and limits? Document and annotate all steps above carefully in a single jupyter notebook and ensure that it will execute reproducibly and without error. Upon completion, submit the project as a single notebook per team. This notebook will be the basis for your presentation and it will be shared with the whole class.\nInclude a statement of originality and contributions at the beginning"
  },
  {
    "objectID": "projects/01_MachineLearning.html#tasks",
    "href": "projects/01_MachineLearning.html#tasks",
    "title": "01 Machine Learning",
    "section": "",
    "text": "This project serves a introduction to the key challenges in machine learning and data analysis. Upon successful completion you should be able to conduct such analysis tasks independently and apply them to other problems.\n\nSetup Jupyter Notebook which includes\n\ndetail environment in which this nodebook was developed and tested\ninstallation of versioned packages (if necessary)\nload required software packages and report their version\nother custom functions (if necessary)\nproject-wide variables\n\nLoad Data (see examples below) and describe in detail their origin, content, structures and data types.\nData Exploration. Prepare statistical and visual data summaries. How many variables/features does the data contain, how many samples? Are there missing values? Explore correlations among variables and conduct at least one unsupervised analysis. Consider variable transformations and normalizations where applicable.\nData Model. Define what you want to model (following the table below) and choose an appropriate model framework. Define a simple baseline model and a loss function - theoretically and practically. Explain the model in terms of input-output data structures and dimensions.\nTrain Model. Define a Train Define Train and Test Data and run the training loop. Monitor the behaviour of the loss function.\nEvaluate Model. Run model predictions on the test data set. Calculate and visualize the appropriate metrics to evaluate model performance. Explain why a separate test data set should be used for such evaluations.\nImprove Model. Suggest extensions to your baseline model and repeat steps 4. - 6. Do these extensions improve the model?\n\n\n\n\n\n\n\nSummary\n\n\n\nProvide a concise summary and critical reflection of this project and all analysis steps. What are the assumptions and limits? Document and annotate all steps above carefully in a single jupyter notebook and ensure that it will execute reproducibly and without error. Upon completion, submit the project as a single notebook per team. This notebook will be the basis for your presentation and it will be shared with the whole class.\nInclude a statement of originality and contributions at the beginning"
  },
  {
    "objectID": "projects/01_MachineLearning.html#originality-and-contributions",
    "href": "projects/01_MachineLearning.html#originality-and-contributions",
    "title": "01 Machine Learning",
    "section": "Originality and Contributions",
    "text": "Originality and Contributions\nAuthors: X.Y., A.I.\nThis notebook is our own work. Any other sources have been clearly marked and cited.\nAll authors contributed equally. (alternative: state specific contributions explicitly)"
  },
  {
    "objectID": "projects/01_MachineLearning.html#references",
    "href": "projects/01_MachineLearning.html#references",
    "title": "01 Machine Learning",
    "section": "References",
    "text": "References\nProvide references and citations where necessary"
  },
  {
    "objectID": "projects/01_MachineLearning.html#data-sets",
    "href": "projects/01_MachineLearning.html#data-sets",
    "title": "01 Machine Learning",
    "section": "Data Sets",
    "text": "Data Sets\nThe data sets below are heavily used in machine learning. They also have the advantage of being easily accessible and highly standardized.\nEach team should work with the data set (Project ID) assigned to them.\nWhen modelling the given target variable \\(y\\), it should be modelled as a multivariate function of other variables/features \\(X\\) in the data set. You can also consider relevant subsets of features, if you can argue to exclude some features, or for the purpose of a first simple baseline model.\n\n\n\nProject ID\nLink\ntarget variable \\(y\\)\n\n\n\n\n1.1\nwine data\nwine quality\n\n\n1.2\ngapminder\nlifeExpectancy\n\n\n1.3\nbreast cancer\ncancer status\n\n\n1.4\nMNIST\ndigit label"
  },
  {
    "objectID": "projects/01_MachineLearning.html#data-loading-hints",
    "href": "projects/01_MachineLearning.html#data-loading-hints",
    "title": "01 Machine Learning",
    "section": "Data Loading Hints",
    "text": "Data Loading Hints\nThere are many ways to obtain these data sets. Below are just a few suggestions.\nPlease explore the data carefully - transform and normalize where necessary.\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n    sep=\";\"\n)\nprint(df.describe())\ny = df['quality']\n\n\n\n\nCode\nimport plotly.express as px\ndf = px.data.gapminder() \ny = df['lifeExp']\n\n\n\n\nCode\nfrom sklearn.datasets import load_breast_cancer\nimport pandas as pd\n\nbc = load_breast_cancer(as_frame=True)\nX = bc.data\ny = pd.Series(bc.target).map({0: \"malignant\", 1: \"benign\"}).rename(\"status\")\ndf = pd.concat([X, y], axis=1)\n\n\n\n\nCode\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# this dataset already comes split in train and test\ntransform = transforms.ToTensor()\ntrain = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\ntest  = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\ntrain_loader = DataLoader(train, batch_size=64, shuffle=True)"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n01 Machine Learning\n\n\nAn ML starter\n\n\n\nThomas Manke\n\n\nOct 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n01 Machine Learning\n\n\nA template for Iris Data\n\n\n\nThomas Manke\n\n\nOct 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n02 Deep Neural Networks\n\n\nMLP for universal approximations\n\n\n\nThomas Manke\n\n\nNov 2, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html",
    "href": "projects/01_MachineLearningIris.html",
    "title": "01 Machine Learning",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import datasets"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#import-packages",
    "href": "projects/01_MachineLearningIris.html#import-packages",
    "title": "01 Machine Learning",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import datasets"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#obtain-iris-data-set",
    "href": "projects/01_MachineLearningIris.html#obtain-iris-data-set",
    "title": "01 Machine Learning",
    "section": "Obtain Iris data set",
    "text": "Obtain Iris data set\n\n\nCode\niris = datasets.load_iris()\nX = iris.data    # measurements\ny = iris.target  # species label\n\nprint(f\"type(X) = {type(X)}\")\nprint(f\"type(y)  = {type(y)}\")\nprint(f\"shape(X)   = {X.shape}\")\nprint(f\"shape(y) = {y.shape}\")\n\n# define number of observations and number of variables\nn_obs, n_var = iris.data.shape\n\n\ntype(X) = &lt;class 'numpy.ndarray'&gt;\ntype(y)  = &lt;class 'numpy.ndarray'&gt;\nshape(X)   = (150, 4)\nshape(y) = (150,)"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#convert-and-reshape",
    "href": "projects/01_MachineLearningIris.html#convert-and-reshape",
    "title": "01 Machine Learning",
    "section": "Convert and Reshape",
    "text": "Convert and Reshape\n… use pandas dataframe for convenience\n\n\nCode\ndf = pd.DataFrame(X, columns=iris.feature_names)\nprint(\"Numerical Summaries\")\nprint(df.describe())\n\nprint(\"Wide Format - top\")\nprint(df.head())\n\n# reshape to long format\ndf_long = df.melt(var_name=\"Feature\", value_name=\"Value\")\nprint('Long Format - top')\nprint(df_long.head())\n\n\nNumerical Summaries\n       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\ncount         150.000000        150.000000         150.000000   \nmean            5.843333          3.057333           3.758000   \nstd             0.828066          0.435866           1.765298   \nmin             4.300000          2.000000           1.000000   \n25%             5.100000          2.800000           1.600000   \n50%             5.800000          3.000000           4.350000   \n75%             6.400000          3.300000           5.100000   \nmax             7.900000          4.400000           6.900000   \n\n       petal width (cm)  \ncount        150.000000  \nmean           1.199333  \nstd            0.762238  \nmin            0.100000  \n25%            0.300000  \n50%            1.300000  \n75%            1.800000  \nmax            2.500000  \nWide Format - top\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                5.1               3.5                1.4               0.2\n1                4.9               3.0                1.4               0.2\n2                4.7               3.2                1.3               0.2\n3                4.6               3.1                1.5               0.2\n4                5.0               3.6                1.4               0.2\nLong Format - top\n             Feature  Value\n0  sepal length (cm)    5.1\n1  sepal length (cm)    4.9\n2  sepal length (cm)    4.7\n3  sepal length (cm)    4.6\n4  sepal length (cm)    5.0"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#histogram",
    "href": "projects/01_MachineLearningIris.html#histogram",
    "title": "01 Machine Learning",
    "section": "Histogram",
    "text": "Histogram\n\n\nCode\nplt.figure()\nsns.histplot(df_long[\"Value\"], bins=30)\nplt.title(\"Histogram\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#violin-plots",
    "href": "projects/01_MachineLearningIris.html#violin-plots",
    "title": "01 Machine Learning",
    "section": "Violin Plots",
    "text": "Violin Plots\n\n\nCode\nprint(f\"variable means: {np.mean(X, axis=0)})\")\nplt.figure()\nsns.violinplot(x=\"Feature\", y=\"Value\", data=df_long)\nplt.title(\"Violin plots for all variables\")\nplt.tight_layout()\nplt.show()\n\n\nvariable means: [5.84333333 3.05733333 3.758      1.19933333])"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#missing-values",
    "href": "projects/01_MachineLearningIris.html#missing-values",
    "title": "01 Machine Learning",
    "section": "Missing Values",
    "text": "Missing Values\n\n\nCode\nna_counts = df.isna().sum() # pandas\n#na_counts = np.isnan(X).sum(axis=0) # numpy\nprint(\"Missing values:\\n\", na_counts)\n\nsns.scatterplot(x=na_counts.index, y=na_counts.values)\nplt.title(\"Number of missing values per feature\")\nplt.xlabel(\"Variable\")\nplt.ylabel(\"# missing\")\nplt.tight_layout()\nplt.show()\n\n\nMissing values:\n sepal length (cm)    0\nsepal width (cm)     0\npetal length (cm)    0\npetal width (cm)     0\ndtype: int64"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#heatmap",
    "href": "projects/01_MachineLearningIris.html#heatmap",
    "title": "01 Machine Learning",
    "section": "Heatmap",
    "text": "Heatmap\n\n\nCode\n# don't use heatmaps before you know what you get ;-)\nplt.figure()\nsns.heatmap(df, cmap=\"Blues\", cbar_kws={'label': 'Value'})\nplt.xlabel(\"Variables (features)\")\nplt.ylabel(\"Samples (observations)\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#correlations",
    "href": "projects/01_MachineLearningIris.html#correlations",
    "title": "01 Machine Learning",
    "section": "Correlations",
    "text": "Correlations\n\n\nCode\nsns.pairplot(df, height=1.5, aspect=1.0)\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorr = df.corr()\n\nplt.figure()\nsns.heatmap(\n    corr,\n    vmin=-1, vmax=1,\n    cmap=\"coolwarm\",    \n    annot=True,        # show correlation values\n    fmt=\".2f\",         # format to 2 decimals\n    square=True,       # make cells square\n    cbar_kws={\"label\": \"Pearson r\"}\n)\nplt.title(\"Correlation Matrix\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#scale-transformation",
    "href": "projects/01_MachineLearningIris.html#scale-transformation",
    "title": "01 Machine Learning",
    "section": "Scale Transformation",
    "text": "Scale Transformation\n… is almost always useful (esp. when variables live on different scales)\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nXs = scaler.fit_transform(X)\n\n# numpy2pandas (mostly for plotting convenience)\ncol_names = [\"SL\", \"SW\", \"PL\", \"PW\"]\ndf_scaled = pd.DataFrame(Xs, columns=col_names)\ndf_scaled_long = df_scaled.melt(var_name=\"Feature\", value_name=\"Value\")\n\nprint(f\"variable means: \\n{df_scaled.mean()})\")\nplt.figure()\nsns.violinplot(x=\"Feature\", y=\"Value\", data=df_scaled_long)\nplt.title(\"Violin plots for scaled variables\")\nplt.tight_layout()\nplt.show()\n\n\nvariable means: \nSL   -1.690315e-15\nSW   -1.842970e-15\nPL   -1.698641e-15\nPW   -1.409243e-15\ndtype: float64)"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#unsupervised-clustering-kmeans",
    "href": "projects/01_MachineLearningIris.html#unsupervised-clustering-kmeans",
    "title": "01 Machine Learning",
    "section": "Unsupervised: Clustering (kmeans)",
    "text": "Unsupervised: Clustering (kmeans)\n\n\nCode\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3)\nlabels = kmeans.fit_predict(Xs)\n\ndf_scaled[\"cluster\"] = labels.astype(str)\nsns.pairplot(df_scaled, hue=\"cluster\", height=1.5, aspect=1.0)"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#unsupervised-principle-component-analysis",
    "href": "projects/01_MachineLearningIris.html#unsupervised-principle-component-analysis",
    "title": "01 Machine Learning",
    "section": "Unsupervised: Principle Component Analysis",
    "text": "Unsupervised: Principle Component Analysis\n…. provides very convenient low-dimensional representation of the data\n\n\nCode\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(Xs)\nvar_exp = pca.explained_variance_ratio_\n\nxlab = f\"PC1 ({var_exp[0]:.3f})\"\nylab = f\"PC2 ({var_exp[1]:.3f})\"\nsns.scatterplot(x=X_pca[:,0], y=X_pca[:,1])\nplt.title(\"PCA plot\")\nplt.xlabel(xlab)\nplt.ylabel(ylab)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#supervised-linear-model-x-to-y",
    "href": "projects/01_MachineLearningIris.html#supervised-linear-model-x-to-y",
    "title": "01 Machine Learning",
    "section": "Supervised: Linear Model \\(x \\to y\\)",
    "text": "Supervised: Linear Model \\(x \\to y\\)\nUse PyTorch to define a linear model (with one input and one output), a suitable loss function, and an optimizer.\n\n\nCode\nimport numpy as np\nimport torch\nimport torch.nn as nn\nprint('torch version:', torch.__version__)\n\n# identical to the 1st lecture (same model)\nmodel = nn.Linear(1, 1) # n_in = 1, n_out = 1\nloss_func = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters())\n\n\ntorch version: 2.7.1"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#define-data",
    "href": "projects/01_MachineLearningIris.html#define-data",
    "title": "01 Machine Learning",
    "section": "Define Data",
    "text": "Define Data\nAgain we need some reformating (numpy \\(\\to\\) pytorch tensor) to make the data fit with the pytorch framework. We also need to define the input data (x_t) and output data (y_t). Their shape and dimensionality should correspond to the model defined above.\n\n\nCode\n# select two arbitrary variables as x=X[:,2] and y=X[:,3] \n# just for illustration - feel free to change\nx_t = torch.as_tensor(X[:,2], dtype=torch.float32).view(-1, 1)\ny_t = torch.as_tensor(X[:,3], dtype=torch.float32).view(-1, 1)\nprint(f\"shape(x_t): {x_t.shape}\")\nprint(f\"shape(y_t): {y_t.shape}\")\n\n\nshape(x_t): torch.Size([150, 1])\nshape(y_t): torch.Size([150, 1])"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#train-the-model",
    "href": "projects/01_MachineLearningIris.html#train-the-model",
    "title": "01 Machine Learning",
    "section": "Train the Model",
    "text": "Train the Model\nChose a number of iterations (e.g 100 “epochs”) and keep track of the loss at each iteration.\n\n\nCode\nlosses = []\nmodel.train()\nfor epoch in range(200):\n    y_pred = model(x_t)\n    loss = loss_func(y_pred, y_t)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    losses.append(loss.item()) # keep track of losses\n\n# finished training. inspect model \nprint('Mean Squared Error (loss):    ',loss.item())\nprint('Parameters:')\nfor name, p in model.named_parameters():\n    print(name, p.detach())\n\n\nMean Squared Error (loss):     0.19705839455127716\nParameters:\nweight tensor([[0.2114]])\nbias tensor([0.5649])\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n“weight” = slope parameter\n“bias” = intercept"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#inspect-losses",
    "href": "projects/01_MachineLearningIris.html#inspect-losses",
    "title": "01 Machine Learning",
    "section": "Inspect losses",
    "text": "Inspect losses\nPlot the losses against the number of iterations an check if they are reducing\n\n\nCode\nplt.figure()\nplt.plot(losses)\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#predicitons",
    "href": "projects/01_MachineLearningIris.html#predicitons",
    "title": "01 Machine Learning",
    "section": "Predicitons",
    "text": "Predicitons\nUse the trained model to make predction for 50 new \\(x\\)-values in a range \\((-10,10)\\). Overlay the model predictions with the original data on which the model was learned.\n\n\nCode\n# pick any range of new x-values (meaningful?)\nx_new = np.arange(-10,10,0.5)\n# convert to tensor\nx_new_t = torch.as_tensor(x_new, dtype=torch.float32).view(-1, 1)\n# run predictions\nyp  = model(x_new_t)\n\n# convert from tensor to numpy and reshape\ny_pred = yp.detach().numpy().flatten()\n\nplt.scatter(x_t, y_t, label=\"Data\")\nplt.plot(x_new,y_pred, c=\"orange\", label=\"Model\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Critique\n\n\n\n\nthere is no perfect fit because\n\nlearning has not yet converged\nreal data is noisy MSE \\(\\ne 0\\)\n\nModels are ignorant: What is the Petal.Width for Petal.Length = -1cm ?\nAssessments of predicted performance should invoke some left out data (test set)\nModels are not necessarily causal\nall models are wrong, some are useful"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#multivariate-multinomial-regression",
    "href": "projects/01_MachineLearningIris.html#multivariate-multinomial-regression",
    "title": "01 Machine Learning",
    "section": "Multivariate Multinomial Regression",
    "text": "Multivariate Multinomial Regression\n… with test-train split\n\n\nCode\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n\nX = torch.tensor(iris.data, dtype=torch.float32) # numeric measurements\nY = torch.tensor(iris.target, dtype=torch.long)  # class labels\nprint(f\"X.shape = {X.shape}, Y.shape = {Y.shape}\")\n\n# define train and test data sets\nn_samples = X.shape[0] # number of all samples\nn_train = int(0.8 * n_samples) # number of training samples\nperm = torch.randperm(n_samples) # shuffled random indicies\ni_train = perm[:n_train] # train indices\ni_test = perm[n_train:] # test indices\n\nmodel = nn.Linear(4, 3) # n_in = 4, n_out = 3\nloss_func = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters())\n\nmodel.train()\nfor epoch in range(5000):\n    yp = model(X[i_train])\n    loss = loss_func(yp, Y[i_train])\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nmodel.eval()\nz = model(X[i_test])\ny_pred_class = torch.argmax(z, dim=1)\ny_true_class = Y[i_test].numpy()\nacc = accuracy_score(y_true_class, y_pred_class)\nprint('Accuracy: ', acc)   \n\ncm = confusion_matrix(y_true_class, y_pred_class)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=iris.target_names)\ndisp.plot(cmap='Blues')\n\n\nX.shape = torch.Size([150, 4]), Y.shape = torch.Size([150])\nAccuracy:  0.9333333333333333"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neural Networks and Deep Learning",
    "section": "",
    "text": "Upon successful completion of this course, you will\n\nunderstand the key concepts and challenges of neural networks\nbe proficient in common software solutions for deep learning (pytorch)\nbe able to define, train and evaluate your own models\nhave completed 4 concrete data analysis projects and provide reproducible code\nhave worked in teams and presented your work\nwant to learn more and deeper … open"
  },
  {
    "objectID": "index.html#objectives",
    "href": "index.html#objectives",
    "title": "Neural Networks and Deep Learning",
    "section": "",
    "text": "Upon successful completion of this course, you will\n\nunderstand the key concepts and challenges of neural networks\nbe proficient in common software solutions for deep learning (pytorch)\nbe able to define, train and evaluate your own models\nhave completed 4 concrete data analysis projects and provide reproducible code\nhave worked in teams and presented your work\nwant to learn more and deeper … open"
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "Neural Networks and Deep Learning",
    "section": "Content",
    "text": "Content\n\nMachine Learning 101\nNeural Networks\nConvolutional Neural Networks\nGenerative Networks"
  },
  {
    "objectID": "index.html#format",
    "href": "index.html#format",
    "title": "Neural Networks and Deep Learning",
    "section": "Format",
    "text": "Format\nThe course comprises the following blocks (with the estimated time effort)\n\nLectures: (16x 2h) will provide an introduction and overview\nProjects: 4 larger assignments that are solved in teams of 2 students. Each team will submit a fully documented and executable notebook per project.\nTutorials: (16x 2h): weekly tutorials will focus on practical implementations and the review of lecture material. Up to 2 teams will present their project solutions (~30 min / team)."
  },
  {
    "objectID": "index.html#evaluation",
    "href": "index.html#evaluation",
    "title": "Neural Networks and Deep Learning",
    "section": "Evaluation",
    "text": "Evaluation\n\nProject Submissions (28 P). Completed projects are submitted as single jupyter noteboooks and are evaluated per team. Notebooks have to be fully documented to explain both the analysis goals and the code. They should clearly state software dependencies and be fully executable.\nProject Presentation (12 P). Presentation and submission of a reproducible data analysis project in an executable markdown format.\nExam (60 P): this will be a written individual exam with conceptual questions, including multiple choice and short essays."
  },
  {
    "objectID": "index.html#materials-and-references",
    "href": "index.html#materials-and-references",
    "title": "Neural Networks and Deep Learning",
    "section": "Materials and References",
    "text": "Materials and References\n\nClassic Books (Theory & Maths)\n\nGoodfellow et al (2016)\nBishop & Bishop (2024)\n\nClassic Courses (YouTube)\n\nAndrew Ng @ Coursera\nStandford CS230, 2018\n\nPytorch Tutorials (Practical)\nHuggingFace Learning (Next Steps)"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#define-class-label",
    "href": "homework/traditional_ml_to_generative.html#define-class-label",
    "title": "Machine Learning cycle",
    "section": "2. Define class label",
    "text": "2. Define class label"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#visualize-px",
    "href": "homework/traditional_ml_to_generative.html#visualize-px",
    "title": "Machine Learning cycle",
    "section": "3. Visualize p(x)",
    "text": "3. Visualize p(x)\n\n\nCode\nplt.hist(x, bins=40, density=True, alpha=0.6, label=\"p(x) empirical\")\nplt.title(\"Complicated marginal p(x)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"density\")\nplt.legend()\nplt.show()\n\nplt.scatter(y, x, c=y, cmap=\"coolwarm\", alpha=0.6)\nplt.xlabel(\"x\")\nplt.ylabel(\"latent z (true)\")\nplt.title(\"GMM clustering of x\")\nplt.show()"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#unsupervised-clustering-gmm",
    "href": "homework/traditional_ml_to_generative.html#unsupervised-clustering-gmm",
    "title": "Machine Learning cycle",
    "section": "4. Unsupervised clustering (GMM)",
    "text": "4. Unsupervised clustering (GMM)\n\n\nCode\nfrom sklearn.mixture import GaussianMixture\n\ngmm = GaussianMixture(n_components=2).fit(X)\nclusters = gmm.predict(X)\n\nplt.scatter(x, z, c=clusters, cmap=\"coolwarm\", alpha=0.6)\nplt.xlabel(\"x\")\nplt.ylabel(\"latent z (true)\")\nplt.title(\"GMM clustering of x\")\nplt.show()"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#supervised-logistic-regression",
    "href": "homework/traditional_ml_to_generative.html#supervised-logistic-regression",
    "title": "Machine Learning cycle",
    "section": "5. Supervised logistic regression",
    "text": "5. Supervised logistic regression\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\nclf = LogisticRegression().fit(X_train, y_train) # claim: y ~ f(X)\ny_pred = clf.predict(X_test)\ny_prob = clf.predict_proba(X_test)[:, 1]"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#evaluation-classification-report-and-roc-curve",
    "href": "homework/traditional_ml_to_generative.html#evaluation-classification-report-and-roc-curve",
    "title": "Machine Learning cycle",
    "section": "6. Evaluation (classification report and ROC curve)",
    "text": "6. Evaluation (classification report and ROC curve)\n\n\nCode\nfrom sklearn.metrics import classification_report, roc_curve, auc\n\nprint(classification_report(y_test, y_pred))\n\nfpr, tpr, _ = roc_curve(y_test, y_prob)\nplt.plot(fpr, tpr, label=f\"AUC = {auc(fpr, tpr):.2f}\")\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC curve\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#feature-interpretation-logistic-coefficients",
    "href": "homework/traditional_ml_to_generative.html#feature-interpretation-logistic-coefficients",
    "title": "Machine Learning cycle",
    "section": "7. Feature interpretation (logistic coefficients)",
    "text": "7. Feature interpretation (logistic coefficients)\n\n\nCode\nprint(f\"Logistic coef: {clf.coef_}, intercept: {clf.intercept_}\")"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#approximate-inverse-estimate-z-from-x",
    "href": "homework/traditional_ml_to_generative.html#approximate-inverse-estimate-z-from-x",
    "title": "Machine Learning cycle",
    "section": "8. Approximate inverse: estimate z from x",
    "text": "8. Approximate inverse: estimate z from x\n\n\nCode\nfrom scipy.stats import norm, rankdata\n\n# Empirical CDF of x\nranks = rankdata(x)\nempirical_cdf = ranks / (len(x) + 1)\nz_hat = norm.ppf(empirical_cdf)\n\n# Compare distribution\nplt.hist(z_hat, bins=40, density=True, alpha=0.6, label=\"z_hat ~ N(0,1)\")\nx_vals = np.linspace(-3, 3, 200)\nplt.plot(x_vals, norm.pdf(x_vals), 'k--', label=\"True N(0,1)\")\nplt.legend()\nplt.title(\"Transformed variable z_hat from x\")\nplt.show()\n\n\n\n\nCode\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(PolynomialFeatures(degree=10), LinearRegression())\nmodel.fit(z_hat.reshape(-1, 1), x)\n\nz_test = np.linspace(-3, 3, 300)\nx_pred = model.predict(z_test.reshape(-1, 1))\n\nplt.plot(z_test, x_pred, label=\"Learned g(z)\")\nplt.scatter(z, x, alpha=0.2, s=10, label=\"True (z, x)\")\nplt.xlabel(\"z\")\nplt.ylabel(\"x\")\nplt.title(\"Learned inverse mapping z → x\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#generate-new-data-using-z-n0-1",
    "href": "homework/traditional_ml_to_generative.html#generate-new-data-using-z-n0-1",
    "title": "Machine Learning cycle",
    "section": "9. Generate new data using z ~ N(0, 1)",
    "text": "9. Generate new data using z ~ N(0, 1)\n\n\nCode\nz_new = np.random.normal(0, 1, 500)\nepsilon_new = np.random.normal(0, 0.1, 500)\nx_gen = np.sin(3 * z_new) + 0.3 * z_new + epsilon_new\n\nplt.hist(x_gen, bins=40, density=True, alpha=0.6, label=\"Generated x from z\")\nplt.hist(x, bins=40, density=True, alpha=0.4, label=\"Original x\")\nplt.legend()\nplt.title(\"Compare p(x) and generated samples\")\nplt.show()"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#summary",
    "href": "homework/traditional_ml_to_generative.html#summary",
    "title": "Machine Learning cycle",
    "section": "10. Summary",
    "text": "10. Summary\nThis example shows how a nonlinear, complex distribution p(x) can be constructed from a simple latent variable z ~ N(0,1). Without any neural networks, we explored: - Clustering with GMM - Binary classification with logistic regression - Evaluation and interpretability - Latent space approximation and data generation\nThis prepares the stage for introducing Autoencoders, VAEs, and GANs.\n\n\n\n\n\n\n\n\nConcept\nTraditional Method\nMapping to Deep Learning\n\n\n\n\nLatent variable z\nN(0,1)\nLatent code\n\n\nComplex p(x)\nsin(3z) + noise\nGenerator network\n\n\nInverse mapping x → z\nRidge regression\nEncoder network\n\n\nDimensionality reduction\nPCA, GMM\nAutoencoder\n\n\np(z) → x\ndeterministic function\nDecoder or GAN generator\n\n\nLearning p(x)\nKDE or histograms\nLikelihood via VAE, GANs\n\n\nSupervised y\nLogistic regression\nClassifier layer"
  },
  {
    "objectID": "homework/04_Autoencoder.html",
    "href": "homework/04_Autoencoder.html",
    "title": "01 Autoencoder",
    "section": "",
    "text": "Data Download: Obtain both train and test data for the MNIST data set of handwritten digits from torchvision.datasets\nData Massage: For numerical efficiency use pytorch dataloaders and split the dataset into train, test and validation. Check data values and if normalization is required.\nModel Definition: Define a simple autoencoder (model1) with 4 layers to reduce dimensions: 728 -&gt; 128 -&gt; 64 -&gt; 16 -&gt; 3 count dimensions and parameters\nGoal and Strategy: Define a loss function and a suitable optimizer\nModel Training: Train the model and track the following parameters for each epochs\n\n\nloss and validation loss\ntraining images, latent representations, reconstructred images\n\n\nModel Evaluation:\n\n\nplot traing and validation loss.\nFor the best model calculate loss on. test data\ncompare several real images and compare to their reconstruction\n\n\nLatent Assessment: Compare the latent representation of images with the PCA embedding\n\n\nhow much variability is encoded by the first 3 components of PCA(X) compared to PCA(Z)\n\n\n\nData Generation: For a given image, generate new images by changing selected latent variable in PCA space or latent space.\noptional: replace the simple autoencoder model1 by a neural network with 3 convolutional layers (model2)\n\n\ncount parameters and compare to model1\ncompare learning curves with model1\n\n\nexplore what happens if you reduce the size of the data set"
  },
  {
    "objectID": "homework/04_Autoencoder.html#tasks",
    "href": "homework/04_Autoencoder.html#tasks",
    "title": "01 Autoencoder",
    "section": "",
    "text": "Data Download: Obtain both train and test data for the MNIST data set of handwritten digits from torchvision.datasets\nData Massage: For numerical efficiency use pytorch dataloaders and split the dataset into train, test and validation. Check data values and if normalization is required.\nModel Definition: Define a simple autoencoder (model1) with 4 layers to reduce dimensions: 728 -&gt; 128 -&gt; 64 -&gt; 16 -&gt; 3 count dimensions and parameters\nGoal and Strategy: Define a loss function and a suitable optimizer\nModel Training: Train the model and track the following parameters for each epochs\n\n\nloss and validation loss\ntraining images, latent representations, reconstructred images\n\n\nModel Evaluation:\n\n\nplot traing and validation loss.\nFor the best model calculate loss on. test data\ncompare several real images and compare to their reconstruction\n\n\nLatent Assessment: Compare the latent representation of images with the PCA embedding\n\n\nhow much variability is encoded by the first 3 components of PCA(X) compared to PCA(Z)\n\n\n\nData Generation: For a given image, generate new images by changing selected latent variable in PCA space or latent space.\noptional: replace the simple autoencoder model1 by a neural network with 3 convolutional layers (model2)\n\n\ncount parameters and compare to model1\ncompare learning curves with model1\n\n\nexplore what happens if you reduce the size of the data set"
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#models",
    "href": "lectures/01_MachineLearning/03_Models101.html#models",
    "title": "Models 101",
    "section": "Models",
    "text": "Models\nGoal: Use some variables in the data to predict others.\n\n\n\n\n\n\n\n\n\nJargon Alert: “predictors” (“independent variables”, “features”, \\(X\\)) predict “responses” (“dependent variables”, \\(Y\\)).\nCausality Alert: a good, complex, deep or even perfect model does not mean that we found a causal connection \\(X \\to Y\\).\n\nMathematical Goal\nModel expected value \\(\\hat y_i\\) as a (linear) function of \\(x_i\\)\n\\[\n\\begin{array}{ll}\ny_i &= \\hat y_i + \\epsilon_i ~~~~~ i = 1 \\ldots n \\\\ \\\\\n\\hat y_i &= f(x_i, \\theta) = \\theta_0 + \\theta_1 x_i \\\\ \\\\\n\\epsilon_i &\\sim N(0, \\sigma^2)\n\\end{array}\n\\]\nand in matrix form (also for multiple variables)\n\n\n\nMatrix form of linear regression\n\n\n\\[\nY = X \\theta + \\epsilon\n\\]\n\n\n\n\n\n\nCompact notation is useful:\n\n\n\n\njargon: model matrix \\(X\\)\nextendable to multiple predictor variables \\(y_i=f(x_i, z_i, \\ldots)\\)\nextendable to categorical variables (dummy)\nsame intersect (\\(\\theta_0\\)) for all observation\nfocus on shapes and rules of matrix multiplication\nfor each observation: same parameters, different error terms\n\n\n\n\n\nLoss functions: Residual Sum of Squares\nFor \\(n\\) data points, choose parameter vector \\(\\theta\\) by ordinary least squares:\n\\[\nRSS(\\theta) = \\sum_{i=1}^{n} (y_i - \\hat y_i)^2 = \\sum_i \\epsilon_i^2 = \\epsilon^T \\epsilon \\to min\n\\]\n\n\nCode\nset.seed(42)\nxr &lt;- seq(-3, 3, by=0.1)  \n\n# Generate parameter combinations and linear functions over xr\nresult &lt;- params_func(5, pr=c(-2,2,-2,2,0,0), xr=xr)\n\n# Create xy-plot\nplot_xy &lt;- result$func %&gt;%\n  ggplot(aes(x = x, y = y, color = point)) +\n  geom_line() +\n  labs(title = \"X-Y Space: Linear Models\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Create parameter space plot\nplot_param &lt;- result$params %&gt;%\n  ggplot(aes(x = beta0, y = beta1, color = point)) +\n  geom_point(size = 2) +\n  labs(title = \"Parameter Space: Models\", x = \"theta0\", y = \"theta1\") +\n  xlim(-5, 5) +  ylim(-5, 5) + \n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# generate data from quadratic function with one parameter combination + noise\n# for a range of x values in xr\nres &lt;- params_func(1, pr=c(1,1,2,2,0.5,0.5), xr=xr)\ndata &lt;- res$func %&gt;% mutate(yd = y + rnorm( n() ))\n\n# plot data\nplot_data &lt;- ggplot(data, aes(x = x, y = yd)) +\n  geom_point(alpha=0.3) +                                       # data\n  geom_line(aes(y = y), linetype = \"dashed\") +                  # true function\n  geom_smooth(method='lm', formula='y~x', colour=\"lightblue\") + # best linear fit\n  labs(title = \"X-Y Space: Sampled Data\", x=\"x\", y=\"y\") +\n  theme_minimal()\n\n# calculate Residual Sum of Squares for each combinations (beta0, beta1) in grid\nbeta0_seq &lt;- seq(-5, 5, length.out = 100)\nbeta1_seq &lt;- seq(-5, 5, length.out = 100)\ngrid &lt;- expand_grid(beta0 = beta0_seq, beta1 = beta1_seq) %&gt;%\n  mutate(RSS = map2_dbl(beta0, beta1, function(b0, b1) {\n    sum((data$yd - (b0 + b1 * xr))^2)\n  }))\n\n# Plot RSS + contour\nplot_rss &lt;- ggplot(grid, aes(x = beta0, y = beta1, z = RSS)) +\n  geom_tile(aes(fill = RSS)) +  geom_contour(color = \"grey\") +  \n  scale_fill_gradient(low = \"lightblue\", high = \"red\") +\n  labs(title = \"Parameter Space: RSS Contours\", x = \"theta0\", y = \"theta1\") + \n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nplot_xy + plot_param + plot_data + plot_rss + plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEach parameter combination corresponds to a specific (linear) model\nBetter models have lower RSS (= “cost” = “loss”)\nsmallest RSS corresponds to maximal likelihood: \\(P(Y|M)\\)\neven the “best” model may be wrong\n“all models are wrong, some are useful”"
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#goal-revisited",
    "href": "lectures/01_MachineLearning/03_Models101.html#goal-revisited",
    "title": "Models 101",
    "section": "Goal: revisited",
    "text": "Goal: revisited\nThe linear regression has minimzed the residual sum of squares \\(RSS\\). Below I visualize (in red) the remaining residual errors as deviations of the fitted line from the true data. I also compare it to the mean model where no dependency on X=Petal.Length is used.\n\n\nCode\nmu = iris$Petal.Width %&gt;% mean                      # simplest model\nmodel &lt;- lm(Petal.Width ~ Petal.Length, data=iris)  # linear model\n\np1 &lt;- iris %&gt;%\n  mutate(mean = mu) %&gt;%\n  ggplot(aes(x = 0, y = Petal.Width)) +\n  geom_point() +\n  geom_hline(yintercept=mu, colour=\"blue\") +\n  geom_segment(aes(xend = 0, yend = mean), color = \"red\") + \n  theme(\n    axis.title.x = element_blank(), \n    axis.text.x = element_blank(), \n    axis.ticks.x = element_blank()) +\n  ggtitle('Mean Model')\n\n\np2 &lt;- iris %&gt;%\n  mutate(predicted = predict(model, iris),\n         residuals = residuals(model)\n         ) %&gt;%\n  ggplot(aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point() +\n  geom_line(aes(y=predicted), color=\"blue\") +\n  geom_segment(aes(xend = Petal.Length, yend = predicted), color = \"red\") +  \n  ggtitle('Linear Model: Residual Sum of Squares')\n\np1 + p2 + plot_layout(widths = c(1, 3))"
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#model-fit-model-train",
    "href": "lectures/01_MachineLearning/03_Models101.html#model-fit-model-train",
    "title": "Models 101",
    "section": "Model Fit = Model Train",
    "text": "Model Fit = Model Train\n\n\n\n\n\n\nMathematical Solution\n\n\n\n\n\n\nMatrix Algebra for Expectations\nFor linear regression, \\(f(x,\\theta) =X \\theta\\), there is an explicit formula for the parameters \\(\\hat \\theta\\) and \\(\\hat \\sigma\\)\n\\[\n\\begin{array}{ll}\n\\hat {\\theta} &= (X^TX)^{-1} X^T Y\\\\\n\\hat {\\sigma}^2 &= \\epsilon^T \\epsilon / (n-p) = RSS /(n-p)\n\\end{array}\n\\]\n\n\nStatistical Analysis\nRemaining sampling fluctuation around \\(\\hat y\\) (measured by \\(\\hat \\sigma\\)) also induced uncertainties in the parameter estimate (error propagation)\n\\[\nCov(\\theta) = \\hat \\sigma (X^TX)^{-1}\n\\]\nGiven sampling fluctuations, and the corresponding uncertainty in the parameter estimate, we want to quantify our surprise for \\(\\hat {\\theta} \\ne 0\\) assuming that \\(\\theta=0\\). This can be done by a confidence interval\n\\[\n\\hat {\\theta} \\pm c \\cdot \\sigma_\\theta\n\\] There choice for \\(c\\) is arbitrary and it corresponds to our desired level of confidence that the “true” parameter \\(\\theta\\) is contained within this interval (\\(c=1.96\\) is a popular choice).\nOften we are interested if the confidence interval contains zero (no dependence). The p-value denotes the probability that zero is outside the region, even if the true value \\(\\theta=0\\)\n\n\n\n\n\n\n\n\n\n\nPractical Solutions\n\n\n\n\n\nEasy with R: lm()\n\n\nCode\n# fit a linear model\nmodel &lt;- lm(Petal.Width ~ Petal.Length, data=iris)  \n#model %&gt;% summary\n\n\nEasy with Python/sklearn: LinearRegression()\n\n\nCode\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\n\n# 1. Load iris data\niris_data = load_iris(as_frame=True)\nX = iris_data.data\n\n# 2. munge and reshape data\nx = iris['petal length (cm)']   # predictor\ny = iris['petal width (cm)']    # response \nx = x.to_numpy().reshape(-1, 1) # shape it as samples x features\n\n# 3. define model\nlm = LinearRegression()  \n\n# 4. fit / train model\nlm.fit(x, y)             \n\n# 5. report model & fit parameters\nprint(\"R² score:\", lm.score(x, y))\nprint('Fitted Parameters: ', lm.intercept_, lm.coef_)\n\nyp = lm.predict(x)                 # prediction\nMSE= mean_squared_error(y, yp)     # evaluate fit \nprint('Mean Squared Error: ', MSE)"
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#model-evaluations",
    "href": "lectures/01_MachineLearning/03_Models101.html#model-evaluations",
    "title": "Models 101",
    "section": "Model Evaluations",
    "text": "Model Evaluations\n\n\n\n\n\n\nWhat is a good model (“good fit”)\n\n\n\n\nRSS small \\(\\to\\) how small?\nslope \\(\\theta_1 \\ne 0\\) (confidence interval, small \\(P\\)-value)\ngood explanation of variance: large coefficient of determination \\(R^2\\)\ngood improvement of explained variance: large F-statistics (small \\(P\\)-value)\nvisualization\n\n\n\n\n\n\n\n\n\n\\(R^2\\) and \\(F\\)\n\n\n\n\n\nTotal Sum of Squares (Mean-only Model):\n\\[\nTSS = \\sum_i(y_i - \\bar y)^2 = ||Y - \\bar{Y}||^2\n\\]\nTwo components of variation: \\[\n\\begin{array}{ll}\n||Y - \\bar{Y}||^2 &= ||Y-\\hat{Y}||^2 + ||\\hat{Y} - \\bar{Y}||^2 \\\\\nTSS &= RSS + ESS\n\\end{array}\n\\]\nCoefficient of Determination: Fraction of variation explained by new model (\\(R^2\\)):\n\\[\nR^2 = \\frac{TSS-RSS}{TSS} = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i-\\bar{y})^2}\n\\]\nF-statistics \\[\nF = \\frac{(TSS-RSS)/(p-1)}{RSS/(n-p)}  \n\\]\n\n\n\n\n\n\n\n\n\nPoor Models \\(\\to\\) useless models"
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#model-predictions",
    "href": "lectures/01_MachineLearning/03_Models101.html#model-predictions",
    "title": "Models 101",
    "section": "Model Predictions",
    "text": "Model Predictions\n\nwith confidence intervals\nUncertainties in model parameters become uncertainties in prediction:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice\n\n\n\nWe model the expected mean and the confidence interval denotes the “standard deviation” of the mean, not the variability in the data\n\n\n\n\nwith prediction intervals\nThe variation of the data is clearly larger than suggested by the confidence interval shown above.\nIt is important to recognize that there are two components of variations:\n\\[\n||Y - \\bar{Y}||^2 = ||Y-\\hat{Y}||^2 + ||\\hat{Y} - \\bar{Y}||^2\n\\]\n\nthe variance explained by the linear model: \\(||\\hat{Y} - \\bar{Y}||^2\\) where \\(\\hat{Y} = X \\hat \\theta\\). In this context errors in \\(\\hat \\theta\\) will induce errors in Petal.Width.\nthe unexplained variance \\(||Y-\\hat{Y}||^2 = ||\\epsilon||^2\\) that remains unaccounted for, but we assume that \\(\\epsilon = (Y-\\hat{Y}) \\propto N(0,\\sigma^2)\\)\n\n\n\nCode\npred &lt;-predict(model, newdata=iris, interval = 'prediction' , level = 0.893)\n\ncbind(iris, pred) %&gt;%\n  ggplot(aes(x=Petal.Length)) + \n  geom_point(aes(y = Petal.Width, colour=Species)) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.2) +\n  geom_line(aes(y = fit)) +\n  ggtitle('Prediction Interval')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe prediction interval covers the broader variability of the data.\nBut as with the confidence interval, the level can be set at will.\nA common but arbitrary choice is 0.95\n\n\n\n\n\n\n\n\n\nPredictions with Linear Algebra\n\n\n\n\n\nThe confidence interval (CI) denotes the range within which we expect the expected means to lie\nThe prediction interval (PI) also includes the residual variance \\(\\sigma^2\\)\n\\[\n\\begin{array}{ll}\n\\hat y & = X\\theta = E[Y(X)] = \\mbox{expected mean}\\\\\nCov(\\hat \\theta) &= \\sigma^2 (X^TX)^{-1} \\\\\nCov(\\hat y) &= X \\cdot Cov(\\hat \\theta) \\cdot X^T = \\sigma^2 X(X^TX)^{-1} X^T \\\\\nSE(\\hat y) &= \\sqrt{diag(Cov(\\hat y))} \\\\\nCI:  & \\hat{y} \\pm ~t_{n-2} \\cdot SE(\\hat{y}) \\\\\nPI:  & \\hat{y}  \\pm ~t_{n-2} \\cdot \\sqrt{SE(\\hat{y}) + \\sigma^2}\n\\end{array}\n\\]\n\n\n\n\n\nPredicting Outside\nRegardless of prediction or confidence intervals, be careful when predicting (and interpreting) beyond the data range in which the model was trained/fitted\n\n\nCode\nmodel &lt;- lm(Petal.Width ~ Petal.Length, data=iris)                    # just in case we forgot\nnew_data &lt;- data.frame(Petal.Length = seq(0, 10, 0.1))                # create new data\npred &lt;- predict(model, newdata = new_data, interval = \"prediction\")   # + prediction interval (default)\npred &lt;- pred %&gt;% as.data.frame %&gt;% mutate(Petal.Length=new_data$Petal.Length) # add Petal.Length \n\nggplot(data=iris, aes(x = Petal.Length)) +\n  geom_point(aes(y = Petal.Width)) +\n  geom_smooth(aes(y = Petal.Width), method = \"lm\", formula = y ~ x) +   # CI from geom_smooth\n  geom_ribbon(data = pred, aes(ymin = lwr, ymax = upr), alpha = 0.2) +  # PI from pred\n  theme_minimal() +\n  labs(title = \"Hic sunt dracones\")"
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#factorial-variables-as-predictors",
    "href": "lectures/01_MachineLearning/03_Models101.html#factorial-variables-as-predictors",
    "title": "Models 101",
    "section": "Factorial variables as predictors",
    "text": "Factorial variables as predictors\nIn the iris example the “Species” variable is a factorial (categorical) variable with 3 levels.\nOther typical examples: different experimental conditions or treatments.\n\n\nCode\nplot(Petal.Width ~ Species, data=iris)\n\n\n\n\n\n\n\n\n\nCode\n# can also be modelled with model matrix X\n#model_cat=lm(Petal.Width ~ Species, data=iris)\n#summary(model_cat)\n\n\nEach species level (dummy variable X) has their own expected value \\(y\\).\nRequires choice of dummy encoding and reference level."
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#model-checking",
    "href": "lectures/01_MachineLearning/03_Models101.html#model-checking",
    "title": "Models 101",
    "section": "Model Checking",
    "text": "Model Checking\nLinear models \\(y_i=\\hat y_i + \\epsilon_i\\) make certain assumptions: \\(\\epsilon_i \\propto N(0,\\sigma^2)\\)\n\nresiduals \\(\\epsilon_i\\) are independent from each other (non-linear patterns?)\nresiduals are normally distributed\nhave equal variance \\(\\sigma^2\\) (“homoscedasticity”)\nno outliers (large residuals) or observations with strong influence on fit\n\nDifferent methods and software for diagnostic plots\n\n\nCode\nfit=lm(Petal.Width ~ ., data=iris)\nop=par(no.readonly=TRUE)   # safe only resettable graphical parameters, avoids many warnings\npar(mfrow=c(2,2))          # change graphical parameters: 2x2 images on device\nplot(fit,col=iris$Species) # four plots rather than one\n\n\n\n\n\n\n\n\n\nCode\npar(op)                    # reset graphical parameters"
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#review",
    "href": "lectures/01_MachineLearning/03_Models101.html#review",
    "title": "Models 101",
    "section": "Review",
    "text": "Review\n\ndependencies between variables can often be modeled\nfind best parameters and loss function (RSS)\nlinear models:\n\nfast analytical and computational solutions\nmany evaluation metrics and checking: \\(R^2, F, \\ldots\\)\n\nmay not be appropriate (complex relationships, \\(y\\) categorical)\n\ndangers:\n\ngood models not necessarily causal\nextrapolation beyond fit domains\n\nfuture\n\nbeyond linear models\nfocus on RSS, accuracy (and other loss functions)\nparameters less interesting"
  },
  {
    "objectID": "lectures/01_MachineLearning/05_MultinomialRegression.html",
    "href": "lectures/01_MachineLearning/05_MultinomialRegression.html",
    "title": "Multinomial Regression",
    "section": "",
    "text": "A neuron layer receiving multiple inputs - and calculating multiple outputs.\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport torch\nimport torch.nn as nn"
  },
  {
    "objectID": "lectures/01_MachineLearning/05_MultinomialRegression.html#generalization-more-output-variables",
    "href": "lectures/01_MachineLearning/05_MultinomialRegression.html#generalization-more-output-variables",
    "title": "Multinomial Regression",
    "section": "",
    "text": "A neuron layer receiving multiple inputs - and calculating multiple outputs.\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport torch\nimport torch.nn as nn"
  },
  {
    "objectID": "lectures/01_MachineLearning/05_MultinomialRegression.html#get-iris-data",
    "href": "lectures/01_MachineLearning/05_MultinomialRegression.html#get-iris-data",
    "title": "Multinomial Regression",
    "section": "Get Iris data",
    "text": "Get Iris data\n\n\nCode\n# get iris data from sklearn\niris = datasets.load_iris() \nX = iris.data # all four columns  \ny = iris.target  # forth column\n\n# convert to tensors\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.long)\nprint(f\"X.shape = {X.shape}, y.shape = {y.shape}\")\n\n\nX.shape = torch.Size([150, 4]), y.shape = torch.Size([150])"
  },
  {
    "objectID": "lectures/01_MachineLearning/05_MultinomialRegression.html#model-training",
    "href": "lectures/01_MachineLearning/05_MultinomialRegression.html#model-training",
    "title": "Multinomial Regression",
    "section": "Model Training",
    "text": "Model Training\n… requires minimal changes to pytorch code\n\n\nCode\nn_epochs = 10000\nmodel = nn.Linear(4, 3)  # change: n_in = 4 (features), n_out = 3 (classes)\nloss_func = nn.CrossEntropyLoss()  # Cross-Entropy\n\n# all below as before\noptimizer = torch.optim.SGD(model.parameters()) \n\nmodel.train()\nfor epoch in range(n_epochs):\n    # model predictions are interpreted as z = logits\n    z = model(X)  \n    loss = loss_func(z, y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(\"Fitted parameters:\")\nprint(\"Intercept (bias):\", model.bias.detach().numpy())\n\n## Notice more weights\nprint(\"Coefficients (weights):\", model.weight.detach().numpy())\nprint('Loss: ', loss)    \n\n\nFitted parameters:\nIntercept (bias): [ 0.4097849  -0.00864549  0.22877324]\nCoefficients (weights): [[ 0.31035995  0.61378133 -1.4084837  -0.27761978]\n [ 0.09734378 -0.3853656   0.26491988 -0.44041577]\n [-0.6314199  -0.75229895  1.0699543   0.51602477]]\nLoss:  tensor(0.3601, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "lectures/01_MachineLearning/05_MultinomialRegression.html#predict-and-plot",
    "href": "lectures/01_MachineLearning/05_MultinomialRegression.html#predict-and-plot",
    "title": "Multinomial Regression",
    "section": "Predict and Plot",
    "text": "Predict and Plot\nNow the model predictions \\(z\\) (logits) can be interpreted as probabilities, and the class with the maximal probability will be the class prediction.\n\n\n\n\n\n\nConverting logits to probabilities\n\n\n\n\n\n\\[\np_k = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}\n\\]\nThis function is also called softmax, because it basically gives the largest probability for largest logit \\(z_k\\) (for every sample)\nThe probabilities are normalized \\[\n\\sum_{k=1}^{K} p_k = 1\n\\]\n\n\n\n\n\n\n\n\n\nCross-entropy loss\n\n\n\nfor one sample \\[\n\\mathcal{L}(\\mathbf{y}, \\mathbf{p}) = -\\sum_{k=1}^{K} y_k \\log(p_k)\n\\]\nfor all samples\n\\[\nL = \\frac{1}{N} \\sum_i \\mathcal{L}(\\mathbf{y}_i, \\mathbf{p}_i)\n\\]\n\n\n\n\nCode\nmodel.eval()\nwith torch.no_grad():\n    z = model(X)\n    y_prob = z.softmax(dim=1) # for easier interpretation\n    y_pred = z.argmax(dim=1)  # class predition = largest logit\n    acc = (y_pred == y).float().mean().item()\n\nprint('y_true:     ', y[:3])\nprint('z (logits): ', z[:3])\nprint('y_prob:     ', y_prob[:3])\nprint('y_pred:     ', y_pred[:3])\n\nprint(f\"\\n\\nTraining accuracy: {acc:.3f}\")\n\n\ny_true:      tensor([0, 0, 0])\nz (logits):  tensor([[ 2.1135, -0.5782, -4.0234],\n        [ 1.7445, -0.4050, -3.5209],\n        [ 1.9460, -0.5280, -3.6521]])\ny_prob:      tensor([[0.9346, 0.0633, 0.0020],\n        [0.8915, 0.1039, 0.0046],\n        [0.9192, 0.0774, 0.0034]])\ny_pred:      tensor([0, 0, 0])\n\n\nTraining accuracy: 0.967"
  },
  {
    "objectID": "lectures/01_MachineLearning/05_MultinomialRegression.html#confusion-matrix",
    "href": "lectures/01_MachineLearning/05_MultinomialRegression.html#confusion-matrix",
    "title": "Multinomial Regression",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\n\nCode\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n\ncm = confusion_matrix(y, y_pred.numpy())\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=iris.target_names)\ndisp.plot(cmap='Blues')"
  },
  {
    "objectID": "lectures/01_MachineLearning/05_MultinomialRegression.html#classification-report",
    "href": "lectures/01_MachineLearning/05_MultinomialRegression.html#classification-report",
    "title": "Multinomial Regression",
    "section": "Classification Report",
    "text": "Classification Report\n\n\nCode\nreport = classification_report(y, y_pred.numpy(), target_names=iris.target_names)\nprint(report)\n\n\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        50\n  versicolor       1.00      0.90      0.95        50\n   virginica       0.91      1.00      0.95        50\n\n    accuracy                           0.97       150\n   macro avg       0.97      0.97      0.97       150\nweighted avg       0.97      0.97      0.97       150\n\n\n\nConfusion Matrix & Jargon (Wikipedia)"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "neural networks: layers of neurons connected by weight\nin each neuron: weights + bias + non-linear activation function\noptimization: gradient descent, SGD , momentum, ADAM\nbackprogation: gradients with chain rule on computational graph\ntechnical: vanishing gradients –&gt; batch norm, weight initialization\noverfitting and generalization: data augmentation, regularization (L2, dropout)"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#review",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#review",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "neural networks: layers of neurons connected by weight\nin each neuron: weights + bias + non-linear activation function\noptimization: gradient descent, SGD , momentum, ADAM\nbackprogation: gradients with chain rule on computational graph\ntechnical: vanishing gradients –&gt; batch norm, weight initialization\noverfitting and generalization: data augmentation, regularization (L2, dropout)"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#why-not-add-more-layers",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#why-not-add-more-layers",
    "title": "Convolutional Neural Networks",
    "section": "Why not add more layers",
    "text": "Why not add more layers\n\nNN with one hidden layer are universal function approximators\notpimization hard: finding parameters to give optimal decision boundary (for classification)\nno structure; just brute force\nblack boxes: parameter interpretation\nperformance plateau\n\n–&gt; more structure: - MLP: no structure –&gt; permutation of input no changes - CNN: spatial structure (images) - RNN: temporal structure (sequences) Transformers –&gt; from classifiers to generators"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#problem",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#problem",
    "title": "Convolutional Neural Networks",
    "section": "Problem:",
    "text": "Problem:\nfully connected layers: input(1000 x 1000 x 3 pixels) –&gt; 1 hidden layer(1000 neurons) –&gt; 3 billion parameters\nrestrict capacity of each layer –&gt; weight sharing\ninvariance assumption: should not matter where feature is\nCNN: start point for subsequence task - classification - object localization (bounding box) - object detection (multiple bounding boxes) - instance segmentation (point-wise classification)"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#what-are-convolutions",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#what-are-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "What are convolutions?",
    "text": "What are convolutions?\n\n1D\n–&gt; i2DL\n\n\n2D\n–&gt; i2DL"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#cnn-architecture",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#cnn-architecture",
    "title": "Convolutional Neural Networks",
    "section": "CNN Architecture",
    "text": "CNN Architecture\n\nNN: pass 1-D vectors from layer to layer\nCNN: match network to spatial structure (2D images)\n\nkeep input 2-D (actually: CxWxH) –&gt; also arrange neurons in 2-D (actually DxWxH)\n\nNumber of outputs from Convolutional Layer\n\\[\nN_{out} = \\frac{N_{in} - F + 2P}{S} + 1\n\\]\nNotice\n\n\\((N_{in} - F + 2P)/S\\) has to be integer for this to work properly.\n\n\n\nNumbers and Memory\n\n2-layer conv\n\n\n\n\n\n\n\n\n\n\n\n\nLayer\nOutput Shape\nParam Formula\n#Outputs\n#Params\nParam Mem (MB)\nOut Mem (MB)\n\n\n\n\nInput\n(1, 3, 224, 224)\n—\n150,528\n0\n0.00\n0.60\n\n\nConv1\n(1, 64, 224, 224)\n3×3×3×64 + 64\n3,211,264\n1,792\n0.01\n12.84\n\n\nMaxPool\n(1, 64, 112, 112)\n—\n802,816\n0\n0.00\n3.21\n\n\nConv2\n(1, 64, 112, 112)\n3×3×64×64 + 64\n802,816\n36,928\n0.15\n3.21\n\n\nFlatten\n(1, 802816)\n—\n802,816\n0\n0.00\n3.21\n\n\nFC\n(1, 10)\n802816×10 + 10\n10\n8,028,170\n32.11\n0.00\n\n\nTotal\n—\n—\n—\n8,066,890\n32.27\n23.07\n\n\n\n\n\nVggNet (2014)\n\n\n\n\n\n\n\n\n\n\n\n\nLayer\nOutput Shape\nParam Formula\n#Outputs\n#Params\nParam Mem (MB)\nOutput Mem (MB)\n\n\n\n\nInput\n224×224×3\n—\n150,528\n0\n0.00\n0.60\n\n\nConv1_1\n224×224×64\n3×3×3×64 + 64\n3,211,264\n1,792\n0.01\n12.84\n\n\nConv1_2\n224×224×64\n3×3×64×64 + 64\n3,211,264\n36,928\n0.15\n12.84\n\n\nMaxPool1\n112×112×64\n—\n802,816\n0\n0.00\n3.21\n\n\nConv2_1\n112×112×128\n3×3×64×128 + 128\n1,605,632\n73,856\n0.30\n6.42\n\n\nConv2_2\n112×112×128\n3×3×128×128 + 128\n1,605,632\n147,584\n0.59\n6.42\n\n\nMaxPool2\n56×56×128\n—\n401,408\n0\n0.00\n1.61\n\n\nConv3_1\n56×56×256\n3×3×128×256 + 256\n802,816\n295,168\n1.18\n3.21\n\n\nConv3_2\n56×56×256\n3×3×256×256 + 256\n802,816\n590,080\n2.36\n3.21\n\n\nConv3_3\n56×56×256\n3×3×256×256 + 256\n802,816\n590,080\n2.36\n3.21\n\n\nMaxPool3\n28×28×256\n—\n200,704\n0\n0.00\n0.80\n\n\nConv4_1\n28×28×512\n3×3×256×512 + 512\n401,408\n1,180,160\n4.72\n1.61\n\n\nConv4_2\n28×28×512\n3×3×512×512 + 512\n401,408\n2,359,808\n9.44\n1.61\n\n\nConv4_3\n28×28×512\n3×3×512×512 + 512\n401,408\n2,359,808\n9.44\n1.61\n\n\nMaxPool4\n14×14×512\n—\n100,352\n0\n0.00\n0.40\n\n\nConv5_1\n14×14×512\n3×3×512×512 + 512\n100,352\n2,359,808\n9.44\n0.40\n\n\nConv5_2\n14×14×512\n3×3×512×512 + 512\n100,352\n2,359,808\n9.44\n0.40\n\n\nConv5_3\n14×14×512\n3×3×512×512 + 512\n100,352\n2,359,808\n9.44\n0.40\n\n\nMaxPool5\n7×7×512\n—\n25,088\n0\n0.00\n0.10\n\n\nFC_1\n4096\n7×7×512×4096 + 4096\n4,096\n102,764,544\n411.06\n0.02\n\n\nFC_2\n4096\n4096×4096 + 4096\n4,096\n16,781,312\n67.13\n0.02\n\n\nFC_3\n1000\n4096×1000 + 1000\n1,000\n4,097,000\n16.39\n0.00\n\n\nTotal\n—\n—\n—\n138M+\n564 MB\n~77 MB\n\n\n\nNotice\n\nmemory and compute in early layers, parameters in last layer\nnumbers can more than double when the gradients are calculated + caching\nmaintain also multiple images (batch normalization)\n\n\n\nPython Implementation\nDefine Model:\n\n\nCode\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport numpy as np\n\nclass my_model(nn.Module):\n    \"\"\" \n    My simple Convolutional Network\n    input shape: [,3,224,224] \n    output shape: [,10]\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.flatten = nn.Flatten()\n        self.fc = nn.Linear(112 * 112 * 64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.conv2(x)\n        x = self.flatten(x) \n        x = self.fc(x)\n        return x\n\n\nNotice that each model has a defined input shape (e.g. [,3,224,224]) and a defined output (e.g [,10]). The inner layers should work such that the dimensions match.\nIdeally we also define pre-processing steps and transformers to adjust those dimensions when other data is provided.\nEmploy Model:\n\n\nCode\nmodel = my_model()\n#model = models.vgg16() # precompiled model from torchvision.models\n\n\nGet parameter dimensions\n\n\nCode\n# number and shapes of parameters can be obtained directly from model\n# there is also a package that could do the same\n# pip install torchsummary --&gt; torchsummary.summary(model, input_shape) \nfor name, layer in model.named_modules():\n    num_params = sum(p.numel() for p in layer.parameters())\n    print(f\"{layer.__class__.__name__:12} {num_params}\")\n\n# shapes of parameters: notice that we have weights and biases\nfor name, parameter in model.named_parameters():\n    print(name, parameter.shape)\n\n\nmy_model     8066890\nConv2d       1792\nMaxPool2d    0\nConv2d       36928\nFlatten      0\nLinear       8028170\nconv1.weight torch.Size([64, 3, 3, 3])\nconv1.bias torch.Size([64])\nconv2.weight torch.Size([64, 64, 3, 3])\nconv2.bias torch.Size([64])\nfc.weight torch.Size([10, 802816])\nfc.bias torch.Size([10])\n\n\nNotice that the number and shapes of parameters can be obtained directly from the model.\nIn contrast, the output sizes of each layer will depend on the dimension of the input data and can only be done at execution (forward pass)\nBelow I show how to use, hooks that allow for efficient manipulation of the forward path, such that all quantities of interest can be tracked. Here I will track the shapes\nDefine Hook\n\n\nCode\ndef describe_model_forward(model, input_tensor):\n    \"\"\" \n    describe_model_forward collects information on the shapes of parameters and outputs \n    on each computational layer of a neural network\n    \"\"\"\n    layer_info = []\n    hooks = []\n\n    # define a \"hook\" function that can be passed to the model\n    # and evaluate as each layer is run in the foward path\n    def register_hook(module):\n        def hook(module, input, output):\n            name = module.__class__.__name__\n            # assumption: output of model is single tensor\n            output_shape = tuple(output.shape)\n            param_shapes = [tuple(p.shape) for p in module.parameters() if p.requires_grad]\n\n            layer_info.append((name, output_shape, param_shapes))\n\n        # add hook to computational layers (not containers Sequential, ModuleList)\n        # Warning: layer and module is ALMOST synonymous, but modules may also be container of layers\n        # here we exclude them conditionally\n        # This was trial and error. I'm not sure if this exlcusion list is exhaustive in general\n        if not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList):\n            hooks.append(module.register_forward_hook(hook))\n\n    # Recursively add register_hook() function to each computational layer\n    model.apply(register_hook)\n\n    # Run forward pass\n    with torch.no_grad():\n        _ = model(input_tensor)\n\n    # Remove hooks to clean memory\n    for h in hooks:\n        h.remove()\n\n    return layer_info\n\n\nCollect Layer Information\n\n\nCode\nmodel = models.vgg16()\n# Define Data\nx = torch.randn(16, 3, 224, 224) # fake data ~ batch of images\n\n# Run forward path and collect information\nlayer_info = describe_model_forward(model, x)\n\n# Print out information\nfor (name, out_shape, param_shape) in layer_info:\n    # assume that there is always an output shape\n    n_out = int(np.prod(out_shape))\n    n_params = 0\n    # parameters shape maybe empty (e.g ReLU --&gt; 0 params)\n    if len(param_shape) == 2:\n        n_weights = np.prod(param_shape[0])\n        n_bias = np.prod(param_shape[1])\n        n_params = int(n_weights + n_bias)\n\n    print(f\"Layer {name:&lt;14} \\\n        output: {str(out_shape):&lt;25} {n_out:&lt;10,}\\\n        Parameters: {str(param_shape):&lt;15} {n_params:,}\"\n    )\n\n\nLayer Conv2d                 output: (16, 64, 224, 224)        51,380,224        Parameters: [(64, 3, 3, 3), (64,)] 1,792\nLayer ReLU                   output: (16, 64, 224, 224)        51,380,224        Parameters: []              0\nLayer Conv2d                 output: (16, 64, 224, 224)        51,380,224        Parameters: [(64, 64, 3, 3), (64,)] 36,928\nLayer ReLU                   output: (16, 64, 224, 224)        51,380,224        Parameters: []              0\nLayer MaxPool2d              output: (16, 64, 112, 112)        12,845,056        Parameters: []              0\nLayer Conv2d                 output: (16, 128, 112, 112)       25,690,112        Parameters: [(128, 64, 3, 3), (128,)] 73,856\nLayer ReLU                   output: (16, 128, 112, 112)       25,690,112        Parameters: []              0\nLayer Conv2d                 output: (16, 128, 112, 112)       25,690,112        Parameters: [(128, 128, 3, 3), (128,)] 147,584\nLayer ReLU                   output: (16, 128, 112, 112)       25,690,112        Parameters: []              0\nLayer MaxPool2d              output: (16, 128, 56, 56)         6,422,528         Parameters: []              0\nLayer Conv2d                 output: (16, 256, 56, 56)         12,845,056        Parameters: [(256, 128, 3, 3), (256,)] 295,168\nLayer ReLU                   output: (16, 256, 56, 56)         12,845,056        Parameters: []              0\nLayer Conv2d                 output: (16, 256, 56, 56)         12,845,056        Parameters: [(256, 256, 3, 3), (256,)] 590,080\nLayer ReLU                   output: (16, 256, 56, 56)         12,845,056        Parameters: []              0\nLayer Conv2d                 output: (16, 256, 56, 56)         12,845,056        Parameters: [(256, 256, 3, 3), (256,)] 590,080\nLayer ReLU                   output: (16, 256, 56, 56)         12,845,056        Parameters: []              0\nLayer MaxPool2d              output: (16, 256, 28, 28)         3,211,264         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 28, 28)         6,422,528         Parameters: [(512, 256, 3, 3), (512,)] 1,180,160\nLayer ReLU                   output: (16, 512, 28, 28)         6,422,528         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 28, 28)         6,422,528         Parameters: [(512, 512, 3, 3), (512,)] 2,359,808\nLayer ReLU                   output: (16, 512, 28, 28)         6,422,528         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 28, 28)         6,422,528         Parameters: [(512, 512, 3, 3), (512,)] 2,359,808\nLayer ReLU                   output: (16, 512, 28, 28)         6,422,528         Parameters: []              0\nLayer MaxPool2d              output: (16, 512, 14, 14)         1,605,632         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 14, 14)         1,605,632         Parameters: [(512, 512, 3, 3), (512,)] 2,359,808\nLayer ReLU                   output: (16, 512, 14, 14)         1,605,632         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 14, 14)         1,605,632         Parameters: [(512, 512, 3, 3), (512,)] 2,359,808\nLayer ReLU                   output: (16, 512, 14, 14)         1,605,632         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 14, 14)         1,605,632         Parameters: [(512, 512, 3, 3), (512,)] 2,359,808\nLayer ReLU                   output: (16, 512, 14, 14)         1,605,632         Parameters: []              0\nLayer MaxPool2d              output: (16, 512, 7, 7)           401,408           Parameters: []              0\nLayer AdaptiveAvgPool2d         output: (16, 512, 7, 7)           401,408           Parameters: []              0\nLayer Linear                 output: (16, 4096)                65,536            Parameters: [(4096, 25088), (4096,)] 102,764,544\nLayer ReLU                   output: (16, 4096)                65,536            Parameters: []              0\nLayer Dropout                output: (16, 4096)                65,536            Parameters: []              0\nLayer Linear                 output: (16, 4096)                65,536            Parameters: [(4096, 4096), (4096,)] 16,781,312\nLayer ReLU                   output: (16, 4096)                65,536            Parameters: []              0\nLayer Dropout                output: (16, 4096)                65,536            Parameters: []              0\nLayer Linear                 output: (16, 1000)                16,000            Parameters: [(1000, 4096), (1000,)] 4,097,000\nLayer VGG                    output: (16, 1000)                16,000            Parameters: [(64, 3, 3, 3), (64,), (64, 64, 3, 3), (64,), (128, 64, 3, 3), (128,), (128, 128, 3, 3), (128,), (256, 128, 3, 3), (256,), (256, 256, 3, 3), (256,), (256, 256, 3, 3), (256,), (512, 256, 3, 3), (512,), (512, 512, 3, 3), (512,), (512, 512, 3, 3), (512,), (512, 512, 3, 3), (512,), (512, 512, 3, 3), (512,), (512, 512, 3, 3), (512,), (4096, 25088), (4096,), (4096, 4096), (4096,), (1000, 4096), (1000,)] 0\n\n\n\n\n\nHabits and Recommendations\n\nuse input size \\(LxL = 2^n\\) (e.g. 512)\nuse stride \\(S=1\\)\nuse padding \\(P= (F-1)/2\\) to retain input size –&gt; multiple CONV layers\npooling: \\(F=2 S=2\\) (size reduction: \\(LxL --&gt; L/2xL/2\\) !!! - aggressive reduction,"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#illustration",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#illustration",
    "title": "Convolutional Neural Networks",
    "section": "Illustration:",
    "text": "Illustration:\nhttps://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#summary",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#summary",
    "title": "Convolutional Neural Networks",
    "section": "Summary",
    "text": "Summary\n\n\n\nExample from VGGNet. Image from A. Karpathy"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html",
    "href": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html",
    "title": "Engineering with Gradients",
    "section": "",
    "text": "Tip\n\n\n\nWant to make Gradient Descent faster and more robust"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#goal",
    "href": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#goal",
    "title": "Engineering with Gradients",
    "section": "",
    "text": "Tip\n\n\n\nWant to make Gradient Descent faster and more robust"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#use-mini-batches",
    "href": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#use-mini-batches",
    "title": "Engineering with Gradients",
    "section": "Use Mini-Batches",
    "text": "Use Mini-Batches\nCalcuating the gradient for the full dataset can be expensive.\n\nBreak task and data into mini-batches: \\(N_b &lt; N\\) (e.g. \\(N_b = 64\\))\ngradient estimates will be more noisy\n\n\\[\n\\frac{dL}{dW} \\approx g_t = \\frac{1}{N_b}\\sum_{i}^{N_b} \\frac{dL_i}{dW}\n\\]\nNotice: \\(dL/dW\\) denotes gradient vector for all parameters (weights and biases)\n\n\n\n\n\n\nStochastic Gradient Descent (SGD)\n\n\n\n\nmore rapid\nmore memory efficient\nescape shallow local minima\nnoisy updates as regularization\nearly convergence\nefficiency more important than exactness !"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#consider-momentum",
    "href": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#consider-momentum",
    "title": "Engineering with Gradients",
    "section": "Consider Momentum",
    "text": "Consider Momentum\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nw1 = np.linspace(-1, 1, 401)\nw2 = np.linspace(-1, 1, 401)\nW1, W2 = np.meshgrid(w1, w2)\nsigma1 = 0.5   # wide in w1\nsigma2 = 0.2   # narrow in w2\nZ = 1 - np.exp(-((W1**2) / (2 * sigma1**2) + (W2**2) / (2 * sigma2**2)))\n\nfig, ax = plt.subplots(figsize=(7, 5))\ncont = plt.contourf(W1, W2, Z, cmap=\"Reds\", levels=10)\nax.set_title(\"Loss Function (contour plot)\")\nax.set_xlabel(\"w1\")\nax.set_ylabel(\"w2\")\nplt.colorbar(cont, ax=ax, label=\"loss\")\nplt.show()\n\n\n\n\n\nNot all parameters are created equal.\n\n\n\n\n\ndifferent parameters can have different impact on loss function\nadjust learning rate or gradient depending on parameters (faster/slower learning)\nprevent overshooting/oscillation in “narrow” directions\n\n\n\n\n\n\n\nExponential weighted averages\n\n\n\nAverage gradient estimates over minibatch iterations \\(g_t \\to v_t\\)\n\\[\n\\begin{aligned}\nv_t &= \\beta_1 v_{t-1} + (1-\\beta_1) g_t \\\\\nW &= W - \\alpha v_t\n\\end{aligned}\n\\]\n\nsmoothed gradient (“velocity”)\nnew hyperparameter: e.g. \\(\\beta_1=0.9\\)\nweighted average over \\(1/(1-\\beta_1)\\) batches\ninclude friction term (\\(\\beta_1\\)) and acceleration (\\(1-\\beta\\))\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nx = np.linspace(0,64,65)\neps = np.random.normal(0, 0.2, size=x.shape)\n# simulate decay to true value (y=1)\nW = 1 + np.exp(-0.5*(x+eps))\nG = np.random.normal(1, 0.2, size=x.shape)\n\nbeta = 0.9\nG_smooth = np.zeros_like(G)\nG_smooth[0] = W[0]\nfor t in range(1, len(G)):\n    G_smooth[t] = beta * G_smooth[t-1] + (1 - beta) * G[t]\n\nplt.scatter(x,G, label=\"mini-batch\")\nplt.plot(x, G_smooth, color='orange',label=\"smoothed\")\nplt.title(f'Exponential Weighted Average (beta = {beta})')\nplt.xlabel('minibatch t')\nplt.ylabel('gradient')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nefficient implementation of averaging\nbias for small time \\(t\\) (early minibatches)\naverages out strong fluctuations"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#rmsprop-root-mean-square-prop",
    "href": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#rmsprop-root-mean-square-prop",
    "title": "Engineering with Gradients",
    "section": "RMSProp (root-mean-square prop)",
    "text": "RMSProp (root-mean-square prop)\nSimilar idea; scale gradients by their squared running average\n\\[\n\\begin{aligned}\nS_t &= \\beta_2 S_{t-1} + (1-\\beta_2) g_t^2 \\\\\nW & = W - \\alpha \\left(\\frac{g_t}{\\sqrt{S_t}}\\right)\n\\end{aligned}\n\\]\n\n\\(g_t^2\\): element-wise squares of gradient\nupdate direction of \\(W\\) with small gradients stronger than large gradients\ndampen oscillations in high gradient directions\nsmoother updates permit increased learning rate"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#adam-adaptive-moment-estimation",
    "href": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#adam-adaptive-moment-estimation",
    "title": "Engineering with Gradients",
    "section": "ADAM (Adaptive Moment Estimation)",
    "text": "ADAM (Adaptive Moment Estimation)\n\\[\n\\begin{aligned}\nv_t &= \\beta_1 v_{t-1} + (1-\\beta_1) g_t \\\\\nS_t &= \\beta_2 S_{t-1} + (1-\\beta_2) g_t^2 \\\\\nW & = W - \\alpha \\left(\\frac{v_t}{\\sqrt{S_t}}\\right)\n\\end{aligned}\n\\]\n\ncombines Momentum with RMSprop\nreal ADAM also contains some bias correction\nmost commonly used in modern application: (\\(\\beta_1 = 0.9, \\beta_2=0.999)\\), tune \\(\\alpha\\)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#learning-rate-decay",
    "href": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#learning-rate-decay",
    "title": "Engineering with Gradients",
    "section": "Learning Rate Decay",
    "text": "Learning Rate Decay\nReduce learning steps closer to minimum\n\\[\n\\alpha_t =  \\alpha_0 \\left(\\frac{1}{1+\\tau*{\\mbox epoch}}\\right)\n\\]\n\nDecay rate \\(\\tau\\)\ndifferent ways to specify decay: exponential, manually"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#local-minima-plateaus",
    "href": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#local-minima-plateaus",
    "title": "Engineering with Gradients",
    "section": "Local Minima & Plateaus",
    "text": "Local Minima & Plateaus\n(Local) Ninima: - curvature (second derivative) has to be positive into all directions - increasingly unlikely in high-dimensions\nSaddle Point:\n\ncurvature positive into some direction, negative in others\nincreasingly likely in high-dimensions\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nw1 = np.linspace(-3, 3, 400)\nw2 = np.linspace(-3, 3, 400)\nW1, W2 = np.meshgrid(w1, w2)\n\nZ = W1**2 - W2**2\n\nfig, ax = plt.subplots(figsize=(6, 5))\ncont = ax.contourf(W1, W2, Z, levels=30, cmap=\"RdBu_r\") \nplt.colorbar(cont, ax=ax, label=\"f(w1, w2)\")\n\nax.contour(W1, W2, Z, levels=[0], colors='black', linewidths=1.2)\nax.axhline(0, color='k', lw=0.8)\nax.axvline(0, color='k', lw=0.8)\n\nax.set_aspect('equal', 'box')\nax.set_xlabel(r\"$w_1$\")\nax.set_ylabel(r\"$w_2$\")\nax.set_title(\"Saddle Point:  $Loss = w_1^2 - w_2^2$\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIn high dimensions of parameter space it is more common to have saddle points rather than local minima\nMain concern is long plateaus = slow learning"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#parameter-initialization",
    "href": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#parameter-initialization",
    "title": "Engineering with Gradients",
    "section": "Parameter Initialization",
    "text": "Parameter Initialization\n\ndifferent initialization can lead to different solutions\navoid node symmetry: random initialization not zero\navoid vanishing gradients: no learning\navoid exploding gradients: unstable updates, divergent loss\nchose activation function\n\n\n\n\n\n\n\nDifferent Initializations at different layers:\n\n\n\nFor layer with \\(n_l\\) nodes \\[\n\\begin{aligned}\nz &= \\sum_i w_i x_i \\\\\nVar(z) &= n Var(w_i)\n\\end{aligned}\n\\]\n\nXavier Initialization (for tanh): \\(Var(w) = 1/n_l\\)\nKaiming Initialization (for ReLU); \\(Var(w) = 2/n_l\\)\nmany more: e.g. torch.nn.init"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#vectorization-gpu",
    "href": "lectures/02_NeuralNetworks/03_EngineeringWithGradients.html#vectorization-gpu",
    "title": "Engineering with Gradients",
    "section": "Vectorization & GPU",
    "text": "Vectorization & GPU\nMatrix Multiplication:\n\n\\(n\\) samples\n\\(k\\) input features\n\\(m\\) output features\n\n\\[\n\\begin{aligned}\nz &= X &\\cdot& W &+ b\\\\\n[n,m] &= [n,k] &\\cdot& [k,m] &+ [m]\n\\end{aligned}\n\\]\nBe aware of conventions and possible matrix transposition.\nUsing GPU:\n\n\nCode\nimport torch\nimport torch.nn as nn\n\n#detect GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = nn.Linear(10, 2).to(device) # model --&gt; Device\nx = torch.randn(16, 10).to(device)  # data --&gt; device\ny = model(x)                        # output on device\nprint(\"y.device:\", y.device)        # inspect device\ny_cpu = y.cpu()                     # change device"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/index.html",
    "href": "lectures/02_NeuralNetworks/index.html",
    "title": "02 Deep Neural Networks",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPerceptron\n\n\n\nperceptron\n\ndecision boundaries\n\nneuron\n\n\n\nBrain Connections\n\n\n\nThomas Manke\n\n\nSep 7, 2025\n\n\n\n\n\n\n\n\n\n\n\nNon-linearities\n\n\nInserting non-linearities in neural networks\n\n\n\nThomas Manke\n\n\nNov 10, 2025\n\n\n\n\n\n\n\n\n\n\n\nLoss and Gradients\n\n\n\nchain rule\n\nbackpropagation\n\ngradient descent\n\n\n\nChain rules on networks\n\n\n\nThomas Manke\n\n\nNov 10, 2025\n\n\n\n\n\n\n\n\n\n\n\nEngineering with Gradients\n\n\n\nstochastic gradient descent\n\nmomentum\n\nAdam\n\n\n\nImprovements for simple gradient descent\n\n\n\nThomas Manke\n\n\nNov 11, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html",
    "title": "Perceptron",
    "section": "",
    "text": "A neuron cell receives electrical signals (dendrites) and passes it to other neurons (axon). Source: wikipedia"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#real-neurons",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#real-neurons",
    "title": "Perceptron",
    "section": "",
    "text": "A neuron cell receives electrical signals (dendrites) and passes it to other neurons (axon). Source: wikipedia"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#neuron-models",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#neuron-models",
    "title": "Perceptron",
    "section": "Neuron Models",
    "text": "Neuron Models\n\n\n\nMcCulloch and Pitts (1943)\n\n\n\n\n\n\n\n\nA brief history of neuron++ models\n\n\n\n\nMcCulloch and Pitts (1943): Neurons as Boolean Gates (synaptic weights and activation thresholds)\nHebbs (1949): Neurons can learn weights; “fire together & wire together”\nRosenblatt (1957): A mathematical learning rule + a machine"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#an-application-binary-classification",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#an-application-binary-classification",
    "title": "Perceptron",
    "section": "An Application: Binary Classification",
    "text": "An Application: Binary Classification\n\n\n\nMark I Perceptron (1958). Distinguish pairs of letters (20x20 pixels) - with 80% accuracy!"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#perceptions-and-citations",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#perceptions-and-citations",
    "title": "Perceptron",
    "section": "Perceptions and Citations",
    "text": "Perceptions and Citations\n– Mechanisation of Thought Processes –\nRosenblatt (1957): “Devices of this sort are expected ultimately to be capable of concept formation, language translation, collation of military intelligence, and the solution of problems through inductive logic.”\n\n\n\nNew York Times, July 7 1958"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#boolean-gates-ai-winter",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#boolean-gates-ai-winter",
    "title": "Perceptron",
    "section": "Boolean Gates & AI Winter",
    "text": "Boolean Gates & AI Winter\nHow to adjust weights and thresholds to calculate Boolean functions?\n\n\n\nBoolean Gates with weights and thresholds. \\(x_1, x_2, y \\in \\{0,1\\}\\)."
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#an-algorithm",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#an-algorithm",
    "title": "Perceptron",
    "section": "An Algorithm",
    "text": "An Algorithm\nGoal given a Boolean truth table \\((x,y) \\to\\) find weights \\(w_l\\) and thresholds \\(b_l\\)\n\n\n\nPerceptron Calculation. A linear collection function + a non-linear activation function.\n\n\n\n\n\n\n\n\n\nLearning = updating weights\n\n\n\n\\[\nw_{l} \\to w_{l} + \\alpha (y - \\hat y)*x_l\n\\]\n\nall Boolean \\(x_l, y, \\hat y \\in \\{0, 1\\}\\)\nmodified Hebbs learning: weights are updated only if target \\(y\\) is not yet reached: \\(y \\ne \\hat y\\)\nonline learning: one sample at a time\nBUT: solution only for linearly separated data (not for XOR)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#classification",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#classification",
    "title": "Perceptron",
    "section": "Classification",
    "text": "Classification\nFor continuous \\(x\\), each neuron in a perceptron defines a (linear) decision boundary based on the value of \\(z\\).\n\\[\nz = w_1 x_1 + w_2 x_2 + b\n\\]\n\n\n\nDecision Boundary for continous \\(x\\)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#universality-with-multiple-layers",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#universality-with-multiple-layers",
    "title": "Perceptron",
    "section": "Universality with Multiple Layers",
    "text": "Universality with Multiple Layers\nWith a multilayer perceptron (MLP) it is possible to calculate XOR, or any Boolean function\n\n\n\nMulti-Layer-Perceptron to calculate XOR. Source: wikipedia\n\n\n\n\n\n\n\n\nBUT\n\n\n\n\nsimple perceptron algorithm only works for single layer and linearly separaberable data.\nneed new algorithm: Backpropagation (Rumelhart, Hinton, Williams; Nature 1986)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#complex-classification",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#complex-classification",
    "title": "Perceptron",
    "section": "Complex Classification",
    "text": "Complex Classification\nBelow I have hand-selected weights and thresholds for an MLP (with 3 hidden nodes) to create 3 decision boundaries that form a triangle.\nBased on this I have sampled points \\((x_1, x_2)\\) and assigned 2 labels (0=outside triangle, 1=inside triangle)\nLet’s explore if we can learn the known boundaries from just the data \\((x,y)\\).\nWe will discuss the algorithm later, but here I just want to illustrate that it seems to work.\n\n\nCode\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# some convenience functions\nfrom lecture_utils.helper import detect_device\nfrom lecture_utils.perceptron_utils import  plot_data_and_boundaries, plot_decision_boundary, get_data, train_model\n\n# define 3 decision boundaries with W and b\nW = np.array([[2,-1],[-2,-1],[0,1]])\nb = np.array([1,1,1]).reshape(1,-1)\n\n# sample data and assign labels based on known boundaries\nns = 10000\nX, labels = get_data(W, b, n_samples = ns)\n\n# plot data and boundaries\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\ntitle_str = f'True Boundaries and {ns} data points'\nplot_data_and_boundaries(ax, X.cpu(), labels.cpu(), W, b, title=title_str)\nplt.show()\n\n\n\n\n\nboundary parameters (w1, w2, b) specify boundary equation: w1 x1 + w2 x2 + b = 0\n\n\n\n\n\n\nCode\n# a simple model with minimal number of layers (see data generation)\nclass MinimalModel(nn.Module):\n    def __init__(self):\n        super(MinimalModel, self).__init__()\n\n        self.hidden_layer = nn.Linear(2, 3)\n        self.output_layer = nn.Linear(3, 1)\n\n    def forward(self, x):\n        x = torch.sigmoid(self.hidden_layer(x)) # sigmoid just for positivity\n        x = torch.sigmoid(self.output_layer(x)) # Sigmoid activation for classification\n        return x\n\n# a complex model with 2 hidden layers\nclass ComplexModel(nn.Module):\n    def __init__(self):\n        super(ComplexModel, self).__init__()\n\n        self.hidden1 = nn.Linear(2, 16)\n        self.hidden2 = nn.Linear(16,32)\n        self.output_layer = nn.Linear(32, 1)\n\n    def forward(self, x):\n        x = torch.sigmoid(self.hidden1(x))\n        x = torch.sigmoid(self.hidden2(x))\n        x = torch.sigmoid(self.output_layer(x)) # Sigmoid activation for classification\n        return x\n\n\n\n\nCode\ndevice = detect_device()\nn_epochs = 9000\nmodel = MinimalModel().to(device)\nX = X.to(device)\nlabels = labels.to(device)\nminimal_model, minimal_loss = train_model(model, X, labels, n_epochs=n_epochs, lr=0.001)\n\n# for the minimal model, we can actually interprete the parameters\n# so I store them here for later\nWp = minimal_model.state_dict()['hidden_layer.weight'].cpu().numpy()\nbp = minimal_model.state_dict()['hidden_layer.bias'].cpu().numpy().reshape(1,-1)\n\nmodel = ComplexModel().to(device)\ncomplex_model, complex_loss = train_model(model, X, labels, n_epochs=n_epochs, lr=0.001) \n\n\nEpoch [9000/9000], Loss: 0.0877\nEpoch [9000/9000], Loss: 0.0073\n\n\n\n\nCode\nfig, axes = plt.subplots(2, 2, figsize=(10, 5))\n\ntitle_str = f'True Boundaries and {ns} data points'\nplot_data_and_boundaries(axes[0][0], X.cpu(), labels.cpu(), W, b, title=title_str)\n\ntitle_str = f'Predicted Boundaries from Minimal Model'\nplot_data_and_boundaries(axes[0][1], X.cpu(), labels.cpu(), Wp, bp, title=title_str)\n\ntitle_str = f\"Minimal Model. Loss = {minimal_loss:.4f}\"\nplot_decision_boundary(axes[1][0], minimal_model, -2, 2, -2, 2, X, labels, title_str)\n\ntitle_str = f\"Complex Model. Loss = {complex_loss:.4f}\"\nplot_decision_boundary(axes[1][1], complex_model, -2, 2, -2, 2, X, labels, title_str)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nTrue Decision Boundaries and Predicted Decision Boundaries for 2 Models\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nfor the simple model (1 hidden layer with 3 neurons) we can still interprete the parameters as the linear boundaries.\ncomplex model (2 hidden layers with 16 + 32 neurons) is difficult to interpret: “black box”\ncomplex model “better”: smaller loss, higher confidence\nthe number of training iterations was quite high for both models (10,000)\nparameterization not unique (scaling factors and order)\n\n\n\nYuval Harari: “intelligence is not about the truth, it’s about the ability to solve problems”"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#universalities",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#universalities",
    "title": "Perceptron",
    "section": "Universalities",
    "text": "Universalities\nA multi-layered perceptron can\n\ncalculate any Boolean function\ncalculate any decision boundary (not just XOR and triangles)\napproximate any continuous function with arbitrary accuracy"
  },
  {
    "objectID": "help/03_BigData.html",
    "href": "help/03_BigData.html",
    "title": "Big Data",
    "section": "",
    "text": "… and common starters for deep learning.\n\n\n\nDataset\nYear\n# Samples\nFeatures per sample\nApprox. Size\nAuthors / Creators\n\n\n\n\n\nMNIST\n~1998‑1999 (released)\n60 000 train + 10 000 test = 70 000 images\n28×28 gray‑scale pixels (784 features)\n~50 MB\nYann LeCun, C. Cortes, C. Burges (via NIST)\n\n\n\nCIFAR‑10\n2009\n50 000 train + 10 000 test = 60 000 images\n32×32 color images (3×32×32 = 3 072 features)\n~170 MB\nAlex Krizhevsky, Vinod Nair, Geoffrey Hinton\n\n\n\nImageNet‑1K\n2009\n1 281 167 train + 50 000 validation + 100 000 test = ~1.43 M images\nHigh‑res RGB images (often resized to 256×256)\n~150 GB\nJia Deng, Li Fei‑Fei, et al. (ImageNet team)"
  },
  {
    "objectID": "help/03_BigData.html#famous-image-datasets",
    "href": "help/03_BigData.html#famous-image-datasets",
    "title": "Big Data",
    "section": "",
    "text": "… and common starters for deep learning.\n\n\n\nDataset\nYear\n# Samples\nFeatures per sample\nApprox. Size\nAuthors / Creators\n\n\n\n\n\nMNIST\n~1998‑1999 (released)\n60 000 train + 10 000 test = 70 000 images\n28×28 gray‑scale pixels (784 features)\n~50 MB\nYann LeCun, C. Cortes, C. Burges (via NIST)\n\n\n\nCIFAR‑10\n2009\n50 000 train + 10 000 test = 60 000 images\n32×32 color images (3×32×32 = 3 072 features)\n~170 MB\nAlex Krizhevsky, Vinod Nair, Geoffrey Hinton\n\n\n\nImageNet‑1K\n2009\n1 281 167 train + 50 000 validation + 100 000 test = ~1.43 M images\nHigh‑res RGB images (often resized to 256×256)\n~150 GB\nJia Deng, Li Fei‑Fei, et al. (ImageNet team)"
  },
  {
    "objectID": "help/03_BigData.html#data-resources",
    "href": "help/03_BigData.html#data-resources",
    "title": "Big Data",
    "section": "Data Resources",
    "text": "Data Resources\nSome popular links - personal bias and far from complete\n\n\n\nRepository / Platform\nCategories Covered\n# of Datasets (approx.)\nTotal Size (approx.)\n\n\n\n\n\nHugging Face Datasets\nNLP, vision, audio, multimodal\nThousands\nTens of TB\n\n\n\nRegistry of Open Data on AWS\nGenomics, earth science, satellite, healthcare\nMany hundreds\nHundreds of TB\n\n\n\nAwesome Public Datasets (GitHub)\nCurated lists across topics—including genomics, medical, climate\nHundreds (topics)\nVariable\n\n\n\nEuropean Genome‑phenome Archive (EGA)\nHuman genomics & phenotypic clinical data\n~4,500 studies from 1,000+ institutions\nTens to hundreds of PB\n\n\n\nThe Cancer Imaging Archive (TCIA)\nMedical imaging (cancer CT, MRI, PET)\nHundreds of collections; millions of images\nTens/hundreds of TB\n\n\n\nMedMNIST v2\nBiomedical images (2D/3D small‑scale)\n~718K images across 18 tasks\nTens of GB"
  },
  {
    "objectID": "help/01_Frameworks.html",
    "href": "help/01_Frameworks.html",
    "title": "Frameworks",
    "section": "",
    "text": "Machine Learning and Deep-Learning have been powered by a number of open frameworks to simplify all modeling steps:\nHere we review 3 popular frameworks: scikit-learn, pytorch, keras/tensorflow (not an exhaustive list)"
  },
  {
    "objectID": "help/01_Frameworks.html#goal-x-to-y-fx",
    "href": "help/01_Frameworks.html#goal-x-to-y-fx",
    "title": "Frameworks",
    "section": "Goal: \\(X \\to Y = f(X)\\)",
    "text": "Goal: \\(X \\to Y = f(X)\\)\nRemember the simple example from the lecture.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.array([-1,  0, 1, 2, 3, 4])     # Data x\ny = np.array([-3, -1, 1, 3, 5, 7])     # Answer y\n\nplt.plot(x,y,'o')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()"
  },
  {
    "objectID": "help/01_Frameworks.html#scikit-learn-2007",
    "href": "help/01_Frameworks.html#scikit-learn-2007",
    "title": "Frameworks",
    "section": "scikit-learn (2007++)",
    "text": "scikit-learn (2007++)\n\nPythonic ML-alternative to R\ndesigned for small datasets (in-memory)\nno GPU support\n\n\n\nCode\nimport sklearn\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nprint('sklearn version:', sklearn.__version__)\n\nlm = linear_model.LinearRegression()  # define model\nxr = x.reshape(-1,1)                  # reshape data for tool (samples as rows) --&gt; x[:, np.newaxis]\nlm.fit(xr, y)                         # fit c.f. R: lm(Y ~ X)\n\n# report fit\nprint(f\"Fitted Parameters {lm.intercept_}, {lm.coef_}\")\n\nyp = lm.predict(xr)                   # prediction\nMSE= mean_squared_error(y, yp)        # evaluate fit. other scores: R2=lm.score(xr, y)\nprint(f\"Mean Squared Error: {MSE}\")\n\n# predict y for some new x\nx_new=np.array([10, -40])\ny_new = lm.predict(x_new.reshape(-1,1))\n\nprint(f\"predictions:  {y_new}\")\n\n\nsklearn version: 1.7.1\nFitted Parameters -1.0, [2.]\nMean Squared Error: 0.0\npredictions:  [ 19. -81.]"
  },
  {
    "objectID": "help/01_Frameworks.html#tensorflowkeras-2015",
    "href": "help/01_Frameworks.html#tensorflowkeras-2015",
    "title": "Frameworks",
    "section": "TensorFlow/Keras (2015)",
    "text": "TensorFlow/Keras (2015)\n\nTensorFlow (Google Brain 2015): very complex\nKeras (F. Chollet 2015): user-friendly frontend to TF & other frameworks\nTF2: merged TF & Keras\nstrength: enterprise solutions & integration with Google Cloud/TPU\n\n\n\nCode\nimport tensorflow as tf\nprint('tf version:',tf.__version__)\n\n# define model - the \"black box\"\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.Input(shape=(1,))) # define input shape\nmodel.add(tf.keras.layers.Dense(units=1))\n\n# define optimization and loss\nmodel.compile(optimizer='sgd', loss='mean_squared_error') \n\n# fit model\nfit_history = model.fit(x,y, epochs=100, verbose=0)       \n\n# report fit\nprint('Fitted Parameters             ', model.trainable_variables)\nprint('Mean Squared Error (loss):    ', fit_history.history['loss'][-1])\n\n# make predictions\nx_new = np.array([ 10.0 , -40.0 ])\ny_new = model.predict(x_new)\ny_ana = -1 + 2.0*np.array(x_new)\n\nprint('analytical: ', y_ana)\nprint('numerical:  ', y_new.flatten())\n\n\n\ntf version: 2.18.0\n\nFitted Parameters              [&lt;Variable path=sequential/dense/kernel, shape=(1, 1), dtype=float32, value=[[1.8115696]]&gt;, &lt;Variable path=sequential/dense/bias, shape=(1,), dtype=float32, value=[-0.41584674]&gt;]\n\nMean Squared Error (loss):     0.19854427874088287\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step\n\nanalytical:  [ 19. -81.]\n\nnumerical:   [ 17.69985 -72.87863]"
  },
  {
    "objectID": "help/01_Frameworks.html#pytorch-facebook-2017",
    "href": "help/01_Frameworks.html#pytorch-facebook-2017",
    "title": "Frameworks",
    "section": "PyTorch (Facebook 2017)",
    "text": "PyTorch (Facebook 2017)\n\nPythonic alternative to Tensorflow 1.x\ndefault choice in AI research and community\nbacked by Linux Foundation\n\n\n\nCode\nimport torch\n#import torch.nn as nn\n#import torch.optim as optim\nprint('torch version:', torch.__version__)\n\n# Convert data to PyTorch tensors \n# define dtype and reshape to column vectors (c.f np.reshape())\nx_tensor = torch.tensor(x, dtype=torch.float32).view(-1, 1)\ny_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n\n# Define Model\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, 1)  # input dim = 1, output dim = 1\n)\n\n# Define Loss function\nloss_function = torch.nn.MSELoss()\n# Choose optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Fit model (explicit loop over epochs)\nloss_history = []\nmodel.train()             # put model in train mode \nfor epoch in range(100):\n\n    outputs = model(x_tensor)               # forward calculation\n    loss = loss_function(outputs, y_tensor) # loss calculation\n    loss_history.append(loss.item())        # append loss for tracking\n\n    # Backward pass\n    optimizer.zero_grad() # set .grad attribute in nn.Parameter to zero\n    loss.backward()       # backpropagation: calculate gradients and store in .grad\n    optimizer.step()      # update parameters: parm = parm - lr*parm.grad\n\n# Report best fit - convert to torch tensor to numpy\nfor name, param in model.named_parameters():\n    print(f\"{name}: {param.data.numpy().flatten()}\")\n\n# report last (=best?) loss from history\nprint(\"Mean Squared Error (loss):\", loss_history[-1])\n\n# make predictions\nx_new = np.array([10.0, -40.0])\nx_new_tensor = torch.tensor(x_new, dtype=torch.float32).view(-1, 1)\ny_new = model(x_new_tensor).detach().numpy()\n\ny_ana = -1 + 2.0 * np.array(x_new)\n\nprint(\"analytical: \", y_ana)\nprint(\"numerical:  \", y_new.flatten())\n\n\ntorch version: 2.6.0\n0.weight: [1.748867]\n0.bias: [-0.22144231]\nMean Squared Error (loss): 0.3526820242404938\nanalytical:  [ 19. -81.]\nnumerical:   [ 17.267227 -70.176125]\n\n\n\n\nCode\nplt.figure()\nplt.plot(fit_history.history['loss'][1:], label=\"TensorFlow\")\nplt.plot(loss_history, label=\"PyTorch\")\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "help/01_Frameworks.html#summary-table",
    "href": "help/01_Frameworks.html#summary-table",
    "title": "Frameworks",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\n\nFeature\nScikit-learn\nTensorFlow (TF)\nPyTorch\n\n\n\n\nInitial Release\n2010\n2015\n2017\n\n\nDeveloper\nINRIA, open-source community\nGoogle Brain\nFacebook AI Research (FAIR)\n\n\nLanguage\nPython (NumPy, SciPy)\nPython + C++ backend\nPython + C++ backend\n\n\nPrimary Use\nClassical ML\nDeep Learning + Production Pipelines\nDeep Learning + R&D\n\n\nModel Types\nTrees, SVMs, Linear Models, etc.\nNeural Networks\nNeural Networks\n\n\nGPU Support\n❌ No\n✅ Yes\n✅ Yes\n\n\nEase of Use\n✅ Very simple API\n⚠️ Steep TF1.x; better in TF2.x\n✅ Pythonic and intuitive\n\n\nCommunity Focus\nData Science / ML practitioners\nCloud, deployment, industry\nResearch, academia, experimental models\n\n\nIntegration\nPandas, NumPy, matplotlib\nTF Hub, TF Lite, Google Cloud\nHuggingFace\n\n\nGovernance\nCommunity-led (INRIA)\nGoogle\nLinux Foundation (since 2022)"
  },
  {
    "objectID": "help/index.html",
    "href": "help/index.html",
    "title": "Help",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nFrameworks\n\n\n\nsklearn\n\npytorch\n\ntensorflow\n\n\n\npowerful tools for neural networks\n\n\n\nThomas Manke\n\n\nOct 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nBig Data\n\n\n\nbig data\n\n\n\nFamous image sets and other Big Data\n\n\n\nThomas Manke\n\n\nOct 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytorch Starter\n\n\na basic intro\n\n\n\nThomas Manke\n\n\nNov 5, 2025\n\n\n\n\n\n\nNo matching items"
  }
]