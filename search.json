[
  {
    "objectID": "help/03_Pytorch.html",
    "href": "help/03_Pytorch.html",
    "title": "Pytorch Starter",
    "section": "",
    "text": "…using conda environments (also mamba/micromamba)\n# first define dedicated environment\nconda create -n pytorch_env python=3.10\nconda activate pytorch_env\n# from pytorch channel\nconda install pytorch torchvision torchaudio -c pytorch\n… or using pip\npip install pytorch"
  },
  {
    "objectID": "help/03_Pytorch.html#install",
    "href": "help/03_Pytorch.html#install",
    "title": "Pytorch Starter",
    "section": "",
    "text": "…using conda environments (also mamba/micromamba)\n# first define dedicated environment\nconda create -n pytorch_env python=3.10\nconda activate pytorch_env\n# from pytorch channel\nconda install pytorch torchvision torchaudio -c pytorch\n… or using pip\npip install pytorch"
  },
  {
    "objectID": "help/03_Pytorch.html#import",
    "href": "help/03_Pytorch.html#import",
    "title": "Pytorch Starter",
    "section": "Import",
    "text": "Import\n\n\nCode\nimport torch\nprint(f'torch version: {torch.__version__}')\n\n# new functionality/vocabulary (nouns and verbs): dir(torch) \n\n\ntorch version: 2.7.1"
  },
  {
    "objectID": "help/03_Pytorch.html#tensors",
    "href": "help/03_Pytorch.html#tensors",
    "title": "Pytorch Starter",
    "section": "Tensors",
    "text": "Tensors\n\nGeneration\n\n\nCode\nX = [[0,1,2], [3,4,5]] # list\nXt = torch.tensor(X)   # tensor\n\nprint(f'type(X): {type(X)}')\nprint(f'type(Xt): {type(Xt)}')\n\n#help(torch.tensor) # first aid\n#print(\"Xt address:\", Xt.data_ptr()) \n\n\ntype(X): &lt;class 'list'&gt;\ntype(Xt): &lt;class 'torch.Tensor'&gt;\n\n\n\n\nData Type and Type conversion\n\n\nCode\nprint(f'Xt.dtype): {Xt.dtype}') # single data type for elements of Xt\nYt = Xt.to(dtype=torch.float32) # shortcut: Xt.float()\nprint(f'Yt.dtype): {Yt.dtype}') \n\n\nXt.dtype): torch.int64\nYt.dtype): torch.float32\n\n\n\n\nAccess\n\n\nCode\nprint(Xt)\nprint(Xt[:,2]) # simple index, e.g. column=2\nind_select = torch.tensor([[1],[0]])      # complex index (along)\ntorch.gather(Xt, dim=1, index=ind_select) # pick [0,1] and [1,0]\n\n\ntensor([[0, 1, 2],\n        [3, 4, 5]])\ntensor([2, 5])\n\n\ntensor([[1],\n        [3]])\n\n\n\n\nAttributes\n\n\nCode\nprint(f'Xt.shape: {Xt.shape}')\nprint(f'Xt.dtype: {Xt.dtype}')\nprint(f'Xt.device: {Xt.device}')\n\n\nXt.shape: torch.Size([2, 3])\nXt.dtype: torch.int64\nXt.device: cpu\n\n\n\n\nMethods\n\n\nCode\nprint(f'Xt.numpy: {Xt.numpy()}') # converting\nprint(f'Xt.sum: {Xt.sum(dim=0)}') # summarizing along dimensions\nprint(f'Xt.argmax: {Xt.argmax(dim=1)}') # indices with largest value\n\n# many more attributes and methods: dir(Xt)\n\n\nXt.numpy: [[0 1 2]\n [3 4 5]]\nXt.sum: tensor([3, 5, 7])\nXt.argmax: tensor([2, 2])\n\n\n\n\nGeneration by shape\n\n\nCode\nshape=(3,4) # rows, columns\nprint('zeros: ', torch.zeros(shape))\nprint('rand: ', torch.rand(shape))\nprint('randn: ', 1 + 0.01*torch.randn(shape))\n\n\nzeros:  tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\nrand:  tensor([[0.2505, 0.1409, 0.8899, 0.7524],\n        [0.1809, 0.3016, 0.4057, 0.4317],\n        [0.1356, 0.9248, 0.5920, 0.5241]])\nrandn:  tensor([[1.0009, 0.9982, 0.9960, 0.9930],\n        [1.0066, 1.0008, 0.9987, 1.0007],\n        [0.9966, 1.0017, 1.0107, 1.0067]])\n\n\n\n\nReshaping\n\n\nCode\nXt = torch.tensor([[0,1,2], [3,4,5]] )\n\n# a special reshape = flatten\nprint(f'Xt.reshape(6): {Xt.reshape(6)}') \nprint(f'Xt.flatten: {Xt.flatten(start_dim=0)}') \n\n# more complicated reshapes\nYt = Xt.reshape(3,2)\nprint(f'Xt: {Xt}') \nprint(f'Yt: {Yt}') # \n\n\n# Warning: reshape() may not create a copy of Xt\n# Yt shares the same storage with Xt !!\nYt[0,0] = 42\nprint(f'Yt: {Yt}') #\nprint(f'Xt: {Xt}') # \n\n# to copy explicitly use clone()\n#Yt = Xt.clone().reshape(3, 2)\n\n\nXt.reshape(6): tensor([0, 1, 2, 3, 4, 5])\nXt.flatten: tensor([0, 1, 2, 3, 4, 5])\nXt: tensor([[0, 1, 2],\n        [3, 4, 5]])\nYt: tensor([[0, 1],\n        [2, 3],\n        [4, 5]])\nYt: tensor([[42,  1],\n        [ 2,  3],\n        [ 4,  5]])\nXt: tensor([[42,  1,  2],\n        [ 3,  4,  5]])\n\n\n\n\nPermuting\n\n\nCode\nXt = torch.arange(6).reshape(1,2,3) # 3D tensor with first direction 1-D\nprint(f'Xt: {Xt}')  \n\nprint(f'Xt: {Xt.shape}') # \nprint(f'Xt.squeeze: {Xt.squeeze().shape}') # squeezing\nprint(f'Xt.permute: {Xt.permute(2,0,1).shape}') # permuting\n#print(f'Xt.permute: {Xt.permute(2,0,1)}')\n\n\nXt: tensor([[[0, 1, 2],\n         [3, 4, 5]]])\nXt: torch.Size([1, 2, 3])\nXt.squeeze: torch.Size([2, 3])\nXt.permute: torch.Size([3, 1, 2])\n\n\n\n\nCombining\n\n\nCode\nXt = torch.tensor([[0,1,2], [3,4,5]] )\nprint(f'element_wise add {Xt + Xt}') \nprint(f'element_wise mult {Xt * Xt}') \nprint(f'matrix multiply {Xt.T @ Xt}') \n\n\nelement_wise add tensor([[ 0,  2,  4],\n        [ 6,  8, 10]])\nelement_wise mult tensor([[ 0,  1,  4],\n        [ 9, 16, 25]])\nmatrix multiply tensor([[ 9, 12, 15],\n        [12, 17, 22],\n        [15, 22, 29]])"
  },
  {
    "objectID": "help/03_Pytorch.html#devices",
    "href": "help/03_Pytorch.html#devices",
    "title": "Pytorch Starter",
    "section": "Devices",
    "text": "Devices\nPyTorch objects are associated with a device. If possible use GPU for speed.\n@GoogleColab or Kaggle: You may have to change the Runtime/Accelerator\n\n\nCode\ndef detect_device():\n    import torch\n    if torch.backends.mps.is_available():\n        # Mac\n        return \"mps\"\n    elif torch.cuda.is_available():\n        # NVIDIA\n        return \"cuda\"\n    else:\n        return \"cpu\"\n\ndevice = detect_device()\nprint('device: ', device)\n\n\ndevice:  mps\n\n\n\n\nCode\n# Initalize on device\nx = torch.tensor(\n    [3.0, 6.0, 9.0],\n    dtype=torch.float32,\n    device=device)\n\nprint(x)\n# transfer to CPU\nx=x.cpu()\nprint(x)\n\n# transfer to device\nx = x.to(device)\nprint(x)\n\n\ntensor([3., 6., 9.], device='mps:0')\ntensor([3., 6., 9.])\ntensor([3., 6., 9.], device='mps:0')"
  },
  {
    "objectID": "help/03_Pytorch.html#parameters",
    "href": "help/03_Pytorch.html#parameters",
    "title": "Pytorch Starter",
    "section": "Parameters",
    "text": "Parameters\n\n\nCode\nshape=(2,3)\nW = torch.rand(shape, requires_grad=True)\nprint(W.grad_fn)\n\n\nNone"
  },
  {
    "objectID": "help/03_Pytorch.html#autograd",
    "href": "help/03_Pytorch.html#autograd",
    "title": "Pytorch Starter",
    "section": "Autograd",
    "text": "Autograd\n\n\nCode\n# Lead nodes; define by used with\na = torch.tensor(2.0, requires_grad=True)\nb = torch.tensor(3.0, requires_grad=True)\nx = torch.tensor(1.0, requires_grad=True)\n\n# operations in computational graph: nodes z and L\nz = a + 2*b\nL = x*z  # L = x*(a+2b);  dL/dx = z; dL/dz = x\n\n# endows inner nodes (z and L) with gradient functions\nprint(f'z_grad = {z.grad_fn}')\nprint(f'L_grad = {L.grad_fn}')\n\n\nz_grad = &lt;AddBackward0 object at 0x323de64a0&gt;\nL_grad = &lt;MulBackward0 object at 0x323e42d40&gt;\n\n\n\n\nCode\na.grad = b.grad = x.grad = None  # clear old grads if any\nz.retain_grad() # per default pytorch keeps gradients only for leave nodes\nL.backward() # fill gradients with respect to L\n\nprint('x.grad:', x.grad) # dL/dx = z  = a + b = 5\nprint('a.grad:', a.grad) # dL/da = (dL/dz) (dz/da) = x * 1\nprint('b.grad:', b.grad) # dL/db = (dL/dz) (dz/db) = x * 1\nprint('z.grad:', z.grad) # dL/dz = x\n\n\nx.grad: tensor(8.)\na.grad: tensor(1.)\nb.grad: tensor(2.)\nz.grad: tensor(1.)"
  },
  {
    "objectID": "help/03_Pytorch.html#legoblocks-torch.nn-module",
    "href": "help/03_Pytorch.html#legoblocks-torch.nn-module",
    "title": "Pytorch Starter",
    "section": "Legoblocks: torch.nn module",
    "text": "Legoblocks: torch.nn module\n\nSimple Functions\n\n\nCode\nX = torch.tensor([-2.0, 0.0, 0.5, 1.0])\nprint('X           ', X)\nprint('sigmoid(X): ', torch.nn.functional.sigmoid(X)) # sigmoid function\nprint('softmax(X): ', torch.nn.functional.softmax(X, dim=0))   # X --&gt; p, \\sum X = 1\nprint('relu(X):    ', torch.nn.functional.relu(X))    # ReLU: max(0, X)\n\n# similar with classes\nrelu = torch.nn.ReLU() # create instance of relu class \nprint('relu(X):    ', relu(X))\n\n\nX            tensor([-2.0000,  0.0000,  0.5000,  1.0000])\nsigmoid(X):  tensor([0.1192, 0.5000, 0.6225, 0.7311])\nsoftmax(X):  tensor([0.0246, 0.1817, 0.2996, 0.4940])\nrelu(X):     tensor([0.0000, 0.0000, 0.5000, 1.0000])\nrelu(X):     tensor([0.0000, 0.0000, 0.5000, 1.0000])\n\n\n\n\nLinear Layers\n\n\nCode\nD_in = 1\nD_out = 3\n# create D_out x D_in matrix of parameters (randomly initialized)\nlinear_layer = torch.nn.Linear(D_in, D_out)\nprint('weight.shape: ', linear_layer.weight.shape)\nprint('weight matrix:', linear_layer.weight)\n\n# fake data\nX = torch.arange(6, dtype=torch.float32).reshape((-1,D_in))\nprint('X.shape: ', X.shape)\n\n# linear transform: y_hat = f(X) = X @ W.T + bias\ny_hat = linear_layer(X)\n\nprint('After linear transformation = matrix multiplication')\nprint('y_hat.shape: ', y_hat.shape)\n\n\nweight.shape:  torch.Size([3, 1])\nweight matrix: Parameter containing:\ntensor([[-0.5041],\n        [-0.0248],\n        [ 0.2453]], requires_grad=True)\nX.shape:  torch.Size([6, 1])\nAfter linear transformation = matrix multiplication\ny_hat.shape:  torch.Size([6, 3])\n\n\n\n\nEmbeddings\n\n\nCode\nnw = 10 # number of words\nk  = 3  # embedding dimension\n\nembedding_layer = torch.nn.Embedding(nw, k)\nwords_ids = torch.tensor([1, 3, 3, 0, 9]) # integer representation\nwords_vec = embedding_layer(words_ids)    # better representation ?\n\nprint('word vectors: ', words_vec)\n\n\nword vectors:  tensor([[-0.4440,  0.0272, -1.2057],\n        [-0.7078, -0.8524, -0.3536],\n        [-0.7078, -0.8524, -0.3536],\n        [-0.7424,  0.2300,  1.7273],\n        [-0.0244,  2.6445,  1.2962]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\n\nDropout Layer\n\n\nCode\nX = torch.ones(1,8)\n\ndropout_layer = torch.nn.Dropout(p=0.3) # set 30% to 0\ndropout_layer.train() # only use during training for robustness\n\nprint(X)\n# illustrate random drops and scaling\nfor i in range(6):\n    X_drop = dropout_layer(X)\n    print(X_drop)\n\n\ntensor([[1., 1., 1., 1., 1., 1., 1., 1.]])\ntensor([[1.4286, 1.4286, 0.0000, 1.4286, 0.0000, 1.4286, 1.4286, 1.4286]])\ntensor([[1.4286, 1.4286, 0.0000, 1.4286, 1.4286, 0.0000, 1.4286, 0.0000]])\ntensor([[0.0000, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286]])\ntensor([[1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286]])\ntensor([[0.0000, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 0.0000]])\ntensor([[1.4286, 1.4286, 1.4286, 0.0000, 1.4286, 0.0000, 1.4286, 0.0000]])\n\n\n\n\nBuilding Larger Models\nWe can define our own models from building blocks. Ultimately this will help to abstract from individual layers, their initialization, normalization, dropouts etc.\n\n\nCode\nimport torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self, n_in, n_out):\n        # inherit stuff from nn.Module\n        super().__init__()\n        # define layers: just some random examples\n        self.linear = nn.Linear(n_in, n_out)\n        self.norm = nn.Dropout(p=0.3)\n\n    # conncect layers defined above\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.norm(x)\n        x = nn.functional.sigmoid(x)\n        return x\n\nmodel = MyModel(10, 1)\nprint(model)\n\n\nMyModel(\n  (linear): Linear(in_features=10, out_features=1, bias=True)\n  (norm): Dropout(p=0.3, inplace=False)\n)"
  },
  {
    "objectID": "help/03_Pytorch.html#optimizer-and-loss-function",
    "href": "help/03_Pytorch.html#optimizer-and-loss-function",
    "title": "Pytorch Starter",
    "section": "Optimizer and Loss Function",
    "text": "Optimizer and Loss Function\n\n\nCode\nimport torch.optim as optim\n\nlearning_rate = 0.01 # hyperparameter\n\noptimizer = optim.Adam(model.parameters(), lr = learning_rate)\n\nloss_function = nn.MSELoss()"
  },
  {
    "objectID": "help/03_Pytorch.html#model.train-vs.-model.eval",
    "href": "help/03_Pytorch.html#model.train-vs.-model.eval",
    "title": "Pytorch Starter",
    "section": "model.train() vs. model.eval()",
    "text": "model.train() vs. model.eval()\ntwo different modes for training and evaluation\n\n\n\n\n\n\n\n\nFeature\nmodel.train() (Training Mode)\nmodel.eval() (Evaluation Mode)\n\n\n\n\nDropout\nRandomly disables neurons (adds noise)\nAll neurons active (no dropout)\n\n\nBatchNorm\nUses current batch stats (mean/var)\nUses running (saved) stats\n\n\nGradient Tracking\nYes\nUse with torch.no_grad() to disable\n\n\nWhen to use\nDuring training loop\nDuring validation/testing/inference\n\n\n\nTypical use:\n\n\nCode\n# train data\nn_samples, m_features = 100, 10\nx = torch.rand((n_samples, m_features))\ny = torch.bernoulli(x[:,0], 0.5).reshape(-1,1)\n\n# Training\nn_epochs = 10\nmodel.train()\nfor epoch in range(n_epochs):\n\n    output = model(x)           # forward path x -&gt; output\n\n    loss = loss_function(output, y) # loss calculation\n\n    optimizer.zero_grad() # reset gradients (do not accumulate)\n    loss.backward()       # backward path L -&gt; x (gradient calc)\n    optimizer.step()      # update parm = parm - learning_rate * grad\n\n    # print loss\n    if(epoch % 20 == 0):\n        print('epoch {}, loss {}'.format(epoch, loss.data))\n\n# Evaluation (Validation or Test or Prediction)\nx_val = torch.rand((n_samples, m_features))\ny_val = torch.bernoulli(x_val[:,0], 0.5).reshape(-1,1)\n\nmodel.eval()\n# disable grad calculation for speed\nwith torch.no_grad():\n    val_output = model(x_val)\n    val_loss = loss_function(val_output, y_val)\n\n\nepoch 0, loss 0.26598238945007324"
  },
  {
    "objectID": "help/03_Pytorch.html#saving-models",
    "href": "help/03_Pytorch.html#saving-models",
    "title": "Pytorch Starter",
    "section": "Saving models",
    "text": "Saving models\nYou may want to save (and share) your final model. But for long-running jobs it may also be good to save occasional checkpoints during training\n\n\nCode\nimport os\nsave_dir = \"checkpoints\"\nos.makedirs(save_dir, exist_ok=True)\n\n# 1. only saves state_dict, need to know model class\ntorch.save(model.state_dict(), os.path.join(save_dir, \"model_dict.pt\"))\nmodel.load_state_dict(torch.load(\"checkpoints/model_dict.pt\"))\n\n# 2. save full model. Warning: may not port across pytorch versions!!! \ntorch.save(model, os.path.join(save_dir, \"model_full.pth\"))\nmodel = torch.load(\"checkpoints/model_full.pth\", weights_only=False)"
  },
  {
    "objectID": "help/03_Pytorch.html#saving-histories",
    "href": "help/03_Pytorch.html#saving-histories",
    "title": "Pytorch Starter",
    "section": "Saving histories",
    "text": "Saving histories\n\n\nCode\n# 1. npz format (most general and robust)\nnp.savez(os.path.join(save_dir, \"loss_history.npz\"),\n         train=train_losses, val=val_losses)\n\ndata = np.load(\"checkpoints/loss_history.npz\")\ntrain_losses = data[\"train\"]\nval_losses = data[\"val\"]\n\n# 2. pickle format (more flexible less portable)\nwith open(os.path.join(save_dir, \"loss_history.pkl\"), \"wb\") as f:\n    pickle.dump({\"train\": train_losses, \"val\": val_losses}, f)\n\nwith open(\"checkpoints/loss_history.pkl\", \"rb\") as f:\n    losses = pickle.load(f)\ntrain_losses = losses[\"train\"]\nval_losses = losses[\"val\"]"
  },
  {
    "objectID": "help/03_Pytorch.html#saving-everything",
    "href": "help/03_Pytorch.html#saving-everything",
    "title": "Pytorch Starter",
    "section": "Saving Everything",
    "text": "Saving Everything\n\n\nCode\ntorch.save({\n    'model_state': model.state_dict(),\n    'optimizer_state': optimizer.state_dict(),\n    'train_losses': train_losses,\n    'val_losses': val_losses,\n}, os.path.join(save_dir, \"checkpoint_all.pt\"))\n\ncheckpoint = torch.load(\"checkpoints/checkpoint_all.pt\")\nmodel.load_state_dict(checkpoint['model_state'])\noptimizer.load_state_dict(checkpoint['optimizer_state'])\ntrain_losses = checkpoint['train_losses']\nval_losses = checkpoint['val_losses']"
  },
  {
    "objectID": "help/03_Pytorch.html#tensorboard",
    "href": "help/03_Pytorch.html#tensorboard",
    "title": "Pytorch Starter",
    "section": "Tensorboard",
    "text": "Tensorboard\ntensorboard is a popular tool to inspect neural networks and training progress\n\n\nCode\nfrom torch.utils.tensorboard import SummaryWriter\n\n# default `log_dir` is \"runs\" - we'll be more specific here\nwriter = SummaryWriter('log_dir/mnist_experiment')\n\n# typical uses\nwriter.add_graph(model)\nfor epoch in range(100):\n    # training loop\n    # ...\n    # train_loss = loss_function(logits, y)\n    writer.add_scalar(\"Loss\", train_loss, epoch)\n\nwriter.close()\n\n\n\n\nCode\n#echo: false\n#eval: false\n# pytorch in 1 hours: https://www.youtube.com/watch?v=r1bquDz5GGA"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/practical_tips.html#human-level-performance",
    "href": "lectures/02_NeuralNetworks/practical_tips.html#human-level-performance",
    "title": "Practical Tips",
    "section": "Human Level Performance",
    "text": "Human Level Performance"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/practical_tips.html#error-analysis",
    "href": "lectures/02_NeuralNetworks/practical_tips.html#error-analysis",
    "title": "Practical Tips",
    "section": "Error Analysis",
    "text": "Error Analysis"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/practical_tips.html#data-mismatch-trainig-test",
    "href": "lectures/02_NeuralNetworks/practical_tips.html#data-mismatch-trainig-test",
    "title": "Practical Tips",
    "section": "Data Mismatch (Trainig, Test)",
    "text": "Data Mismatch (Trainig, Test)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/practical_tips.html#small-data-transfer-learning",
    "href": "lectures/02_NeuralNetworks/practical_tips.html#small-data-transfer-learning",
    "title": "Practical Tips",
    "section": "Small Data: Transfer Learning",
    "text": "Small Data: Transfer Learning"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/practical_tips.html#multi-task-learning",
    "href": "lectures/02_NeuralNetworks/practical_tips.html#multi-task-learning",
    "title": "Practical Tips",
    "section": "Multi-task Learning",
    "text": "Multi-task Learning"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/practical_tips.html#end-to-end-learning",
    "href": "lectures/02_NeuralNetworks/practical_tips.html#end-to-end-learning",
    "title": "Practical Tips",
    "section": "End-to-end Learning",
    "text": "End-to-end Learning"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html",
    "title": "Backpropagation",
    "section": "",
    "text": "Find the gradients with respect to parameters. How much should paramters be changed to reduce the loss?"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#goal",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#goal",
    "title": "Backpropagation",
    "section": "",
    "text": "Find the gradients with respect to parameters. How much should paramters be changed to reduce the loss?"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#a-simple-network",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#a-simple-network",
    "title": "Backpropagation",
    "section": "A simple network",
    "text": "A simple network\n\n\n\nLogistic regression with two input \\((x_1, x_2\\)) and one output activation \\(a \\in [0,1]\\)\n\n\nThe network encodes a calculation that is applied to each sample"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#calculating-forward-computational-graph",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#calculating-forward-computational-graph",
    "title": "Backpropagation",
    "section": "Calculating Forward: Computational Graph",
    "text": "Calculating Forward: Computational Graph\n\n\n\nComputational Graph shows how data flows through a graph of basic operations (nodes)\n\n\nThe loss function: \\[\nL(a,y) = -y \\log(a) - (1-y)\\log(1-a)\n\\]"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#calculating-backward",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#calculating-backward",
    "title": "Backpropagation",
    "section": "Calculating Backward",
    "text": "Calculating Backward\nThe derivatives can be calculates \\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial a} &= -\\frac{y}{a} + \\frac{1-y}{1-a}\\\\ \\\\\n\\frac{\\partial L}{\\partial z} &= \\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial z} = \\frac{\\partial L}{\\partial a}a(1-a)\\\\ \\\\\n\\frac{\\partial L}{\\partial w_1} &= \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial w_1} = \\frac{\\partial L}{\\partial z} x_1\\\\\\\\\n\\frac{\\partial L}{\\partial w_2} &= \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial w_2} = \\frac{\\partial L}{\\partial z} x_2\\\\\\\\\n\\frac{\\partial L}{\\partial b} &= \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial b} = \\frac{\\partial L}{\\partial z} 1\n\\end{aligned}\n\\]\n\n\n\n\n\n\nBackward Calculation\n\n\n\n\nderivatives = lookup and multiplication\ndecompose larger calculation into smaller local parts\nstrict computational flow ensures that all required contributions are known\nimportance of differentiable activation functions: \\(\\frac{\\partial a}{\\partial z}\\)\nattaches a gradient to each forward value: \\(a \\to da \\equiv \\frac{\\partial L}{\\partial a}\\)\nuse matrices for book-keeping\nflexibility: other loss functions, other inputs, more layers\nthis is for one sample \\((\\mathbf{x},y)\\) only"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#summing-over-all-sample",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#summing-over-all-sample",
    "title": "Backpropagation",
    "section": "Summing over all sample",
    "text": "Summing over all sample\n\\[\nL = \\frac{1}{N} \\sum_i L(\\hat y_i, y_i)\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\nfor iid samples: maximizing log-likelihood\ngradients accummulate: e.g. \\(\\frac{\\partial L}{\\partial b} = \\sum_i \\frac{\\partial L_i}{\\partial b}\\)\nin practice this is done for **batches* of training samples"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#summary",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#summary",
    "title": "Backpropagation",
    "section": "Summary",
    "text": "Summary\n\n\n\nForward and Backward Propagation through layer \\(l\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ngradients are byproduct of backpropagation\nshapes of gradients agree with shape of tensors\nmatrix notation for simplification and vectorized speedup\nlookup and matrix multiplication"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#further-reading",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#further-reading",
    "title": "Backpropagation",
    "section": "Further reading",
    "text": "Further reading\n\n3Blue1Brown. Backpropagation.\nAutomatic Differentiation.\nAndrew Ng @ 6:23h\n\nobserved data: \\(x\\) true label: \\(y\\) predicted label probability: \\(\\hat y = f(x, \\theta)\\) \\[\np(y|x) = \\hat{y}^y (1-\\hat{y})^y \\to \\mbox{maximize !}\n\\]\nLoss for one sample \\(x\\) \\[\nL(\\hat y, y) = - \\log p(y|x) =  - y \\log \\hat y - (1-y) \\log (1 - \\hat y) \\to \\mbox{minimize !}\n\\]\nLoss for all \\(N\\) samples \\((x_1,y_1),  (x_2,y_2) \\ldots (x_N, y_N)\\)\n\\[\nL(, x) = \\frac{1}{N} \\sum_i L(\\hat y_i, y_i)\n\\]"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#the-strategy-iteration",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#the-strategy-iteration",
    "title": "Backpropagation",
    "section": "The strategy: iteration",
    "text": "The strategy: iteration\n\nGradient Descent\n\n\nADAM\n\n\nA single neuron\n\n\nA network\n\nmultiple logistic regressions\ninput layers, hidden layers (representations), output layer\nactivations (values that neurons path on to next layer)\nnotation: 2 layer network (without input)\nparameters: weights W and biases b - dimensions and shapes"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#the-tool-backpropagation",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#the-tool-backpropagation",
    "title": "Backpropagation",
    "section": "The tool: Backpropagation",
    "text": "The tool: Backpropagation\n\nComputation Graph\n\n\nOverview\n\nfrom a single neuron to network\nfrom numbers to matrices\nstacked layers of computation (passing information)\nNN = matrix multiplications\nNN = complex, non-linear functions\n\n\n\nGPU and vectorized implementation\n\nmatrix multiplication for one sample x = column vector [m x 1]\nmatrix multiplication for n samples x = [m x n] matrix"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#initialistion",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#initialistion",
    "title": "Backpropagation",
    "section": "Initialistion",
    "text": "Initialistion\n\ndifferent initialization can lead to different (local) optima in non-convex landscapes\navoid vanishing/exploding gradients\nXavier Initialization (for tanh): \\(Var(w) = 1/n\\)\nKaiming Initialization (for ReLU); \\(Var(w) = 2/n\\)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#activations-functions",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#activations-functions",
    "title": "Backpropagation",
    "section": "Activations Functions",
    "text": "Activations Functions\nAim: introduce non-linearity\n\\[\n\\begin{aligned}\nz_1 & = W_1 \\cdot x + b_1 \\\\\nz_2 & = W_2 \\cdot z_1 + b_2 \\\\\n& = W_2 ( W_1 \\cdot x + b_1) + b_2 \\\\\n&= W^\\prime x + b^\\prime\n\\end{aligned}\n\\]\n\n\n\n\n\n\nUse (non-linear) activation functions\n\n\n\n\nfor hidden layers for richer expressiveness\nonly exception: final layers for regression problems\n\n\n\n\n\nCode\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nx = torch.linspace(-5, 5, steps=200)\n\n\n\nsigmoid\n\\[\n\\begin{aligned}\ng(z) &= \\frac{1}{1+\\exp(-z)}\\\\\ng'(z) &= g(z) (1 - g(z))\n\\end{aligned}\n\\]\n\n\nCode\ny_sigmoid = torch.sigmoid(x)\ny_prime = y_sigmoid * ( 1 - y_sigmoid) \nplt.figure(figsize=(8, 5))\nplt.plot(x, y_sigmoid, label='Sigmoid', linewidth=2)\nplt.plot(x, y_prime, label='Derviative', linewidth=2)\nplt.title(\"Sigmoid and Derivative\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\n\n\n\nsaturation and vanishing gradients –&gt; no learning\n\n\n\n\ntanh\n\\[\n\\begin{aligned}\ng(z) &= \\tanh(z)\\\\\ng'(z) &= 1 - g(z)^2\n\\end{aligned}\n\\]\n\n\nCode\ny_tanh = torch.tanh(x)\ny_prime = 1 - y_tanh**2\nplt.figure(figsize=(8, 5))\nplt.plot(x, y_tanh, label='tanh', linewidth=2)\nplt.plot(x, y_prime, label='Derviative', linewidth=2)\nplt.title(\"tanh and Derivative\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nzero-centered\n\n\n\nReLU\n\\[\n\\begin{aligned}\ng(z)  &= \\max(0,z)\\\\\ng'(z) &= 0 \\mbox{ for } z &lt; 0 \\\\\n      &= 1 \\mbox{ for } z \\ge 0\n\\end{aligned}\n\\]\n\n\nCode\ny_relu = F.relu(x)\ny_prime = (x &gt; 0).float()\nplt.figure(figsize=(8, 5))\nplt.plot(x, y_relu, label='ReLU', linewidth=2)\nplt.plot(x, y_prime, label='Derviative', linewidth=2)\nplt.title(\"ReLU and Derivative\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSoftmax:\n\ninterprete output as prob with \\(\\sum p_i = 1\\)\nsees all neurons\nlog-sum trick\n\n\\[\n\\log\\sum_i e^{x_i} = x^\\ast + \\log \\sum_i e^{x_i - x^\\ast}\n\\]\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\nSigmoid\nTanh\nReLU\nLeakyReLU\n\n\n\n\nMathematical Range\n(0, 1)\n(-1, 1)\n[0, ∞)\n(-∞, ∞)\n\n\nAdvantages\n• Smooth, interpretable as probability  • Historically important\n• Zero-centered  • Smooth transitions around 0\n• Simple, fast computation  • Sparse activations  • No vanishing gradient for x&gt;0\n• Avoids dying ReLU problem  • Retains advantages of ReLU\n\n\nDisadvantages\n• Vanishing gradient for large |x|  • Not zero-centered  • Saturation\n• Vanishing gradient for large |x|  • More expensive than ReLU\n• Dying ReLU: neurons stuck at 0  • Unbounded outputs → exploding activations\n• Slightly more computation than ReLU  • Still unbounded above • new hyperparameter \n\n\nTypical Use Cases\n• Binary classification output\n• Recurrent networks  • Some hidden layers\n• Default choice for deep nets’ hidden layers\n• When ReLU causes dead neurons or sparse gradients\n\n\n\n\n\n\n\n\n\n\n\n\nActivation Function\nTypical Use\nAdvantages\nDisadvantages\n\n\n\n\nSigmoid\nBinary classification output (logistic regression); hidden units (rarely used now)\n- Smooth output between 0 and 1  - Interpretable as probability\n- Vanishing gradients  - Saturates at extremes  - Not zero-centered\n\n\nTanh\nHidden units in older networks; similar to sigmoid but zero-centered\n- Outputs between -1 and 1  - Zero-centered\n- Still suffers from vanishing gradients  - Saturates for large inputs\n\n\nReLU\nDefault for hidden layers\n- Computationally efficient  - Avoids vanishing gradients for positive inputs  - Sparse activations\n- “Dying ReLU” problem: neurons can output 0 permanently  - Not zero-centered\n\n\nLeaky ReLU\nHidden layers (especially when ReLU is too brittle)\n- Fixes dying ReLU by allowing small gradient when input &lt; 0\n- Slope for negative part is a hyperparameter  - Still not zero-centered\n\n\nSoftmax\nOutput layer for multi-class classification\n- Converts raw scores to probabilities  - Highlights strongest class\n- Not useful for hidden layers  - Sensitive to outliers  - Can be numerically unstable (requires log-sum-exp trick)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#data-preprocessing",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#data-preprocessing",
    "title": "Backpropagation",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nsplitting\ntransformation & normalization"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#data-augmentation",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#data-augmentation",
    "title": "Backpropagation",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\nembedd into ML workflow\nidentify symmetries (invariances): shift, rotate, scale, crop, brightness\naugment only training data\naugment all samples of the training data in same proportions"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#regualarization",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#regualarization",
    "title": "Backpropagation",
    "section": "Regualarization",
    "text": "Regualarization\n\nL2 regularization\n\\[\n\\theta_{k+1} = \\theta_k - \\epsilon \\nabla_\\theta f(\\theta_k) - \\lambda \\theta_k\n\\]\n\npenalizes large parameters\nfavors more even distribution\nimproves generalization, every neuron contributes\n\n\n\nweight decay\n\\[\n\\theta_{k+1} = (1-\\lambda) \\theta_k - \\alpha \\nabla_\\theta f(\\theta_k)\n\\]\nlooks equivalent to L2 norm (for GD), but not for Adam optimizer (momentum) ### data augmentation\n\n\nEarly stopping\n\n\nbagging and ensemble methods\n\nmassive effort for marginal gains (1-2%)\nensemble: train \\(k\\) different models and pool/average\nbagging: use \\(k\\) different data sets\nquantify uncertainty\n\n\n\nDropout\n\ndisable random set of neurons temporarily (20-50%)\nensemble of smaller network within a big one\nreduced network capacity; makes it harder to optimize/minimize loss -&gt; increase training time\nforce all neurons to act –&gt; creates robustness\nredundant representations\nhuman neurons are not reliable!\nmodel.train vs model.eval –&gt; normalize differently\nMonte-Carlo dropout also at inference (p=0.1 - 0.2) run inference multiple times to put confidence estimates on final layer and interpret softmax really probabilistically\nreliable AI: important for doctors\n\n\\[\nz = x. \\theta^T \\to E_{train}[z] = p E_{test}[z]\n\\]\n\n\nBatch Norm\n\nbring activations closer to 0\n\n\n\nOther norms\n\n\n\nWu and He, ECCV 2018"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#interpreting-loss-curves",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#interpreting-loss-curves",
    "title": "Backpropagation",
    "section": "Interpreting Loss curves",
    "text": "Interpreting Loss curves\n–&gt; Andrew NG"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#key-challenges",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#key-challenges",
    "title": "Backpropagation",
    "section": "Key Challenges",
    "text": "Key Challenges\n\nFinding Optimal Solutions\nVanishing/Exploding Gradients\nOverfitting\n\nWhen comparing model performance, compare data + data processing + optimization strategy + model"
  },
  {
    "objectID": "lectures/01_MachineLearning/index.html",
    "href": "lectures/01_MachineLearning/index.html",
    "title": "01 Machine Learning",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nData Stories\n\n\n\nData Formats\n\nData Types\n\nData Analysis Workflow\n\nData Challenges\n\nMetadata\n\n\n\nData is everywhere and in all shapes\n\n\n\nThomas Manke\n\n\nOct 5, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html",
    "href": "lectures/01_MachineLearning/01_Data.html",
    "title": "Data Stories",
    "section": "",
    "text": "Domain\nOpen Formats\nProprietary Formats\nTypical Sizes\n\n\n\n\nUnstructured text\nTXT, Markdown, LaTeX\nDOCX (Microsoft), Pages (Apple)\nkB - MB\n\n\ntabular data\nCSV, TSV\nXLSX (Microsoft), Numbers (Apple)\nkB - MB\n\n\nkey-value data (nested)\nJSON YAML, XML\n???\nkB - MB\n\n\nImages\nPNG, JPEG, TIFF, SVG\nPSD (Photoshop), HEIF/HEIC (Apple)\nkB - GB\n\n\nHuman Genome\nFASTA\n-\n1 GB\n\n\nUbuntu 25.04\n*.c\n-\n5.8 GB\n\n\nWindows 11\n\n.EXE, .DLL (Microsoft)\n7 GB\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nsizes = [7, 1]   # sizes in GB\nlabels = [\"Windows 11\", \"Human Genome\"]\nexplode = [0, 0.075]\n\nfig, ax = plt.subplots()\nwedges, texts = ax.pie(sizes, \n  labels=labels,\n  explode=explode,\n  wedgeprops = dict(width=0.75),\n  startangle=45\n)  \n\nfor t in texts:\n    t.set_fontsize(16)\n\nplt.show()\n\n\n\n\n\nYour Genome Is Not Big !"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-sizes",
    "href": "lectures/01_MachineLearning/01_Data.html#data-sizes",
    "title": "Data Stories",
    "section": "",
    "text": "Domain\nOpen Formats\nProprietary Formats\nTypical Sizes\n\n\n\n\nUnstructured text\nTXT, Markdown, LaTeX\nDOCX (Microsoft), Pages (Apple)\nkB - MB\n\n\ntabular data\nCSV, TSV\nXLSX (Microsoft), Numbers (Apple)\nkB - MB\n\n\nkey-value data (nested)\nJSON YAML, XML\n???\nkB - MB\n\n\nImages\nPNG, JPEG, TIFF, SVG\nPSD (Photoshop), HEIF/HEIC (Apple)\nkB - GB\n\n\nHuman Genome\nFASTA\n-\n1 GB\n\n\nUbuntu 25.04\n*.c\n-\n5.8 GB\n\n\nWindows 11\n\n.EXE, .DLL (Microsoft)\n7 GB\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nsizes = [7, 1]   # sizes in GB\nlabels = [\"Windows 11\", \"Human Genome\"]\nexplode = [0, 0.075]\n\nfig, ax = plt.subplots()\nwedges, texts = ax.pie(sizes, \n  labels=labels,\n  explode=explode,\n  wedgeprops = dict(width=0.75),\n  startangle=45\n)  \n\nfor t in texts:\n    t.set_fontsize(16)\n\nplt.show()\n\n\n\n\n\nYour Genome Is Not Big !"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-rates",
    "href": "lectures/01_MachineLearning/01_Data.html#data-rates",
    "title": "Data Stories",
    "section": "Data Rates",
    "text": "Data Rates\n\n\n\n\n\n\n\n\n\nDomain\nOpen Formats\nProprietary Formats\nTypical Rates\n\n\n\n\nAudio\nFLAC, WAV\n(MP3), M4A (Apple), WMA (Microsoft)\n1–10 MB/min\n\n\nVideo\nWebM/VP9\n(MP4/H.264), MOV (Apple), WMV (Microsoft)\n10 MB/min (HD) → GBs/min (4K+)\n\n\nInternet (Common Crawl)\nWARC (HTTP)\n-\n100 TB/ month ~ 2GB/min\n\n\nGenomics (sequencing machines)\nFASTQ, VCF, BAM, …\nBCL (Illumina), pod5 (ONT)\n34 GB/min → 2 GB/min\n\n\nEarth Observation System (NASA)\nHDF5\n-\n?? → 2 GB/min\n\n\nParticle physics (CERN LHC)\nROOT\n-\n1 PB/s → 2000 GB/min\n\n\nSquare Kilometer Array\nHDF5, FITS\n-\n75 TB/min → 1 TB/min\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\non-the-fly processing for large data\nship metadata with data\navoid data duplication\ncompression"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-io",
    "href": "lectures/01_MachineLearning/01_Data.html#data-io",
    "title": "Data Stories",
    "section": "Data I/O",
    "text": "Data I/O\nThere are several ways to get data into RAM for analysis\n\ntabular data\n\n\nCode\nimport pandas as pd\n\n# from local paths or URL\nurl=\"https://github.com/thomasmanke/DataSets/raw/refs/heads/main/iris.csv.gz\"\ndf = pd.read_csv(url, compression=\"gzip\")\n\nprint(df.head()) # A glimpse\n\n\n   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n0           5.1          3.5           1.4          0.2  setosa\n1           4.9          3.0           1.4          0.2  setosa\n2           4.7          3.2           1.3          0.2  setosa\n3           4.6          3.1           1.5          0.2  setosa\n4           5.0          3.6           1.4          0.2  setosa"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-integrity",
    "href": "lectures/01_MachineLearning/01_Data.html#data-integrity",
    "title": "Data Stories",
    "section": "Data Integrity",
    "text": "Data Integrity\nSometimes you may have to check data integrity after downloads and transfers\n\n\nCode\nimport requests\nimport hashlib\nimport pandas as pd\nfrom io import BytesIO\n\nurl=\"https://github.com/thomasmanke/DataSets/raw/refs/heads/main/iris.csv.gz\"\nresponse = requests.get(url)\ndata = response.content\nchecksum = hashlib.sha256(data).hexdigest()\nprint(f\"Checksum (SHA256) of compressed data: {checksum}\")\n\n# treat data as compressed file and load as before\n#data_io = BytesIO(data) \n#df = pd.read_csv(data_io, compression=\"gzip\")\n\n\nChecksum (SHA256) of compressed data: cc954f855bffd5fa2a5c80194f4c527b951a69e35b8ddb88661b9abb1cf84911"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-summaries",
    "href": "lectures/01_MachineLearning/01_Data.html#data-summaries",
    "title": "Data Stories",
    "section": "Data Summaries",
    "text": "Data Summaries\n\n\nCode\nprint(\"== Iris - Missing Values ==\")\nprint(df.isna().sum())     # column-wise      \nprint(\"== Iris - Statistical Summmary ==\")\nprint(df.describe(include=\"all\"))       \n\n\n== Iris - Missing Values ==\nSepal.Length    0\nSepal.Width     0\nPetal.Length    0\nPetal.Width     0\nSpecies         0\ndtype: int64\n== Iris - Statistical Summmary ==\n        Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\ncount     150.000000   150.000000    150.000000   150.000000     150\nunique           NaN          NaN           NaN          NaN       3\ntop              NaN          NaN           NaN          NaN  setosa\nfreq             NaN          NaN           NaN          NaN      50\nmean        5.843333     3.057333      3.758000     1.199333     NaN\nstd         0.828066     0.435866      1.765298     0.762238     NaN\nmin         4.300000     2.000000      1.000000     0.100000     NaN\n25%         5.100000     2.800000      1.600000     0.300000     NaN\n50%         5.800000     3.000000      4.350000     1.300000     NaN\n75%         6.400000     3.300000      5.100000     1.800000     NaN\nmax         7.900000     4.400000      6.900000     2.500000     NaN\n\n\n\n\n\n\n\n\nData \\(\\gg\\) Numbers\n\n\n\n\nsubject experise required \\(\\to\\) information on iris can be found here\ndata have metadata: when, who, where, why\nhave a question\nhave a model\n\n\n\n\nimage data\n\n\nCode\nfrom PIL import Image\nimport numpy as np\nimport requests\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\n\nurl = 'https://img-datasets.s3.amazonaws.com/elephant.jpg'\nresponse = requests.get(url)\nfo = BytesIO(response.content) # content2fileobject\nimg_obj = Image.open(fo)       # image object \nprint(f\"img_obj type: {type(img_obj)} size={img_obj.size} mode={img_obj.mode}\")\n\n# transformations (optional)\n#img_obj = img_obj.resize((200, 200))\n#img_obj = img_obj.rotate(45) \n\n# conversion: numpy array\nimg = np.array(img_obj, dtype=np.uint8)\nprint(f\"img type: {type(img)} shape: {img.shape}\")\n\n\nimg_obj type: &lt;class 'PIL.JpegImagePlugin.JpegImageFile'&gt; size=(1440, 1440) mode=RGB\nimg type: &lt;class 'numpy.ndarray'&gt; shape: (1440, 1440, 3)\n\n\n\n\nCode\nplt.imshow(img) # imshow accepts both pillow img and np\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\npackaged data\nSome datasets are very famous and they have been packaged for easy access.\n\n\nCode\nimport pandas as pd\nfrom seaborn import load_dataset\nfrom plotly.express import data\nfrom torchvision import datasets\n\n# iris from seaborn\niris = load_dataset(\"iris\")\nprint(f\"{type(iris)} {iris.shape}\")  \n\n# gapminder from px\ngapminder = data.gapminder()\nprint(f\"{type(gapminder)} {gapminder.shape}\")\n\n# mnist from torchvision\nmnist = datasets.MNIST(root=\"./data\", train=True, download=True)\nprint(f\"{type(mnist.data)} {mnist.data.shape}\")  \n\n# Reshaping and data type conversions\nmnist_np = mnist.data.flatten(start_dim=1).numpy()\nprint(f\"{type(mnist_np)} {mnist_np.shape}\")  \n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt; (150, 5)\n&lt;class 'pandas.core.frame.DataFrame'&gt; (1704, 8)\n&lt;class 'torch.Tensor'&gt; torch.Size([60000, 28, 28])\n&lt;class 'numpy.ndarray'&gt; (60000, 784)\n\n\n\n\n\n\n\n\nTask (10 min)\n\n\n\nSearch online for more detailed descriptions of those famous datasets (Iris, Gapminder, MNIST)\nDiscuss with your neighbour:\n\nWhat is in the data?\nHow many samples?\nHow many variables/features?\nWhich questions could we ask about the data?\n\nReport back to class.\n\n\n\n\nCode\n# Sometimes the data comes with descriptions\nfrom sklearn.datasets import load_iris\niris = load_iris()\nprint(iris.DESCR)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nclean data from packages\ndata types: package-dependent, may need conversion/reshaping\npay attention to shapes and dimensions\nwatch out for metadata and descriptions"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-analysis",
    "href": "lectures/01_MachineLearning/01_Data.html#data-analysis",
    "title": "Data Stories",
    "section": "Data Analysis",
    "text": "Data Analysis\n\n\n\nData Analysis Workflow. Iterations and Repetitions (from Hadley Wickham).\n\n\n\n\n\n\n\n\nData Analysis \\(\\gg\\) Modeling\n\n\n\n\n\n\n\n\n\n\n\nStage\nExample tasks\nTypical time share\n\n\n\n\nData Acquisition\ngathering data and metadata, checking consistency, negotiating access\n10–20 %\n\n\nData Cleaning\nHandling missing values, reshaping tables, fixing types, deduplication\n20–60 %\n\n\nData Transformation\nNormalization, scaling, filtering, creating derived variables\n20–30 %\n\n\nData Exploration\nVisualization, summary statistics, feature selection, sanity checks\n10–20 %\n\n\nData Modeling\nModel selection, Model training/(deep)learning, evaluation\n5–20 %\n\n\nData Communication\nReports, dashboards, storytelling\n5–10 %\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMost of this course will focus on “Data Modeling”"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-science-challenges",
    "href": "lectures/01_MachineLearning/01_Data.html#data-science-challenges",
    "title": "Data Stories",
    "section": "Data Science Challenges",
    "text": "Data Science Challenges\n\nLanguages and Jargon\nDomain Expertise\nDirty Data\nFAIR data\nStatistics & Maths\nFAIR data\n(Programming)\n\n\n\n\n“Many safe roads at land; countless doomed routes at sea.”"
  },
  {
    "objectID": "homework/04_Autoencoder.html",
    "href": "homework/04_Autoencoder.html",
    "title": "01 Autoencoder",
    "section": "",
    "text": "Data Download: Obtain both train and test data for the MNIST data set of handwritten digits from torchvision.datasets\nData Massage: For numerical efficiency use pytorch dataloaders and split the dataset into train, test and validation. Check data values and if normalization is required.\nModel Definition: Define a simple autoencoder (model1) with 4 layers to reduce dimensions: 728 -&gt; 128 -&gt; 64 -&gt; 16 -&gt; 3 count dimensions and parameters\nGoal and Strategy: Define a loss function and a suitable optimizer\nModel Training: Train the model and track the following parameters for each epochs\n\n\nloss and validation loss\ntraining images, latent representations, reconstructred images\n\n\nModel Evaluation:\n\n\nplot traing and validation loss.\nFor the best model calculate loss on. test data\ncompare several real images and compare to their reconstruction\n\n\nLatent Assessment: Compare the latent representation of images with the PCA embedding\n\n\nhow much variability is encoded by the first 3 components of PCA(X) compared to PCA(Z)\n\n\n\nData Generation: For a given image, generate new images by changing selected latent variable in PCA space or latent space.\noptional: replace the simple autoencoder model1 by a neural network with 3 convolutional layers (model2)\n\n\ncount parameters and compare to model1\ncompare learning curves with model1\n\n\nexplore what happens if you reduce the size of the data set"
  },
  {
    "objectID": "homework/04_Autoencoder.html#tasks",
    "href": "homework/04_Autoencoder.html#tasks",
    "title": "01 Autoencoder",
    "section": "",
    "text": "Data Download: Obtain both train and test data for the MNIST data set of handwritten digits from torchvision.datasets\nData Massage: For numerical efficiency use pytorch dataloaders and split the dataset into train, test and validation. Check data values and if normalization is required.\nModel Definition: Define a simple autoencoder (model1) with 4 layers to reduce dimensions: 728 -&gt; 128 -&gt; 64 -&gt; 16 -&gt; 3 count dimensions and parameters\nGoal and Strategy: Define a loss function and a suitable optimizer\nModel Training: Train the model and track the following parameters for each epochs\n\n\nloss and validation loss\ntraining images, latent representations, reconstructred images\n\n\nModel Evaluation:\n\n\nplot traing and validation loss.\nFor the best model calculate loss on. test data\ncompare several real images and compare to their reconstruction\n\n\nLatent Assessment: Compare the latent representation of images with the PCA embedding\n\n\nhow much variability is encoded by the first 3 components of PCA(X) compared to PCA(Z)\n\n\n\nData Generation: For a given image, generate new images by changing selected latent variable in PCA space or latent space.\noptional: replace the simple autoencoder model1 by a neural network with 3 convolutional layers (model2)\n\n\ncount parameters and compare to model1\ncompare learning curves with model1\n\n\nexplore what happens if you reduce the size of the data set"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#define-class-label",
    "href": "homework/traditional_ml_to_generative.html#define-class-label",
    "title": "Machine Learning cycle",
    "section": "2. Define class label",
    "text": "2. Define class label"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#visualize-px",
    "href": "homework/traditional_ml_to_generative.html#visualize-px",
    "title": "Machine Learning cycle",
    "section": "3. Visualize p(x)",
    "text": "3. Visualize p(x)\n\n\nCode\nplt.hist(x, bins=40, density=True, alpha=0.6, label=\"p(x) empirical\")\nplt.title(\"Complicated marginal p(x)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"density\")\nplt.legend()\nplt.show()\n\nplt.scatter(y, x, c=y, cmap=\"coolwarm\", alpha=0.6)\nplt.xlabel(\"x\")\nplt.ylabel(\"latent z (true)\")\nplt.title(\"GMM clustering of x\")\nplt.show()"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#unsupervised-clustering-gmm",
    "href": "homework/traditional_ml_to_generative.html#unsupervised-clustering-gmm",
    "title": "Machine Learning cycle",
    "section": "4. Unsupervised clustering (GMM)",
    "text": "4. Unsupervised clustering (GMM)\n\n\nCode\nfrom sklearn.mixture import GaussianMixture\n\ngmm = GaussianMixture(n_components=2).fit(X)\nclusters = gmm.predict(X)\n\nplt.scatter(x, z, c=clusters, cmap=\"coolwarm\", alpha=0.6)\nplt.xlabel(\"x\")\nplt.ylabel(\"latent z (true)\")\nplt.title(\"GMM clustering of x\")\nplt.show()"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#supervised-logistic-regression",
    "href": "homework/traditional_ml_to_generative.html#supervised-logistic-regression",
    "title": "Machine Learning cycle",
    "section": "5. Supervised logistic regression",
    "text": "5. Supervised logistic regression\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\nclf = LogisticRegression().fit(X_train, y_train) # claim: y ~ f(X)\ny_pred = clf.predict(X_test)\ny_prob = clf.predict_proba(X_test)[:, 1]"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#evaluation-classification-report-and-roc-curve",
    "href": "homework/traditional_ml_to_generative.html#evaluation-classification-report-and-roc-curve",
    "title": "Machine Learning cycle",
    "section": "6. Evaluation (classification report and ROC curve)",
    "text": "6. Evaluation (classification report and ROC curve)\n\n\nCode\nfrom sklearn.metrics import classification_report, roc_curve, auc\n\nprint(classification_report(y_test, y_pred))\n\nfpr, tpr, _ = roc_curve(y_test, y_prob)\nplt.plot(fpr, tpr, label=f\"AUC = {auc(fpr, tpr):.2f}\")\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC curve\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#feature-interpretation-logistic-coefficients",
    "href": "homework/traditional_ml_to_generative.html#feature-interpretation-logistic-coefficients",
    "title": "Machine Learning cycle",
    "section": "7. Feature interpretation (logistic coefficients)",
    "text": "7. Feature interpretation (logistic coefficients)\n\n\nCode\nprint(f\"Logistic coef: {clf.coef_}, intercept: {clf.intercept_}\")"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#approximate-inverse-estimate-z-from-x",
    "href": "homework/traditional_ml_to_generative.html#approximate-inverse-estimate-z-from-x",
    "title": "Machine Learning cycle",
    "section": "8. Approximate inverse: estimate z from x",
    "text": "8. Approximate inverse: estimate z from x\n\n\nCode\nfrom scipy.stats import norm, rankdata\n\n# Empirical CDF of x\nranks = rankdata(x)\nempirical_cdf = ranks / (len(x) + 1)\nz_hat = norm.ppf(empirical_cdf)\n\n# Compare distribution\nplt.hist(z_hat, bins=40, density=True, alpha=0.6, label=\"z_hat ~ N(0,1)\")\nx_vals = np.linspace(-3, 3, 200)\nplt.plot(x_vals, norm.pdf(x_vals), 'k--', label=\"True N(0,1)\")\nplt.legend()\nplt.title(\"Transformed variable z_hat from x\")\nplt.show()\n\n\n\n\nCode\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(PolynomialFeatures(degree=10), LinearRegression())\nmodel.fit(z_hat.reshape(-1, 1), x)\n\nz_test = np.linspace(-3, 3, 300)\nx_pred = model.predict(z_test.reshape(-1, 1))\n\nplt.plot(z_test, x_pred, label=\"Learned g(z)\")\nplt.scatter(z, x, alpha=0.2, s=10, label=\"True (z, x)\")\nplt.xlabel(\"z\")\nplt.ylabel(\"x\")\nplt.title(\"Learned inverse mapping z → x\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#generate-new-data-using-z-n0-1",
    "href": "homework/traditional_ml_to_generative.html#generate-new-data-using-z-n0-1",
    "title": "Machine Learning cycle",
    "section": "9. Generate new data using z ~ N(0, 1)",
    "text": "9. Generate new data using z ~ N(0, 1)\n\n\nCode\nz_new = np.random.normal(0, 1, 500)\nepsilon_new = np.random.normal(0, 0.1, 500)\nx_gen = np.sin(3 * z_new) + 0.3 * z_new + epsilon_new\n\nplt.hist(x_gen, bins=40, density=True, alpha=0.6, label=\"Generated x from z\")\nplt.hist(x, bins=40, density=True, alpha=0.4, label=\"Original x\")\nplt.legend()\nplt.title(\"Compare p(x) and generated samples\")\nplt.show()"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#summary",
    "href": "homework/traditional_ml_to_generative.html#summary",
    "title": "Machine Learning cycle",
    "section": "10. Summary",
    "text": "10. Summary\nThis example shows how a nonlinear, complex distribution p(x) can be constructed from a simple latent variable z ~ N(0,1). Without any neural networks, we explored: - Clustering with GMM - Binary classification with logistic regression - Evaluation and interpretability - Latent space approximation and data generation\nThis prepares the stage for introducing Autoencoders, VAEs, and GANs.\n\n\n\n\n\n\n\n\nConcept\nTraditional Method\nMapping to Deep Learning\n\n\n\n\nLatent variable z\nN(0,1)\nLatent code\n\n\nComplex p(x)\nsin(3z) + noise\nGenerator network\n\n\nInverse mapping x → z\nRidge regression\nEncoder network\n\n\nDimensionality reduction\nPCA, GMM\nAutoencoder\n\n\np(z) → x\ndeterministic function\nDecoder or GAN generator\n\n\nLearning p(x)\nKDE or histograms\nLikelihood via VAE, GANs\n\n\nSupervised y\nLogistic regression\nClassifier layer"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neural Networks and Deep Learning",
    "section": "",
    "text": "Upon successful completion of this course, you will\n\nunderstand the key concepts and challenges of neural networks\nbe proficient in common software solutions for deep learning (pytorch)\nbe able to define, train and evaluate your own models\nhave completed 4 concrete data analysis projects and provide reproducible code\nhave worked in teams and presented your work\nwant to learn more and deeper … open"
  },
  {
    "objectID": "index.html#objectives",
    "href": "index.html#objectives",
    "title": "Neural Networks and Deep Learning",
    "section": "",
    "text": "Upon successful completion of this course, you will\n\nunderstand the key concepts and challenges of neural networks\nbe proficient in common software solutions for deep learning (pytorch)\nbe able to define, train and evaluate your own models\nhave completed 4 concrete data analysis projects and provide reproducible code\nhave worked in teams and presented your work\nwant to learn more and deeper … open"
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "Neural Networks and Deep Learning",
    "section": "Content",
    "text": "Content\n\nMachine Learning 101\nNeural Networks\nConvolutional Neural Networks\nGenerative Networks"
  },
  {
    "objectID": "index.html#format",
    "href": "index.html#format",
    "title": "Neural Networks and Deep Learning",
    "section": "Format",
    "text": "Format\nThe course comprises the following blocks (with the estimated time effort)\n\nLectures: (16x 2h) will provide an introduction and overview\nProjects (4x 12h): 4 larger assignments that are solved in teams of 2 students. Each team will submit a fully documented and executable notebook per project.\nTutorials: (16x 2h): weekly tutorials will focus on practical implementations and the review of lecture material. Up to 2 teams will present their project solutions (~30 min / team)."
  },
  {
    "objectID": "index.html#evaluation",
    "href": "index.html#evaluation",
    "title": "Neural Networks and Deep Learning",
    "section": "Evaluation",
    "text": "Evaluation\n\nProject Submissions (28 P). Completed projects are submitted as single jupyter noteboooks and are evaluated per team. Notebooks have to be fully documented to explain both the analysis goals and the code. They should clearly state software dependencies and be fully executable.\nProject Presentation (12 P). Presentation and submission of a reproducible data analysis project in an executable markdown format.\nExam (60 P): this will be a written individual exam with conceptual questions, including multiple choice and short essays."
  },
  {
    "objectID": "index.html#materials-and-references",
    "href": "index.html#materials-and-references",
    "title": "Neural Networks and Deep Learning",
    "section": "Materials and References",
    "text": "Materials and References\n\nClassic Books (Theory & Maths)\n\nGoodfellow et al (2016)\nBishop & Bishop (2024)\n\nClassic Courses (YouTube)\n\nAndrew Ng @ Coursera\nStandford CS230, 2018\n\nPytorch Tutorials (Practical)\nHuggingFace Learning (Next Steps)"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "homework/02_MNIST.html",
    "href": "homework/02_MNIST.html",
    "title": "01 Homework",
    "section": "",
    "text": "Reference:\n\nhttps://www.kaggle.com/code/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n\nRun a classification on the MNIST data with\n\nA simple logistic (10 classes)\nA three layer NN (128, 128, 128, 10) with ReLU\nA CNN\n\ncompare the accuracies on a test data"
  },
  {
    "objectID": "homework/02_MNIST.html#task",
    "href": "homework/02_MNIST.html#task",
    "title": "01 Homework",
    "section": "",
    "text": "Reference:\n\nhttps://www.kaggle.com/code/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n\nRun a classification on the MNIST data with\n\nA simple logistic (10 classes)\nA three layer NN (128, 128, 128, 10) with ReLU\nA CNN\n\ncompare the accuracies on a test data"
  },
  {
    "objectID": "homework/02_MNIST.html#packages",
    "href": "homework/02_MNIST.html#packages",
    "title": "01 Homework",
    "section": "Packages",
    "text": "Packages"
  },
  {
    "objectID": "homework/02_MNIST.html#data",
    "href": "homework/02_MNIST.html#data",
    "title": "01 Homework",
    "section": "Data",
    "text": "Data\nVerify mean and std"
  },
  {
    "objectID": "homework/02_MNIST.html#model-definition",
    "href": "homework/02_MNIST.html#model-definition",
    "title": "01 Homework",
    "section": "Model Definition",
    "text": "Model Definition"
  },
  {
    "objectID": "homework/02_MNIST.html#parameters",
    "href": "homework/02_MNIST.html#parameters",
    "title": "01 Homework",
    "section": "Parameters",
    "text": "Parameters"
  },
  {
    "objectID": "homework/02_MNIST.html#define-traing-loop",
    "href": "homework/02_MNIST.html#define-traing-loop",
    "title": "01 Homework",
    "section": "Define Traing Loop",
    "text": "Define Traing Loop"
  },
  {
    "objectID": "homework/02_MNIST.html#run-training",
    "href": "homework/02_MNIST.html#run-training",
    "title": "01 Homework",
    "section": "Run Training",
    "text": "Run Training"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "neural networks: layers of neurons connected by weight\nin each neuron: weights + bias + non-linear activation function\noptimization: gradient descent, SGD , momentum, ADAM\nbackprogation: gradients with chain rule on computational graph\ntechnical: vanishing gradients –&gt; batch norm, weight initialization\noverfitting and generalization: data augmentation, regularization (L2, dropout)"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#review",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#review",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "neural networks: layers of neurons connected by weight\nin each neuron: weights + bias + non-linear activation function\noptimization: gradient descent, SGD , momentum, ADAM\nbackprogation: gradients with chain rule on computational graph\ntechnical: vanishing gradients –&gt; batch norm, weight initialization\noverfitting and generalization: data augmentation, regularization (L2, dropout)"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#why-not-add-more-layers",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#why-not-add-more-layers",
    "title": "Convolutional Neural Networks",
    "section": "Why not add more layers",
    "text": "Why not add more layers\n\nNN with one hidden layer are universal function approximators\notpimization hard: finding parameters to give optimal decision boundary (for classification)\nno structure; just brute force\nblack boxes: parameter interpretation\nperformance plateau\n\n–&gt; more structure: - MLP: no structure –&gt; permutation of input no changes - CNN: spatial structure (images) - RNN: temporal structure (sequences) Transformers –&gt; from classifiers to generators"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#problem",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#problem",
    "title": "Convolutional Neural Networks",
    "section": "Problem:",
    "text": "Problem:\nfully connected layers: input(1000 x 1000 x 3 pixels) –&gt; 1 hidden layer(1000 neurons) –&gt; 3 billion parameters\nrestrict capacity of each layer –&gt; weight sharing\ninvariance assumption: should not matter where feature is\nCNN: start point for subsequence task - classification - object localization (bounding box) - object detection (multiple bounding boxes) - instance segmentation (point-wise classification)"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#what-are-convolutions",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#what-are-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "What are convolutions?",
    "text": "What are convolutions?\n\n1D\n–&gt; i2DL\n\n\n2D\n–&gt; i2DL"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#cnn-architecture",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#cnn-architecture",
    "title": "Convolutional Neural Networks",
    "section": "CNN Architecture",
    "text": "CNN Architecture\n\nNN: pass 1-D vectors from layer to layer\nCNN: match network to spatial structure (2D images)\n\nkeep input 2-D (actually: CxWxH) –&gt; also arrange neurons in 2-D (actually DxWxH)\n\nNumber of outputs from Convolutional Layer\n\\[\nN_{out} = \\frac{N_{in} - F + 2P}{S} + 1\n\\]\nNotice\n\n\\((N_{in} - F + 2P)/S\\) has to be integer for this to work properly.\n\n\n\nNumbers and Memory\n\n2-layer conv\n\n\n\n\n\n\n\n\n\n\n\n\nLayer\nOutput Shape\nParam Formula\n#Outputs\n#Params\nParam Mem (MB)\nOut Mem (MB)\n\n\n\n\nInput\n(1, 3, 224, 224)\n—\n150,528\n0\n0.00\n0.60\n\n\nConv1\n(1, 64, 224, 224)\n3×3×3×64 + 64\n3,211,264\n1,792\n0.01\n12.84\n\n\nMaxPool\n(1, 64, 112, 112)\n—\n802,816\n0\n0.00\n3.21\n\n\nConv2\n(1, 64, 112, 112)\n3×3×64×64 + 64\n802,816\n36,928\n0.15\n3.21\n\n\nFlatten\n(1, 802816)\n—\n802,816\n0\n0.00\n3.21\n\n\nFC\n(1, 10)\n802816×10 + 10\n10\n8,028,170\n32.11\n0.00\n\n\nTotal\n—\n—\n—\n8,066,890\n32.27\n23.07\n\n\n\n\n\nVggNet (2014)\n\n\n\n\n\n\n\n\n\n\n\n\nLayer\nOutput Shape\nParam Formula\n#Outputs\n#Params\nParam Mem (MB)\nOutput Mem (MB)\n\n\n\n\nInput\n224×224×3\n—\n150,528\n0\n0.00\n0.60\n\n\nConv1_1\n224×224×64\n3×3×3×64 + 64\n3,211,264\n1,792\n0.01\n12.84\n\n\nConv1_2\n224×224×64\n3×3×64×64 + 64\n3,211,264\n36,928\n0.15\n12.84\n\n\nMaxPool1\n112×112×64\n—\n802,816\n0\n0.00\n3.21\n\n\nConv2_1\n112×112×128\n3×3×64×128 + 128\n1,605,632\n73,856\n0.30\n6.42\n\n\nConv2_2\n112×112×128\n3×3×128×128 + 128\n1,605,632\n147,584\n0.59\n6.42\n\n\nMaxPool2\n56×56×128\n—\n401,408\n0\n0.00\n1.61\n\n\nConv3_1\n56×56×256\n3×3×128×256 + 256\n802,816\n295,168\n1.18\n3.21\n\n\nConv3_2\n56×56×256\n3×3×256×256 + 256\n802,816\n590,080\n2.36\n3.21\n\n\nConv3_3\n56×56×256\n3×3×256×256 + 256\n802,816\n590,080\n2.36\n3.21\n\n\nMaxPool3\n28×28×256\n—\n200,704\n0\n0.00\n0.80\n\n\nConv4_1\n28×28×512\n3×3×256×512 + 512\n401,408\n1,180,160\n4.72\n1.61\n\n\nConv4_2\n28×28×512\n3×3×512×512 + 512\n401,408\n2,359,808\n9.44\n1.61\n\n\nConv4_3\n28×28×512\n3×3×512×512 + 512\n401,408\n2,359,808\n9.44\n1.61\n\n\nMaxPool4\n14×14×512\n—\n100,352\n0\n0.00\n0.40\n\n\nConv5_1\n14×14×512\n3×3×512×512 + 512\n100,352\n2,359,808\n9.44\n0.40\n\n\nConv5_2\n14×14×512\n3×3×512×512 + 512\n100,352\n2,359,808\n9.44\n0.40\n\n\nConv5_3\n14×14×512\n3×3×512×512 + 512\n100,352\n2,359,808\n9.44\n0.40\n\n\nMaxPool5\n7×7×512\n—\n25,088\n0\n0.00\n0.10\n\n\nFC_1\n4096\n7×7×512×4096 + 4096\n4,096\n102,764,544\n411.06\n0.02\n\n\nFC_2\n4096\n4096×4096 + 4096\n4,096\n16,781,312\n67.13\n0.02\n\n\nFC_3\n1000\n4096×1000 + 1000\n1,000\n4,097,000\n16.39\n0.00\n\n\nTotal\n—\n—\n—\n138M+\n564 MB\n~77 MB\n\n\n\nNotice\n\nmemory and compute in early layers, parameters in last layer\nnumbers can more than double when the gradients are calculated + caching\nmaintain also multiple images (batch normalization)\n\n\n\nPython Implementation\nDefine Model:\n\n\nCode\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport numpy as np\n\nclass my_model(nn.Module):\n    \"\"\" \n    My simple Convolutional Network\n    input shape: [,3,224,224] \n    output shape: [,10]\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.flatten = nn.Flatten()\n        self.fc = nn.Linear(112 * 112 * 64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.conv2(x)\n        x = self.flatten(x) \n        x = self.fc(x)\n        return x\n\n\nNotice that each model has a defined input shape (e.g. [,3,224,224]) and a defined output (e.g [,10]). The inner layers should work such that the dimensions match.\nIdeally we also define pre-processing steps and transformers to adjust those dimensions when other data is provided.\nEmploy Model:\n\n\nCode\nmodel = my_model()\n#model = models.vgg16() # precompiled model from torchvision.models\n\n\nGet parameter dimensions\n\n\nCode\n# number and shapes of parameters can be obtained directly from model\n# there is also a package that could do the same\n# pip install torchsummary --&gt; torchsummary.summary(model, input_shape) \nfor name, layer in model.named_modules():\n    num_params = sum(p.numel() for p in layer.parameters())\n    print(f\"{layer.__class__.__name__:12} {num_params}\")\n\n# shapes of parameters: notice that we have weights and biases\nfor name, parameter in model.named_parameters():\n    print(name, parameter.shape)\n\n\nmy_model     8066890\nConv2d       1792\nMaxPool2d    0\nConv2d       36928\nFlatten      0\nLinear       8028170\nconv1.weight torch.Size([64, 3, 3, 3])\nconv1.bias torch.Size([64])\nconv2.weight torch.Size([64, 64, 3, 3])\nconv2.bias torch.Size([64])\nfc.weight torch.Size([10, 802816])\nfc.bias torch.Size([10])\n\n\nNotice that the number and shapes of parameters can be obtained directly from the model.\nIn contrast, the output sizes of each layer will depend on the dimension of the input data and can only be done at execution (forward pass)\nBelow I show how to use, hooks that allow for efficient manipulation of the forward path, such that all quantities of interest can be tracked. Here I will track the shapes\nDefine Hook\n\n\nCode\ndef describe_model_forward(model, input_tensor):\n    \"\"\" \n    describe_model_forward collects information on the shapes of parameters and outputs \n    on each computational layer of a neural network\n    \"\"\"\n    layer_info = []\n    hooks = []\n\n    # define a \"hook\" function that can be passed to the model\n    # and evaluate as each layer is run in the foward path\n    def register_hook(module):\n        def hook(module, input, output):\n            name = module.__class__.__name__\n            # assumption: output of model is single tensor\n            output_shape = tuple(output.shape)\n            param_shapes = [tuple(p.shape) for p in module.parameters() if p.requires_grad]\n\n            layer_info.append((name, output_shape, param_shapes))\n\n        # add hook to computational layers (not containers Sequential, ModuleList)\n        # Warning: layer and module is ALMOST synonymous, but modules may also be container of layers\n        # here we exclude them conditionally\n        # This was trial and error. I'm not sure if this exlcusion list is exhaustive in general\n        if not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList):\n            hooks.append(module.register_forward_hook(hook))\n\n    # Recursively add register_hook() function to each computational layer\n    model.apply(register_hook)\n\n    # Run forward pass\n    with torch.no_grad():\n        _ = model(input_tensor)\n\n    # Remove hooks to clean memory\n    for h in hooks:\n        h.remove()\n\n    return layer_info\n\n\nCollect Layer Information\n\n\nCode\nmodel = models.vgg16()\n# Define Data\nx = torch.randn(16, 3, 224, 224) # fake data ~ batch of images\n\n# Run forward path and collect information\nlayer_info = describe_model_forward(model, x)\n\n# Print out information\nfor (name, out_shape, param_shape) in layer_info:\n    # assume that there is always an output shape\n    n_out = int(np.prod(out_shape))\n    n_params = 0\n    # parameters shape maybe empty (e.g ReLU --&gt; 0 params)\n    if len(param_shape) == 2:\n        n_weights = np.prod(param_shape[0])\n        n_bias = np.prod(param_shape[1])\n        n_params = int(n_weights + n_bias)\n\n    print(f\"Layer {name:&lt;14} \\\n        output: {str(out_shape):&lt;25} {n_out:&lt;10,}\\\n        Parameters: {str(param_shape):&lt;15} {n_params:,}\"\n    )\n\n\nLayer Conv2d                 output: (16, 64, 224, 224)        51,380,224        Parameters: [(64, 3, 3, 3), (64,)] 1,792\nLayer ReLU                   output: (16, 64, 224, 224)        51,380,224        Parameters: []              0\nLayer Conv2d                 output: (16, 64, 224, 224)        51,380,224        Parameters: [(64, 64, 3, 3), (64,)] 36,928\nLayer ReLU                   output: (16, 64, 224, 224)        51,380,224        Parameters: []              0\nLayer MaxPool2d              output: (16, 64, 112, 112)        12,845,056        Parameters: []              0\nLayer Conv2d                 output: (16, 128, 112, 112)       25,690,112        Parameters: [(128, 64, 3, 3), (128,)] 73,856\nLayer ReLU                   output: (16, 128, 112, 112)       25,690,112        Parameters: []              0\nLayer Conv2d                 output: (16, 128, 112, 112)       25,690,112        Parameters: [(128, 128, 3, 3), (128,)] 147,584\nLayer ReLU                   output: (16, 128, 112, 112)       25,690,112        Parameters: []              0\nLayer MaxPool2d              output: (16, 128, 56, 56)         6,422,528         Parameters: []              0\nLayer Conv2d                 output: (16, 256, 56, 56)         12,845,056        Parameters: [(256, 128, 3, 3), (256,)] 295,168\nLayer ReLU                   output: (16, 256, 56, 56)         12,845,056        Parameters: []              0\nLayer Conv2d                 output: (16, 256, 56, 56)         12,845,056        Parameters: [(256, 256, 3, 3), (256,)] 590,080\nLayer ReLU                   output: (16, 256, 56, 56)         12,845,056        Parameters: []              0\nLayer Conv2d                 output: (16, 256, 56, 56)         12,845,056        Parameters: [(256, 256, 3, 3), (256,)] 590,080\nLayer ReLU                   output: (16, 256, 56, 56)         12,845,056        Parameters: []              0\nLayer MaxPool2d              output: (16, 256, 28, 28)         3,211,264         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 28, 28)         6,422,528         Parameters: [(512, 256, 3, 3), (512,)] 1,180,160\nLayer ReLU                   output: (16, 512, 28, 28)         6,422,528         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 28, 28)         6,422,528         Parameters: [(512, 512, 3, 3), (512,)] 2,359,808\nLayer ReLU                   output: (16, 512, 28, 28)         6,422,528         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 28, 28)         6,422,528         Parameters: [(512, 512, 3, 3), (512,)] 2,359,808\nLayer ReLU                   output: (16, 512, 28, 28)         6,422,528         Parameters: []              0\nLayer MaxPool2d              output: (16, 512, 14, 14)         1,605,632         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 14, 14)         1,605,632         Parameters: [(512, 512, 3, 3), (512,)] 2,359,808\nLayer ReLU                   output: (16, 512, 14, 14)         1,605,632         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 14, 14)         1,605,632         Parameters: [(512, 512, 3, 3), (512,)] 2,359,808\nLayer ReLU                   output: (16, 512, 14, 14)         1,605,632         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 14, 14)         1,605,632         Parameters: [(512, 512, 3, 3), (512,)] 2,359,808\nLayer ReLU                   output: (16, 512, 14, 14)         1,605,632         Parameters: []              0\nLayer MaxPool2d              output: (16, 512, 7, 7)           401,408           Parameters: []              0\nLayer AdaptiveAvgPool2d         output: (16, 512, 7, 7)           401,408           Parameters: []              0\nLayer Linear                 output: (16, 4096)                65,536            Parameters: [(4096, 25088), (4096,)] 102,764,544\nLayer ReLU                   output: (16, 4096)                65,536            Parameters: []              0\nLayer Dropout                output: (16, 4096)                65,536            Parameters: []              0\nLayer Linear                 output: (16, 4096)                65,536            Parameters: [(4096, 4096), (4096,)] 16,781,312\nLayer ReLU                   output: (16, 4096)                65,536            Parameters: []              0\nLayer Dropout                output: (16, 4096)                65,536            Parameters: []              0\nLayer Linear                 output: (16, 1000)                16,000            Parameters: [(1000, 4096), (1000,)] 4,097,000\nLayer VGG                    output: (16, 1000)                16,000            Parameters: [(64, 3, 3, 3), (64,), (64, 64, 3, 3), (64,), (128, 64, 3, 3), (128,), (128, 128, 3, 3), (128,), (256, 128, 3, 3), (256,), (256, 256, 3, 3), (256,), (256, 256, 3, 3), (256,), (512, 256, 3, 3), (512,), (512, 512, 3, 3), (512,), (512, 512, 3, 3), (512,), (512, 512, 3, 3), (512,), (512, 512, 3, 3), (512,), (512, 512, 3, 3), (512,), (4096, 25088), (4096,), (4096, 4096), (4096,), (1000, 4096), (1000,)] 0\n\n\n\n\n\nHabits and Recommendations\n\nuse input size \\(LxL = 2^n\\) (e.g. 512)\nuse stride \\(S=1\\)\nuse padding \\(P= (F-1)/2\\) to retain input size –&gt; multiple CONV layers\npooling: \\(F=2 S=2\\) (size reduction: \\(LxL --&gt; L/2xL/2\\) !!! - aggressive reduction,"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#illustration",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#illustration",
    "title": "Convolutional Neural Networks",
    "section": "Illustration:",
    "text": "Illustration:\nhttps://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#summary",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#summary",
    "title": "Convolutional Neural Networks",
    "section": "Summary",
    "text": "Summary\n\n\n\nExample from VGGNet. Image from A. Karpathy"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html",
    "title": "Activation Functions",
    "section": "",
    "text": "observed data: \\(x\\) true label: \\(y\\) predicted label probability: \\(\\hat y = f(x, \\theta)\\) \\[\np(y|x) = \\hat{y}^y (1-\\hat{y})^y \\to \\mbox{maximize !}\n\\]\nLoss for one sample \\(x\\) \\[\nL(\\hat y, y) = - \\log p(y|x) =  - y \\log \\hat y - (1-y) \\log (1 - \\hat y) \\to \\mbox{minimize !}\n\\]\nLoss for all \\(N\\) samples \\((x_1,y_1),  (x_2,y_2) \\ldots (x_N, y_N)\\)\n\\[\nL(, x) = \\frac{1}{N} \\sum_i L(\\hat y_i, y_i)\n\\]"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#the-goal-minimize-lost",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#the-goal-minimize-lost",
    "title": "Activation Functions",
    "section": "",
    "text": "observed data: \\(x\\) true label: \\(y\\) predicted label probability: \\(\\hat y = f(x, \\theta)\\) \\[\np(y|x) = \\hat{y}^y (1-\\hat{y})^y \\to \\mbox{maximize !}\n\\]\nLoss for one sample \\(x\\) \\[\nL(\\hat y, y) = - \\log p(y|x) =  - y \\log \\hat y - (1-y) \\log (1 - \\hat y) \\to \\mbox{minimize !}\n\\]\nLoss for all \\(N\\) samples \\((x_1,y_1),  (x_2,y_2) \\ldots (x_N, y_N)\\)\n\\[\nL(, x) = \\frac{1}{N} \\sum_i L(\\hat y_i, y_i)\n\\]"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#the-strategy-iteration",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#the-strategy-iteration",
    "title": "Activation Functions",
    "section": "The strategy: iteration",
    "text": "The strategy: iteration\n\nGradient Descent\n\n\nADAM\n\n\nA single neuron\n\n\nA network\n\nmultiple logistic regressions\ninput layers, hidden layers (representations), output layer\nactivations (values that neurons path on to next layer)\nnotation: 2 layer network (without input)\nparameters: weights W and biases b - dimensions and shapes"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#the-tool-backpropagation",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#the-tool-backpropagation",
    "title": "Activation Functions",
    "section": "The tool: Backpropagation",
    "text": "The tool: Backpropagation\n\nComputation Graph\n\n\nOverview\n\nfrom a single neuron to network\nfrom numbers to matrices\nstacked layers of computation (passing information)\nNN = matrix multiplications\nNN = complex, non-linear functions\n\n\n\nGPU and vectorized implementation\n\nmatrix multiplication for one sample x = column vector [m x 1]\nmatrix multiplication for n samples x = [m x n] matrix"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#initialistion",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#initialistion",
    "title": "Activation Functions",
    "section": "Initialistion",
    "text": "Initialistion\n\ndifferent initialization can lead to different (local) optima in non-convex landscapes\navoid vanishing/exploding gradients\nXavier Initialization (for tanh): \\(Var(w) = 1/n\\)\nKaiming Initialization (for ReLU); \\(Var(w) = 2/n\\)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#activations-functions",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#activations-functions",
    "title": "Activation Functions",
    "section": "Activations Functions",
    "text": "Activations Functions\nAim: introduce non-linearity\n\\[\n\\begin{aligned}\nz_1 & = W_1 \\cdot x + b_1 \\\\\nz_2 & = W_2 \\cdot z_1 + b_2 \\\\\n& = W_2 ( W_1 \\cdot x + b_1) + b_2 \\\\\n&= W^\\prime x + b^\\prime\n\\end{aligned}\n\\]\n\n\n\n\n\n\nUse (non-linear) activation functions\n\n\n\n\nfor hidden layers for richer expressiveness\nonly exception: final layers for regression problems\n\n\n\n\n\nCode\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nx = torch.linspace(-5, 5, steps=200)\n\n\n\nsigmoid\n\\[\n\\begin{aligned}\ng(z) &= \\frac{1}{1+\\exp(-z)}\\\\\ng'(z) &= g(z) (1 - g(z))\n\\end{aligned}\n\\]\n\n\nCode\ny_sigmoid = torch.sigmoid(x)\ny_prime = y_sigmoid * ( 1 - y_sigmoid) \nplt.figure(figsize=(8, 5))\nplt.plot(x, y_sigmoid, label='Sigmoid', linewidth=2)\nplt.plot(x, y_prime, label='Derviative', linewidth=2)\nplt.title(\"Sigmoid and Derivative\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\n\n\n\nsaturation and vanishing gradients –&gt; no learning\n\n\n\n\ntanh\n\\[\n\\begin{aligned}\ng(z) &= \\tanh(z)\\\\\ng'(z) &= 1 - g(z)^2\n\\end{aligned}\n\\]\n\n\nCode\ny_tanh = torch.tanh(x)\ny_prime = 1 - y_tanh**2\nplt.figure(figsize=(8, 5))\nplt.plot(x, y_tanh, label='tanh', linewidth=2)\nplt.plot(x, y_prime, label='Derviative', linewidth=2)\nplt.title(\"tanh and Derivative\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nzero-centered\n\n\n\nReLU\n\\[\n\\begin{aligned}\ng(z)  &= \\max(0,z)\\\\\ng'(z) &= 0 \\mbox{ for } z &lt; 0 \\\\\n      &= 1 \\mbox{ for } z \\ge 0\n\\end{aligned}\n\\]\n\n\nCode\ny_relu = F.relu(x)\ny_prime = (x &gt; 0).float()\nplt.figure(figsize=(8, 5))\nplt.plot(x, y_relu, label='ReLU', linewidth=2)\nplt.plot(x, y_prime, label='Derviative', linewidth=2)\nplt.title(\"ReLU and Derivative\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSoftmax:\n\ninterprete output as prob with \\(\\sum p_i = 1\\)\nsees all neurons\nlog-sum trick\n\n\\[\n\\log\\sum_i e^{x_i} = x^\\ast + \\log \\sum_i e^{x_i - x^\\ast}\n\\]\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\nSigmoid\nTanh\nReLU\nLeakyReLU\n\n\n\n\nMathematical Range\n(0, 1)\n(-1, 1)\n[0, ∞)\n(-∞, ∞)\n\n\nAdvantages\n• Smooth, interpretable as probability  • Historically important\n• Zero-centered  • Smooth transitions around 0\n• Simple, fast computation  • Sparse activations  • No vanishing gradient for x&gt;0\n• Avoids dying ReLU problem  • Retains advantages of ReLU\n\n\nDisadvantages\n• Vanishing gradient for large |x|  • Not zero-centered  • Saturation\n• Vanishing gradient for large |x|  • More expensive than ReLU\n• Dying ReLU: neurons stuck at 0  • Unbounded outputs → exploding activations\n• Slightly more computation than ReLU  • Still unbounded above • new hyperparameter \n\n\nTypical Use Cases\n• Binary classification output\n• Recurrent networks  • Some hidden layers\n• Default choice for deep nets’ hidden layers\n• When ReLU causes dead neurons or sparse gradients\n\n\n\n\n\n\n\n\n\n\n\n\nActivation Function\nTypical Use\nAdvantages\nDisadvantages\n\n\n\n\nSigmoid\nBinary classification output (logistic regression); hidden units (rarely used now)\n- Smooth output between 0 and 1  - Interpretable as probability\n- Vanishing gradients  - Saturates at extremes  - Not zero-centered\n\n\nTanh\nHidden units in older networks; similar to sigmoid but zero-centered\n- Outputs between -1 and 1  - Zero-centered\n- Still suffers from vanishing gradients  - Saturates for large inputs\n\n\nReLU\nDefault for hidden layers\n- Computationally efficient  - Avoids vanishing gradients for positive inputs  - Sparse activations\n- “Dying ReLU” problem: neurons can output 0 permanently  - Not zero-centered\n\n\nLeaky ReLU\nHidden layers (especially when ReLU is too brittle)\n- Fixes dying ReLU by allowing small gradient when input &lt; 0\n- Slope for negative part is a hyperparameter  - Still not zero-centered\n\n\nSoftmax\nOutput layer for multi-class classification\n- Converts raw scores to probabilities  - Highlights strongest class\n- Not useful for hidden layers  - Sensitive to outliers  - Can be numerically unstable (requires log-sum-exp trick)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#data-preprocessing",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#data-preprocessing",
    "title": "Activation Functions",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nsplitting\ntransformation & normalization"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#data-augmentation",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#data-augmentation",
    "title": "Activation Functions",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\nembedd into ML workflow\nidentify symmetries (invariances): shift, rotate, scale, crop, brightness\naugment only training data\naugment all samples of the training data in same proportions"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#regualarization",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#regualarization",
    "title": "Activation Functions",
    "section": "Regualarization",
    "text": "Regualarization\n\nL2 regularization\n\\[\n\\theta_{k+1} = \\theta_k - \\epsilon \\nabla_\\theta f(\\theta_k) - \\lambda \\theta_k\n\\]\n\npenalizes large parameters\nfavors more even distribution\nimproves generalization, every neuron contributes\n\n\n\nweight decay\n\\[\n\\theta_{k+1} = (1-\\lambda) \\theta_k - \\alpha \\nabla_\\theta f(\\theta_k)\n\\]\nlooks equivalent to L2 norm (for GD), but not for Adam optimizer (momentum) ### data augmentation\n\n\nEarly stopping\n\n\nbagging and ensemble methods\n\nmassive effort for marginal gains (1-2%)\nensemble: train \\(k\\) different models and pool/average\nbagging: use \\(k\\) different data sets\nquantify uncertainty\n\n\n\nDropout\n\ndisable random set of neurons temporarily (20-50%)\nensemble of smaller network within a big one\nreduced network capacity; makes it harder to optimize/minimize loss -&gt; increase training time\nforce all neurons to act –&gt; creates robustness\nredundant representations\nhuman neurons are not reliable!\nmodel.train vs model.eval –&gt; normalize differently\nMonte-Carlo dropout also at inference (p=0.1 - 0.2) run inference multiple times to put confidence estimates on final layer and interpret softmax really probabilistically\nreliable AI: important for doctors\n\n\\[\nz = x. \\theta^T \\to E_{train}[z] = p E_{test}[z]\n\\]\n\n\nBatch Norm\n\nbring activations closer to 0\n\n\n\nOther norms\n\n\n\nWu and He, ECCV 2018"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#interpreting-loss-curves",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#interpreting-loss-curves",
    "title": "Activation Functions",
    "section": "Interpreting Loss curves",
    "text": "Interpreting Loss curves\n–&gt; Andrew NG"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#key-challenges",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#key-challenges",
    "title": "Activation Functions",
    "section": "Key Challenges",
    "text": "Key Challenges\n\nFinding Optimal Solutions\nVanishing/Exploding Gradients\nOverfitting\n\nWhen comparing model performance, compare data + data processing + optimization strategy + model"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html",
    "href": "lectures/02_NeuralNetworks/neural_networks.html",
    "title": "Neural Networks (Fragments)",
    "section": "",
    "text": "early layers learn to represent lower features (images: edges, audio: sounds, …), deeper layers learn concepts (shapes, eyes, faces or phonemes, words, sentences)\nneuroscientist inspiration\ncircuit theory: computable functions, e.g. parity \\((x_1, ... , x_n)\\) = x_1 XOR x_2 .. XOR x_n$ need \\(O(log n)\\) neurons in multiple layers, or \\(O(2^n)\\) in a single layer (exponential)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#why-deep",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#why-deep",
    "title": "Neural Networks (Fragments)",
    "section": "",
    "text": "early layers learn to represent lower features (images: edges, audio: sounds, …), deeper layers learn concepts (shapes, eyes, faces or phonemes, words, sentences)\nneuroscientist inspiration\ncircuit theory: computable functions, e.g. parity \\((x_1, ... , x_n)\\) = x_1 XOR x_2 .. XOR x_n$ need \\(O(log n)\\) neurons in multiple layers, or \\(O(2^n)\\) in a single layer (exponential)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#dimesnion-counting-convertions",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#dimesnion-counting-convertions",
    "title": "Neural Networks (Fragments)",
    "section": "Dimesnion Counting & Convertions",
    "text": "Dimesnion Counting & Convertions\n\n\n\n\n\n\n\n\nAspect\nTextbook / Theory (Samples as Columns)\nTensorFlow / PyTorch (Batch-First, transposed definition)\n\n\n\n\nActivations \\(A^{(l-1)}\\)\n\\(A^{(l-1)} \\in \\mathbb{R}^{m \\times n}\\)\n\\(A^{(l-1)} \\in \\mathbb{R}^{n \\times m}\\)\n\n\nWeights \\(W^{(l)}\\)\n\\(W^{(l)} \\in \\mathbb{R}^{k \\times m}\\)\n\\(W^{(l)} \\in \\mathbb{R}^{m \\times k}\\)\n\n\nBias \\(b^{(l)}\\)\n\\(b^{(l)} \\in \\mathbb{R}^{k \\times 1}\\)\n\\(b^{(l)} \\in \\mathbb{R}^{1 \\times k}\\) (broadcast)\n\n\nPre-activation \\(Z^{(l)}\\)\n\\(Z^{(l)} = W^{(l)} A^{(l-1)} + b^{(l)}\\)\n\\(Z^{(l)} = A^{(l-1)} W^{(l)} + b^{(l)}\\)\n\n\nOutput \\(Z^{(l)}\\)\n\\(Z^{(l)} \\in \\mathbb{R}^{k \\times n}\\)\n\\(Z^{(l)} \\in \\mathbb{R}^{n \\times k}\\)\n\n\nSample orientation\nEach column = 1 sample\nEach row = 1 sample\n\n\nTypical in\nMath, statistics, early ML literature\nTensorFlow, PyTorch, NumPy, GPU computation"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#dense-nueral-networks-mlp",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#dense-nueral-networks-mlp",
    "title": "Neural Networks (Fragments)",
    "section": "Dense Nueral Networks (MLP)",
    "text": "Dense Nueral Networks (MLP)\nGeneric equation for forward propagation @ layer \\(l\\):\n\\(x=a^0\\) (input layer), \\(\\hat y = a^L\\) (output layer)\n\nForward: a^{l-1} a^l$\n\\[\n\\begin{aligned}\nz^l &= W^l a^{l-1} + b^l \\\\\na^l &= g(z^l)\n\\end{aligned}\n\\] - cache \\(z^l\\)\n\n\nBackward: da^l a^{l-1}$\n\\[\n\\begin{aligned}\ndz^{l} &= da^{l} * g'(z^l)\\\\\ndW^l &= dz^l {a^{l-1}}^T \\\\\ndb^l &= \\sum dz^l\\\\\nda^{l-1} &= {W^{l}}^T dz^l\n\\end{aligned}\n\\]\n\ncache: dW^l, db^l\n\n\\(\\boxed{\\,dZ^{[l]} \\;=\\; dA^{[l]} \\;\\odot\\; g^{[l]\\,'}(Z^{[l]})\\,} \\qquad (n_l\\times m)\\)\n\\[\n\\begin{aligned} D^{[l]} &= \\frac{\\partial J}{\\partial Z^{[l]}} = \\frac{\\partial J}{\\partial A^{[l]}} \\odot g^{[l]\\,'}(Z^{[l]}) \\quad & (n_l \\times m) \\\\ dW^{[l]} &= \\frac{\\partial J}{\\partial W^{[l]}} = \\frac{1}{m}\\, D^{[l]} (A^{[l-1]})^\\top \\quad & (n_l \\times n_{l-1}) \\\\ db^{[l]} &= \\frac{\\partial J}{\\partial b^{[l]}} = \\frac{1}{m}\\, D^{[l]} \\mathbf{1}_m \\quad & (n_l \\times 1) \\\\ dA^{[l-1]} &= \\frac{\\partial J}{\\partial A^{[l-1]}} = (W^{[l]})^\\top D^{[l]} \\quad & (n_{l-1} \\times m) \\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#a-short-history-of-nn",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#a-short-history-of-nn",
    "title": "Neural Networks (Fragments)",
    "section": "A short history of NN",
    "text": "A short history of NN\n!!!! Double-check !!!!\n\n\n\nYear\nTask Type\nTool / Model\nCompany / Author(s)\nArchitecture / Highlights\nPublication / Link\nCitations (approx.)\nParameters\n\n\n\n\n1997\nSequence Modeling (RNN)\nLSTM\nHochreiter & Schmidhuber\nGated RNN with Constant Error Carousel\nNeural Computation (1997)\n130,000\n?\n\n\n2013\nGenerative Modeling\nVAE\nKingma & Welling\nProbabilistic autoencoder with latent space\narXiv (2013)\n47,000\n?\n\n\n2014\nImage Classification\nVGG-16\nSimonyan & Zisserman (Oxford)\nDeep ConvNet (16 layers)\narXiv (2014)\n—\n138M\n\n\n2015\nImage Classification\nResNet-50\nHe et al. (Microsoft)\nResidual CNN (skip connections)\narXiv (2015)\n280,000\n?\n\n\n2017\nText-to-Speech (TTS)\nTacotron 2\nGoogle\nSeq2Seq + attention + vocoder\nTacotron 2 (2017)\n2,500\n?\n\n\n2019\nText Classification\nBERT\nGoogle\nBidirectional Transformer (encoder)\nACL (2019)\n141,000\n110M / 340M\n\n\n2020\nText Generation\nGPT-3\nOpenAI\nDecoder-only Transformer (autoregressive)\narXiv (2020)\n51,000\n175B\n\n\n2020\nTranslation (Text2Text)\nT5\nGoogle\nEncoder–Decoder Transformer\nT5 (2020)\n10,000\n220M\n\n\n2020\nImage Classification\nViT\nGoogle\nTransformer over patch embeddings\nViT (2020)\n6,000\n86M\n\n\n2020\nObject Detection\nDETR\nMeta (Facebook)\nTransformer + CNN backbone\nDETR (2020)\n2,000\n?\n\n\n2021\nImage Generation\nDALL·E\nOpenAI\nTransformer + discrete VAE + CLIP\nOpenAI (2021)\n—\n12B\n\n\n2021\nImage Segmentation\nSegFormer\nNvidia\nTransformer-based segmentation architecture\nSegFormer (2021)\n500\n?\n\n\n2022\nSpeech-to-Text (ASR)\nWhisper\nOpenAI\nTransformer-based seq2seq with CTC\nWhisper (2022)\n5,700\n?\n\n\n2022\nImage-to-Text (Caption)\nBLIP\nSalesforce\nVision–language encoder–decoder\nBLIP (2022)\n300\n?\n\n\n2022\nVideo Generation\nImagen Video\nGoogle Research\nCascade of video diffusion models with super-resolution\nImagen Video (2022)\n—\n?\n\n\n2023\nVideo Generation\nVideoPoet\nGoogle Research\nDecoder-only autoregressive Transformer (multimodal)\nVideoPoet (2023)\n—\n?\n\n\n2024\nVideo Generation\nVeo 2\nGoogle DeepMind\n4K, cinematographic control, improved physics\nVeo 2 (2024)\n—\n?\n\n\n2025\nVideo Generation\nVeo 3\nGoogle DeepMind\nAdds synchronized audio (dialogue, SFX, ambience)\nVeo 3 (2025)\n—\n?\n\n\n\n\nTrends\n\nmore parameters\nacademic \\(\\to\\) industry\nopen \\(\\to\\) closed\npapers \\(\\to\\) money\nmulti-modal"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#loss-function",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#loss-function",
    "title": "Neural Networks (Fragments)",
    "section": "Loss function",
    "text": "Loss function\n\nthe objective of neural network"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#initialistion",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#initialistion",
    "title": "Neural Networks (Fragments)",
    "section": "Initialistion",
    "text": "Initialistion\n\ndifferent initialization can lead to different (local) optima in non-convex landscapes\navoid vanishing/exploding gradients\nXavier Initialization (for tanh): \\(Var(w) = 1/n\\)\nKaiming Initialization (for ReLU); \\(Var(w) = 2/n\\)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#activations-functions",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#activations-functions",
    "title": "Neural Networks (Fragments)",
    "section": "Activations Functions",
    "text": "Activations Functions\nAim: introduce non-linearity Problem: saturation and vanishing gradients –&gt; no learning ### tanh\n\nzero-centered\n\n\nReLU\n\n\nSoftmax:\n- interprete output as prob with $\\sum p_i = 1$\n- sees all neurons\n- log-sum trick\n\n$$\n\\log\\sum_i e^{x_i} = x^\\ast + \\log \\sum_i e^{x_i - x^\\ast}\n$$\n\n\nSummary\n\n\n\n\n\n\n\n\n\nActivation Function\nTypical Use\nAdvantages\nDisadvantages\n\n\n\n\nSigmoid\nBinary classification output (logistic regression); hidden units (rarely used now)\n- Smooth output between 0 and 1  - Interpretable as probability\n- Vanishing gradients  - Saturates at extremes  - Not zero-centered\n\n\nTanh\nHidden units in older networks; similar to sigmoid but zero-centered\n- Outputs between -1 and 1  - Zero-centered\n- Still suffers from vanishing gradients  - Saturates for large inputs\n\n\nReLU\nDefault for hidden layers\n- Computationally efficient  - Avoids vanishing gradients for positive inputs  - Sparse activations\n- “Dying ReLU” problem: neurons can output 0 permanently  - Not zero-centered\n\n\nLeaky ReLU\nHidden layers (especially when ReLU is too brittle)\n- Fixes dying ReLU by allowing small gradient when input &lt; 0\n- Slope for negative part is a hyperparameter  - Still not zero-centered\n\n\nSoftmax\nOutput layer for multi-class classification\n- Converts raw scores to probabilities  - Highlights strongest class\n- Not useful for hidden layers  - Sensitive to outliers  - Can be numerically unstable (requires log-sum-exp trick)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#data-preprocessing",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#data-preprocessing",
    "title": "Neural Networks (Fragments)",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nsplitting\ntransformation & normalization"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#data-augmentation",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#data-augmentation",
    "title": "Neural Networks (Fragments)",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\nembedd into ML workflow\nidentify symmetries (invariances): shift, rotate, scale, crop, brightness\naugment only training data\naugment all samples of the training data in same proportions"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#regualarization",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#regualarization",
    "title": "Neural Networks (Fragments)",
    "section": "Regualarization",
    "text": "Regualarization\n\nL2 regularization\n\\[\n\\theta_{k+1} = \\theta_k - \\epsilon \\nabla_\\theta f(\\theta_k) - \\lambda \\theta_k\n\\]\n\npenalizes large parameters\nfavors more even distribution\nimproves generalization, every neuron contributes\n\n\n\nweight decay\n\\[\n\\theta_{k+1} = (1-\\lambda) \\theta_k - \\alpha \\nabla_\\theta f(\\theta_k)\n\\]\nlooks equivalent to L2 norm (for GD), but not for Adam optimizer (momentum) ### data augmentation\n\n\nEarly stopping\n\n\nbagging and ensemble methods\n\nmassive effort for marginal gains (1-2%)\nensemble: train \\(k\\) different models and pool/average\nbagging: use \\(k\\) different data sets\nquantify uncertainty\n\n\n\nDropout\n\ndisable random set of neurons temporarily (20-50%)\nensemble of smaller network within a big one\nreduced network capacity; makes it harder to optimize/minimize loss -&gt; increase training time\nforce all neurons to act –&gt; creates robustness\nredundant representations\nhuman neurons are not reliable!\nmodel.train vs model.eval –&gt; normalize differently\nMonte-Carlo dropout also at inference (p=0.1 - 0.2) run inference multiple times to put confidence estimates on final layer and interpret softmax really probabilistically\nreliable AI: important for doctors\n\n\\[\nz = x. \\theta^T \\to E_{train}[z] = p E_{test}[z]\n\\]\n\n\nBatch Norm\n\nbring activations closer to 0\nc.f normalizing input layer (feature normalization)\nfrom the perspective of a deep layer it’s inputs a changing all the time (while learning) - keep their mean and variance stable (“covariate shift”) –&gt; speeds up learning\nside effect: batch norm also adds a little noise while estimating \\(\\mu, \\sigma^2\\) from mini-batch -&gt; regualrization (c.f dropout)\na test time: which \\(\\mu\\), \\(\\sigma\\) to use? exp. weighted (running) average across minimataches Given \\(z [n_l, m]\\) for some layer \\(l\\) and batch size \\(m\\) \\[\n\\begin{aligned}\n\\mu_i &= \\frac{1}{m} \\sum_j z_{ij} \\\\\n\\sigma_i^2 &= \\frac{1}{m} \\sum_j (\\mu - z_{ij})^2 \\\\\nz^{norm}_{ij} &= (z_{ij} - \\mu_i) / \\sqrt{\\sigma^2_i + \\epsilon} \\\\\n\\tilde{z}_{ij} &= \\gamma z^{norm}_{ij} + \\beta\n\\end{aligned}\n\\]\n\n\n\nOther norms\n\n\n\nWu and He, ECCV 2018"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#interpreting-loss-curves",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#interpreting-loss-curves",
    "title": "Neural Networks (Fragments)",
    "section": "Interpreting Loss curves",
    "text": "Interpreting Loss curves\n–&gt; Andrew NG"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#key-challenges",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#key-challenges",
    "title": "Neural Networks (Fragments)",
    "section": "Key Challenges",
    "text": "Key Challenges\n\nFinding Optimal Solutions\nVanishing/Exploding Gradients\nOverfitting\n\nWhen comparing model performance, compare data + data processing + optimization strategy + model"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#increase-in-compute-for-training",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#increase-in-compute-for-training",
    "title": "Neural Networks (Fragments)",
    "section": "Increase in compute for training",
    "text": "Increase in compute for training"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html",
    "title": "Image Classification",
    "section": "",
    "text": "Given many images and their labels, train a neural network to predict the label of a new image (c.f human learning)."
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#goal",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#goal",
    "title": "Image Classification",
    "section": "",
    "text": "Given many images and their labels, train a neural network to predict the label of a new image (c.f human learning)."
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#repetitions",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#repetitions",
    "title": "Image Classification",
    "section": "Repetitions",
    "text": "Repetitions\n\nDefine Model & Optimization Strategy\nFit Model\nMonitor Fitting\nEvaluate Training"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#new-items",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#new-items",
    "title": "Image Classification",
    "section": "New items",
    "text": "New items\n\nData Splitting: Train & Test\nHow to handle images: data structure"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#get-packages",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#get-packages",
    "title": "Image Classification",
    "section": "Get Packages",
    "text": "Get Packages\n\n\nCode\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split, Subset\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\n\nfrom lecture_utils.helper import plot_cm, detect_device\n\ndevice = detect_device()\n\nprint('torch-version: ', torch.__version__)\nprint('device: ', device)\n\n\ntorch-version:  2.7.1\ndevice:  mps\n\n\n\n\n\n\n\n\nUse GPU if available\n\n\n\n\nGoogle Colab: \\(\\to\\) Change Runtime …\nKaggle: \\(\\to\\) Accelerator (GPU T4)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#get-data-mnist",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#get-data-mnist",
    "title": "Image Classification",
    "section": "Get Data: MNIST",
    "text": "Get Data: MNIST\n\nBackground\nMany famous datasets can be found here:\n\n@pytorch datasets\n@keras datasets\n\nIn the following we will focus on squared images of handwritten digits. They have also been annotated (labeled). For historical background see here: wikipedia\n\n\n\nExamples from MNIST handwritten digits.\n\n\n\nthe challenge\nhighly structured data\ndata collection\nhuman error rate\nLeCun (Bell Labs, Facebook/Meta, Turing Award 2018)\n\n\n\nTest and Training Sets\nFor models with many parameters there is a real danger of overfitting, i.e. learning the specifics of one set of samples rather than generalizable rules.\nFor performance evaluation it is crucial to retain an independent (but representative) test data set that it is never used for fitting. For MNIST we can obtain both train and test data from torchvision.dataset (63 MB)\n\n\nCode\n# define data transformation. toTensor() also includes division by 255!  \ntransform = transforms.Compose([transforms.ToTensor()]) \nfull_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_data  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n\n\n\nData Transformations\nAlert: Usually many more steps are necessary to prepare data for analysis: reading, reformating, filtering, shuffeling, transformation, normalization.\nThis can take up a significant amount of time and it is important to do so consistently for train and test data.\nThe transformation to pytorch tensor is minimal, but notice that this also includes an implicit normalization of images \\([0, 255]\\) (integer) \\(\\to [0,1]\\) (float). This will be done automatically and systematically whenever we access full_train or test_data, but the data set still contains the row data.\n\n\nData Inspection\n\n\nCode\ndef show_ascii_arr(arr):\n  for row in arr:\n      print(\" \".join(f\"{val:3d}\" for val in row))\n\n# accessing all *unnormalized* data\nX,y = full_train.data, full_train.targets\nprint('full_train:', type(full_train))\nprint('X:', type(X), X.shape, X.min(), X.max())\nprint('y:', type(y), y.shape)\n\n# ... as ascii\nshow_ascii_arr(X[0])\n\n# ... as grey-scale image\nplt.imshow(X[0], cmap=\"grey\")\nplt.title(y[0])\nplt.show()\n\n\nfull_train: &lt;class 'torchvision.datasets.mnist.MNIST'&gt;\nX: &lt;class 'torch.Tensor'&gt; torch.Size([60000, 28, 28]) tensor(0, dtype=torch.uint8) tensor(255, dtype=torch.uint8)\ny: &lt;class 'torch.Tensor'&gt; torch.Size([60000])\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255 247 127   0   0   0   0\n  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0\n  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82  82  56  39   0   0   0   0   0\n  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253 253 207   2   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201  78   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many samples and how many features does the MNIST training set have?\n\n\nNotice: usually we access only subsets (by index or DataLoader) those will be transformed as specified in transform()\n\n\nCode\nX,y = full_train[0]\n\nplt.imshow(X[0], cmap=\"grey\")\ntitle=f\"label={y} (min={X.min()}, max={X.max()})\"\nplt.title(title)\nplt.show()\n\n\n\n\n\n\n\n\n\nAbove we obtained train and test directly with torchvision.datasets.\nBut sometimes we need addtional subsets from the train data (e.g. validation data) This can be done as follows\n\n\nCode\nfrom torch.utils.data import random_split\n\n# keep 80% for training\nfract = 0.8 \n\n# define lengths\ntrain_len = int(fract * len(full_train))\nval_len = len(full_train) - train_len\n\n# define split\ntrain_ds, val_ds = random_split(full_train, [train_len, val_len])\n\n# Inspect\nprint('train:', len(train_ds))\nprint('val:', len(val_ds))\nX, y = val_ds[0]\nprint('X:', X.shape, y)\n\n\ntrain: 48000\nval: 12000\nX: torch.Size([1, 28, 28]) 7\n\n\n\n\nStratified sampling\nA slightly more complicated scenario may occur if the classes (encoded by targets) are not well balanced.\nIn this case we need to ensure that the subsampled data respects those proportions, since the validation (and test) data should be representative This is called “stratification” and can be obtained with help of scikit-learn.\nSee here for stratified partitioning: StratifiedShuffleSplit()\n\n\nCode\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom torch.utils.data import Subset\n\n# Use targets directly from dataset\ntargets = full_train.targets.numpy()\nsss = StratifiedShuffleSplit(n_splits=1, train_size=fract, random_state=42)\n\n# sss.split() returns an iterator\ntrain_idx, val_idx = next(sss.split(np.zeros(len(targets)), targets))\n\ntrain_ds = Subset(full_train, train_idx)\nval_ds   = Subset(full_train, val_idx)\nprint('train:', len(train_ds))\nprint('val:', len(val_ds))\n\n# Sanity check - label frequencies\ntrain_labels = train_ds.dataset.targets[train_ds.indices]\ntrain_counts = torch.bincount(train_labels)\ntrain_freq = train_counts / train_counts.sum()\n\nval_labels = val_ds.dataset.targets[val_ds.indices]\nval_counts = torch.bincount(val_labels)\nval_freq = val_counts / val_counts.sum()\n\nprint(train_freq)\nprint(val_freq)\n\n\ntrain: 48000\nval: 12000\ntensor([0.0987, 0.1124, 0.0993, 0.1022, 0.0974, 0.0904, 0.0986, 0.1044, 0.0975,\n        0.0991])\ntensor([0.0988, 0.1123, 0.0993, 0.1022, 0.0973, 0.0903, 0.0987, 0.1044, 0.0975,\n        0.0992])\n\n\n\n\ntrain_test_split alternative\nBelow is a frequently used method from scikit-learn. It is shown here only for reference. But notice that it requires several conversions and normalizations that we’ll need track carefully.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset\nfract = 0.8\n\n# convert from tensors to numpy\nX = full_train.data.numpy()\ny = full_train.targets.numpy()\n\n# convenient implementation of stratification\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=fract, stratify=y)\n\n# convert back to tensors\ntrain_ds = TensorDataset(torch.tensor(X_train[:, None]/255., dtype=torch.float32),\n                                          torch.tensor(y_train, dtype=torch.long))\nval_ds   = TensorDataset(torch.tensor(X_val[:, None]/255., dtype=torch.float32),\n                                          torch.tensor(y_val, dtype=torch.long))\ntest_ds  = TensorDataset(test_data.data[:, None]/255., test_data.targets)\n\n\n\n\nSummary\nIn most machine learinng application we want to have a data split\n\n\n\nTrain-Validation-Test Split. The fractions shown are common, but arbitrary.\n\n\n\n\nGoal Repeat\nBuild a predictor of labels for hand-written digits.\n\n\nCode\nfrom sklearn.decomposition import PCA\n\nn_sub = 500 # define a subset of digits for speed\n\n# flattening a tensor to get [samples x features]\nX_sub = full_train.data[:n_sub].flatten(start_dim=1)\ny_sub = full_train.targets[:n_sub]\nprint('X: ', X_sub.shape)\n\nX_pca = PCA(n_components = 2).fit_transform(X_sub)\nprint('Scores: ',X_pca.shape)\n\ncm = plt.get_cmap('tab10')\nplt.scatter( X_pca[:,0], X_pca[:,1] , c=y_sub, cmap=cm)\nplt.title('PCA of MNIST')\nplt.colorbar()\nplt.show()\n\n\nX:  torch.Size([500, 784])\nScores:  (500, 2)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#data-loading",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#data-loading",
    "title": "Image Classification",
    "section": "Data Loading",
    "text": "Data Loading\nRather than accessing samples per index we frequently want to load batches of a certain sizes into memory for further analysis. Really large data sets may not even fit into memory, so it is useful to define DataLoaders that get data only in batches inot memory.\n\n\nCode\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader   = DataLoader(val_ds, batch_size=64)\ntest_loader  = DataLoader(test_data, batch_size=64)\n\nprint(\"number of train_batches: \", len(train_loader))\nprint(\"number of test_batches:  \", len(test_loader))\n\n# Data Loaders are used a iterator - they return batches of X,y tuples\nX,y = next(iter(train_loader))\nprint('X batch: ', X.shape)\nprint('y batch: ', y.shape)\n\n\nnumber of train_batches:  750\nnumber of test_batches:   157\nX batch:  torch.Size([64, 1, 28, 28])\ny batch:  torch.Size([64])"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#define-model",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#define-model",
    "title": "Image Classification",
    "section": "Define Model",
    "text": "Define Model\n\nRectified Linear Unit\nBasic Non-linearity\n\n\nCode\nclass MNISTModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(28*28, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = F.relu(x)\n        logits = self.fc2(x)\n        return logits\n\n\n\n\nCode\nmodel = MNISTModel().to(device)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#summarize-model",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#summarize-model",
    "title": "Image Classification",
    "section": "Summarize Model",
    "text": "Summarize Model\n\n\nCode\nfrom torchinfo import summary\n\nsummary(model, input_size=(1000, 1, 28, 28), device=device)\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMNISTModel                               [1000, 10]                --\n├─Flatten: 1-1                           [1000, 784]               --\n├─Linear: 1-2                            [1000, 128]               100,480\n├─Linear: 1-3                            [1000, 10]                1,290\n==========================================================================================\nTotal params: 101,770\nTrainable params: 101,770\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 101.77\n==========================================================================================\nInput size (MB): 3.14\nForward/backward pass size (MB): 1.10\nParams size (MB): 0.41\nEstimated Total Size (MB): 4.65\n=========================================================================================="
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#define-loss-function-and-optimizer",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#define-loss-function-and-optimizer",
    "title": "Image Classification",
    "section": "Define Loss Function and Optimizer",
    "text": "Define Loss Function and Optimizer\n\nAdam Optimizer\nReference: DP Kingma et al. 2014. Adam: A method for Stochasitic Optimization. 200k+ citations !!!\nDon’t get stuck in local minima \\(\\to\\) adaptive learning rates\n\n\nCode\n# return sum losses over batch\nloss_function = nn.CrossEntropyLoss(reduction=\"sum\") \noptimizer = torch.optim.Adam(model.parameters())"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#train-model",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#train-model",
    "title": "Image Classification",
    "section": "Train Model",
    "text": "Train Model\n\n\nCode\ndef train_model(model, train_loader, val_loader, epochs=25):\n    history = {'loss': [], 'val_loss': []}\n    for epoch in range(epochs):\n        # training\n        model.train()\n        total_loss = 0\n        # loop over all train data (in batches given by train_loader)\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb)\n            loss = loss_function(logits, yb) # total loss for batch\n\n            optimizer.zero_grad()            # set gradients to 0\n            loss.backward()                  # calculate gradients with backprop\n            optimizer.step()                 # parameter update\n\n            total_loss += loss.item()        # cumulative loss over all batches\n            \n        # average train_loss per sample\n        train_loss = total_loss / len(train_loader.dataset)\n        \n        # Validation\n        model.eval()\n        total_loss = 0\n        # switch off gradient calculation\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits = model(xb)\n                loss = loss_function(logits, yb)\n                total_loss += loss.item() \n        val_loss = total_loss / len(val_loader.dataset)\n        \n        history['loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        \n        print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n    return history\n\nhist = train_model(model, train_loader, val_loader, epochs=20)\n\n\nEpoch 1: train_loss=0.3853, val_loss=0.2311\nEpoch 2: train_loss=0.1840, val_loss=0.1639\nEpoch 3: train_loss=0.1263, val_loss=0.1325\nEpoch 4: train_loss=0.0968, val_loss=0.1099\nEpoch 5: train_loss=0.0760, val_loss=0.1035\nEpoch 6: train_loss=0.0619, val_loss=0.0976\nEpoch 7: train_loss=0.0511, val_loss=0.0917\nEpoch 8: train_loss=0.0423, val_loss=0.0918\nEpoch 9: train_loss=0.0338, val_loss=0.0896\nEpoch 10: train_loss=0.0277, val_loss=0.0880\nEpoch 11: train_loss=0.0239, val_loss=0.0869\nEpoch 12: train_loss=0.0188, val_loss=0.0955\nEpoch 13: train_loss=0.0152, val_loss=0.0916\nEpoch 14: train_loss=0.0136, val_loss=0.0939\nEpoch 15: train_loss=0.0116, val_loss=0.0967\nEpoch 16: train_loss=0.0091, val_loss=0.0949\nEpoch 17: train_loss=0.0084, val_loss=0.1144\nEpoch 18: train_loss=0.0075, val_loss=0.0992\nEpoch 19: train_loss=0.0057, val_loss=0.1231\nEpoch 20: train_loss=0.0060, val_loss=0.1042"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#save-model",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#save-model",
    "title": "Image Classification",
    "section": "Save Model",
    "text": "Save Model\n\n\nCode\noutdir = \"output\"\nos.makedirs(outdir, exist_ok=True)\n\ntorch.save(model.state_dict(), f'{outdir}/mnist_model.pt')\ntorch.save(hist, f\"{outdir}/mnist_history.pt\")"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#load-model",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#load-model",
    "title": "Image Classification",
    "section": "Load Model",
    "text": "Load Model\n\n\nCode\noutdir = \"output\"\nmodel = MNISTModel().to(device)\ntdict = torch.load(f\"{outdir}/mnist_model.pt\", map_location=device)\nmodel.load_state_dict(tdict)\nhist = torch.load(f\"{outdir}/mnist_history.pt\")"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#evaluate-training",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#evaluate-training",
    "title": "Image Classification",
    "section": "Evaluate Training",
    "text": "Evaluate Training\n\n\nCode\ndef plot_fit_history(hist, name='loss', test_score=None):\n    if name not in hist:\n        print(f\"{name} not found in history\")\n        return\n    plt.plot(hist[name], label='train')\n    val_name = 'val_' + name\n    if val_name in hist:\n        plt.plot(hist[val_name], label='valid')\n    if test_score is not None:\n        plt.axhline(test_score, color='green', linestyle='-.', label='test')\n    plt.xlabel('Epoch')\n    plt.ylabel(name)\n    plt.legend()\n    plt.show()\n\nplot_fit_history(hist, 'loss')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepeat\n\n\n\nIn principle the steps can be repeated to improve the model (decrease the training loss) or to speedup the training.\n\nincreasing number of neurons or layers\nwork with regularization techniques or data augmentation\nchange learning algorithm or learning rate\nadjust hyperparameters\n\n! But never show the model the test data until the end !"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#test-model",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#test-model",
    "title": "Image Classification",
    "section": "Test Model",
    "text": "Test Model\nIn principle we can test all test samples (10,000) at once. This is conceptually simpler, but it does requires more care with proper transformations (normalizations) which are run when accessing batches. So in this case we have to normalize by hand\n\n\nCode\n# notice that we need to normalize explicitly\nX = test_data.data.to(device) / 255.0\ny = test_data.targets.to(device)\nprint(X.shape)\nwith torch.no_grad():\n  all_logits = model(X)\n  loss = loss_function(all_logits, y)\n\nall_true = y.detach().cpu()\ntest_loss = loss.cpu() / len(X)\nplot_fit_history(hist, 'loss', test_loss)\n\n\ntorch.Size([10000, 28, 28])\n\n\n\n\n\n\n\n\n\n… more commonly batches are also used over test samples, but this requires some collection\n\n\nCode\n# Evaluate on test\nmodel.eval()\ntotal_loss = 0\nall_logits = []\nall_true = []\nwith torch.no_grad():\n    for xb, yb in test_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        logits = model(xb)\n        loss = loss_function(logits, yb)\n\n        # collect total_loss, predicted logits and true labels yb\n        total_loss += loss.item()\n        all_logits.append(logits.cpu())\n        all_true.append(yb.cpu())\n\ntest_loss = total_loss / len(test_loader.dataset) # mean test loss\nall_logits = torch.cat(all_logits)\nall_true = torch.cat(all_true)\n\nplot_fit_history(hist, 'loss', test_loss)\n\n\n\n\nCode\nall_preds = all_logits.argmax(dim=1).detach().cpu()\ncm = confusion_matrix(all_true.numpy(), all_preds.numpy())\n#print(cm)\nplot_cm(cm)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#task-for-home",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#task-for-home",
    "title": "Image Classification",
    "section": "Task for home",
    "text": "Task for home\n\nRepeat the above, but with the ReLU switch off. Tru to increase the number of nodes or layers\nReality check: scan your own handwritten digit and submit it to your trained model. Pay attention to proper normalization and black/white encoding. Does it work?"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#load-own-image",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#load-own-image",
    "title": "Image Classification",
    "section": "Load own image",
    "text": "Load own image\n\n\nCode\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import TensorDataset\n\nimg_size = test_data.data[0].shape\njpg_path = \"images/ANN_Digit3.jpg\"\n\n# convert to greyscale (L=luminance)\nimg = Image.open(jpg_path).convert(\"L\").resize(img_size)\n\nimg_tensor = transform(img) # transform to tensor\nimg_tensor = 1 - img_tensor # swap black and white\nimg_tensor = img_tensor.unsqueeze(0)\n\nprint(img_tensor.shape)\nplt.imshow(img_tensor[0][0], cmap=\"grey\")\nplt.show()\n\n\ntorch.Size([1, 1, 28, 28])\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport torch.nn.functional as F\n\n# predict logits\nlogits = model(img_tensor.to(device))\nprint('logits = ', logits.detach().cpu().numpy())\n\n# convert to probabilities\nprobs = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n\nplt.figure(figsize=(8,4))\nplt.bar(range(10), probs)\nplt.xlabel(\"Digit\")\nplt.ylabel(\"Probability\")\nplt.xticks(range(10))\nplt.title(\"Model probabilities for each digit\")\nplt.show()\n\n\nlogits =  [[-9.476369  -2.5490894 -2.6994314  3.277071  -3.6264172 -0.784245\n  -6.6809206 -6.64236    0.3123123 -1.4218341]]"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#summary-1",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#summary-1",
    "title": "Image Classification",
    "section": "Summary",
    "text": "Summary\n\nnew data: images as flattened feature vectors\nhighly structured data (unrealistic)\nConsistent use of Data Transformation & Data Loading\nTrain-Validation-Test split (with balancing)\nImportance of non-linearity (\\(\\to\\) RELU)\nrepetiton: model definition and parameter count"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html",
    "title": "Perceptron",
    "section": "",
    "text": "A neuron cell receives electrical signals (dendrites) and passes it to other neurons (axon). Source: wikipedia"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#real-neurons",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#real-neurons",
    "title": "Perceptron",
    "section": "",
    "text": "A neuron cell receives electrical signals (dendrites) and passes it to other neurons (axon). Source: wikipedia"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#neuron-models",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#neuron-models",
    "title": "Perceptron",
    "section": "Neuron Models",
    "text": "Neuron Models\n\n\n\nMcCulloch and Pitts (1943)\n\n\n\n\n\n\n\n\nA brief history of neuron++ models\n\n\n\n\nMcCulloch and Pitts (1943): Neurons as Boolean Gates (synaptic weights and activation thresholds)\nHebbs (1949): Neurons can learn weights; “fire together & wire together”\nRosenblatt (1957): A mathematical learning rule + a machine"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#an-application-binary-classification",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#an-application-binary-classification",
    "title": "Perceptron",
    "section": "An Application: Binary Classification",
    "text": "An Application: Binary Classification\n\n\n\nMark I Perceptron (1958). Distinguish pairs of letters (20x20 pixels) - with 80% accuracy!"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#perceptions-and-citations",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#perceptions-and-citations",
    "title": "Perceptron",
    "section": "Perceptions and Citations",
    "text": "Perceptions and Citations\n– Mechanisation of Thought Processes –\nRosenblatt (1957): “Devices of this sort are expected ultimately to be capable of concept formation, language translation, collation of military intelligence, and the solution of problems through inductive logic.”\n\n\n\nNew York Times, July 7 1958"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#boolean-gates-ai-winter",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#boolean-gates-ai-winter",
    "title": "Perceptron",
    "section": "Boolean Gates & AI Winter",
    "text": "Boolean Gates & AI Winter\nHow to adjust weights and thresholds to calculate Boolean functions?\n\n\n\nBoolean Gates with weights and thresholds. \\(x_1, x_2, y \\in \\{0,1\\}\\)."
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#an-algorithm",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#an-algorithm",
    "title": "Perceptron",
    "section": "An Algorithm",
    "text": "An Algorithm\nGoal given a Boolean truth table \\((x,y) \\to\\) find weights \\(w_l\\) and thresholds \\(b_l\\)\n\n\n\nPerceptron Calculation. A linear collection function + a non-linear activation function.\n\n\n\n\n\n\n\n\n\nLearning = updating weights\n\n\n\n\\[\nw_{l} \\to w_{l} + \\alpha (y - \\hat y)*x_l\n\\]\n\nall Boolean \\(x_l, y, \\hat y \\in \\{0, 1\\}\\)\nmodified Hebbs learning: weights are updated only if target \\(y\\) is not yet reached: \\(y \\ne \\hat y\\)\nonline learning: one sample at a time\nBUT: solution only for linearly separated data (not for XOR)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#classification",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#classification",
    "title": "Perceptron",
    "section": "Classification",
    "text": "Classification\nFor continuous \\(x\\), each neuron in a perceptron defines a (linear) decision boundary based on the value of \\(z\\).\n\\[\nz = w_1 x_1 + w_2 x_2 + b\n\\]\n\n\n\nDecision Boundary for continous \\(x\\)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#universality-with-multiple-layers",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#universality-with-multiple-layers",
    "title": "Perceptron",
    "section": "Universality with Multiple Layers",
    "text": "Universality with Multiple Layers\nWith a multilayer perceptron (MLP) it is possible to calculate XOR, or any Boolean function\n\n\n\nMulti-Layer-Perceptron to calculate XOR. Source: wikipedia\n\n\n\n\n\n\n\n\nBUT\n\n\n\n\nsimple perceptron algorithm only works for single layer and linearly separaberable data.\nneed new algorithm: Backpropagation (Rumelhart, Hinton, Williams; Nature 1986)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#complex-classification",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#complex-classification",
    "title": "Perceptron",
    "section": "Complex Classification",
    "text": "Complex Classification\nBelow I have hand-selected weights and thresholds for an MLP (with 3 hidden nodes) to create 3 decision boundaries that form a triangle.\nBased on this I have sampled points \\((x_1, x_2)\\) and assigned 2 labels (0=outside triangle, 1=inside triangle)\nLet’s explore if we can learn the known boundaries from just the data \\((x,y)\\).\nWe will discuss the algorithm later, but here I just want to illustrate that it seems to work.\n\n\nCode\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# some convenience functions\nfrom lecture_utils.helper import detect_device\nfrom lecture_utils.perceptron_utils import  plot_data_and_boundaries, plot_decision_boundary, get_data, train_model\n\n# define 3 decision boundaries with W and b\nW = np.array([[2,-1],[-2,-1],[0,1]])\nb = np.array([1,1,1]).reshape(1,-1)\n\n# sample data and assign labels based on known boundaries\nns = 10000\nX, labels = get_data(W, b, n_samples = ns)\n\n# plot data and boundaries\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\ntitle_str = f'True Boundaries and {ns} data points'\nplot_data_and_boundaries(ax, X.cpu(), labels.cpu(), W, b, title=title_str)\nplt.show()\n\n\n\n\n\nboundary parameters (w1, w2, b) specify boundary equation: w1 x1 + w2 x2 + b = 0\n\n\n\n\n\n\nCode\n# a simple model with minimal number of layers (see data generation)\nclass MinimalModel(nn.Module):\n    def __init__(self):\n        super(MinimalModel, self).__init__()\n\n        self.hidden_layer = nn.Linear(2, 3)\n        self.output_layer = nn.Linear(3, 1)\n\n    def forward(self, x):\n        x = torch.sigmoid(self.hidden_layer(x)) # sigmoid just for positivity\n        x = torch.sigmoid(self.output_layer(x)) # Sigmoid activation for classification\n        return x\n\n# a complex model with 2 hidden layers\nclass ComplexModel(nn.Module):\n    def __init__(self):\n        super(ComplexModel, self).__init__()\n\n        self.hidden1 = nn.Linear(2, 16)\n        self.hidden2 = nn.Linear(16,32)\n        self.output_layer = nn.Linear(32, 1)\n\n    def forward(self, x):\n        x = torch.sigmoid(self.hidden1(x))\n        x = torch.sigmoid(self.hidden2(x))\n        x = torch.sigmoid(self.output_layer(x)) # Sigmoid activation for classification\n        return x\n\n\n\n\nCode\ndevice = detect_device()\nn_epochs = 9000\nmodel = MinimalModel().to(device)\nX = X.to(device)\nlabels = labels.to(device)\nminimal_model, minimal_loss = train_model(model, X, labels, n_epochs=n_epochs, lr=0.001)\n\n# for the minimal model, we can actually interprete the parameters\n# so I store them here for later\nWp = minimal_model.state_dict()['hidden_layer.weight'].cpu().numpy()\nbp = minimal_model.state_dict()['hidden_layer.bias'].cpu().numpy().reshape(1,-1)\n\nmodel = ComplexModel().to(device)\ncomplex_model, complex_loss = train_model(model, X, labels, n_epochs=n_epochs, lr=0.001) \n\n\nEpoch [9000/9000], Loss: 0.1131\nEpoch [9000/9000], Loss: 0.0078\n\n\n\n\nCode\nfig, axes = plt.subplots(2, 2, figsize=(10, 5))\n\ntitle_str = f'True Boundaries and {ns} data points'\nplot_data_and_boundaries(axes[0][0], X.cpu(), labels.cpu(), W, b, title=title_str)\n\ntitle_str = f'Predicted Boundaries from Minimal Model'\nplot_data_and_boundaries(axes[0][1], X.cpu(), labels.cpu(), Wp, bp, title=title_str)\n\ntitle_str = f\"Minimal Model. Loss = {minimal_loss:.4f}\"\nplot_decision_boundary(axes[1][0], minimal_model, -2, 2, -2, 2, X, labels, title_str)\n\ntitle_str = f\"Complex Model. Loss = {complex_loss:.4f}\"\nplot_decision_boundary(axes[1][1], complex_model, -2, 2, -2, 2, X, labels, title_str)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nTrue Decision Boundaries and Predicted Decision Boundaries for 2 Models\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nfor the simple model (1 hidden layer with 3 neurons) we can still interprete the parameters as the linear boundaries.\ncomplex model (2 hidden layers with 16 + 32 neurons) is difficult to interpret: “black box”\ncomplex model “better”: smaller loss, higher confidence\nthe number of training iterations was quite high for both models (10,000)\nparameterization not unique (scaling factors and order)\n\n\n\nYuval Harari: “intelligence is not about the truth, it’s about the ability to solve problems”"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#universalities",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#universalities",
    "title": "Perceptron",
    "section": "Universalities",
    "text": "Universalities\nA multi-layered perceptron can\n\ncalculate any Boolean function\ncalculate any decision boundary (not just XOR and triangles)\napproximate any continuous function with arbitrary accuracy"
  },
  {
    "objectID": "help/03_BigData.html",
    "href": "help/03_BigData.html",
    "title": "Big Data",
    "section": "",
    "text": "… and common starters for deep learning.\n\n\n\nDataset\nYear\n# Samples\nFeatures per sample\nApprox. Size\nAuthors / Creators\n\n\n\n\n\nMNIST\n~1998‑1999 (released)\n60 000 train + 10 000 test = 70 000 images\n28×28 gray‑scale pixels (784 features)\n~50 MB\nYann LeCun, C. Cortes, C. Burges (via NIST)\n\n\n\nCIFAR‑10\n2009\n50 000 train + 10 000 test = 60 000 images\n32×32 color images (3×32×32 = 3 072 features)\n~170 MB\nAlex Krizhevsky, Vinod Nair, Geoffrey Hinton\n\n\n\nImageNet‑1K\n2009\n1 281 167 train + 50 000 validation + 100 000 test = ~1.43 M images\nHigh‑res RGB images (often resized to 256×256)\n~150 GB\nJia Deng, Li Fei‑Fei, et al. (ImageNet team)"
  },
  {
    "objectID": "help/03_BigData.html#famous-image-datasets",
    "href": "help/03_BigData.html#famous-image-datasets",
    "title": "Big Data",
    "section": "",
    "text": "… and common starters for deep learning.\n\n\n\nDataset\nYear\n# Samples\nFeatures per sample\nApprox. Size\nAuthors / Creators\n\n\n\n\n\nMNIST\n~1998‑1999 (released)\n60 000 train + 10 000 test = 70 000 images\n28×28 gray‑scale pixels (784 features)\n~50 MB\nYann LeCun, C. Cortes, C. Burges (via NIST)\n\n\n\nCIFAR‑10\n2009\n50 000 train + 10 000 test = 60 000 images\n32×32 color images (3×32×32 = 3 072 features)\n~170 MB\nAlex Krizhevsky, Vinod Nair, Geoffrey Hinton\n\n\n\nImageNet‑1K\n2009\n1 281 167 train + 50 000 validation + 100 000 test = ~1.43 M images\nHigh‑res RGB images (often resized to 256×256)\n~150 GB\nJia Deng, Li Fei‑Fei, et al. (ImageNet team)"
  },
  {
    "objectID": "help/03_BigData.html#data-resources",
    "href": "help/03_BigData.html#data-resources",
    "title": "Big Data",
    "section": "Data Resources",
    "text": "Data Resources\nSome popular links - personal bias and far from complete\n\n\n\nRepository / Platform\nCategories Covered\n# of Datasets (approx.)\nTotal Size (approx.)\n\n\n\n\n\nHugging Face Datasets\nNLP, vision, audio, multimodal\nThousands\nTens of TB\n\n\n\nRegistry of Open Data on AWS\nGenomics, earth science, satellite, healthcare\nMany hundreds\nHundreds of TB\n\n\n\nAwesome Public Datasets (GitHub)\nCurated lists across topics—including genomics, medical, climate\nHundreds (topics)\nVariable\n\n\n\nEuropean Genome‑phenome Archive (EGA)\nHuman genomics & phenotypic clinical data\n~4,500 studies from 1,000+ institutions\nTens to hundreds of PB\n\n\n\nThe Cancer Imaging Archive (TCIA)\nMedical imaging (cancer CT, MRI, PET)\nHundreds of collections; millions of images\nTens/hundreds of TB\n\n\n\nMedMNIST v2\nBiomedical images (2D/3D small‑scale)\n~718K images across 18 tasks\nTens of GB"
  },
  {
    "objectID": "help/index.html",
    "href": "help/index.html",
    "title": "Help",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBig Data\n\n\n\nbig data\n\n\n\nFamous image sets and other Big Data\n\n\n\nThomas Manke\n\n\nOct 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytorch Starter\n\n\na basic intro\n\n\n\nThomas Manke\n\n\nOct 5, 2025\n\n\n\n\n\n\nNo matching items"
  }
]