[
  {
    "objectID": "help/03_Pytorch.html",
    "href": "help/03_Pytorch.html",
    "title": "Pytorch Starter",
    "section": "",
    "text": "…using conda environments (also mamba/micromamba)\n# first define dedicated environment\nconda create -n pytorch_env python=3.10\nconda activate pytorch_env\n# from pytorch channel\nconda install pytorch torchvision torchaudio -c pytorch\n… or using pip\npip install pytorch"
  },
  {
    "objectID": "help/03_Pytorch.html#install",
    "href": "help/03_Pytorch.html#install",
    "title": "Pytorch Starter",
    "section": "",
    "text": "…using conda environments (also mamba/micromamba)\n# first define dedicated environment\nconda create -n pytorch_env python=3.10\nconda activate pytorch_env\n# from pytorch channel\nconda install pytorch torchvision torchaudio -c pytorch\n… or using pip\npip install pytorch"
  },
  {
    "objectID": "help/03_Pytorch.html#import",
    "href": "help/03_Pytorch.html#import",
    "title": "Pytorch Starter",
    "section": "Import",
    "text": "Import\n\n\nCode\nimport torch\nprint(f'torch version: {torch.__version__}')\n\n# new functionality/vocabulary (nouns and verbs): dir(torch) \n\n\ntorch version: 2.7.1"
  },
  {
    "objectID": "help/03_Pytorch.html#tensors",
    "href": "help/03_Pytorch.html#tensors",
    "title": "Pytorch Starter",
    "section": "Tensors",
    "text": "Tensors\n\nGeneration\n\n\nCode\nX = [[0,1,2], [3,4,5]] # list\nXt = torch.tensor(X)   # tensor\n\nprint(f'type(X): {type(X)}')\nprint(f'type(Xt): {type(Xt)}')\n\n#help(torch.tensor) # first aid\n#print(\"Xt address:\", Xt.data_ptr()) \n\n\ntype(X): &lt;class 'list'&gt;\ntype(Xt): &lt;class 'torch.Tensor'&gt;\n\n\n\n\nData Type and Type conversion\n\n\nCode\nprint(f'Xt.dtype): {Xt.dtype}') # single data type for elements of Xt\nYt = Xt.to(dtype=torch.float32) # shortcut: Xt.float()\nprint(f'Yt.dtype): {Yt.dtype}') \n\n\nXt.dtype): torch.int64\nYt.dtype): torch.float32\n\n\n\n\nAccess\n\n\nCode\nprint(Xt)\nprint(Xt[:,2]) # simple index, e.g. column=2\nind_select = torch.tensor([[1],[0]])      # complex index (along)\ntorch.gather(Xt, dim=1, index=ind_select) # pick [0,1] and [1,0]\n\n\ntensor([[0, 1, 2],\n        [3, 4, 5]])\ntensor([2, 5])\n\n\ntensor([[1],\n        [3]])\n\n\n\n\nAttributes\n\n\nCode\nprint(f'Xt.shape: {Xt.shape}')\nprint(f'Xt.dtype: {Xt.dtype}')\nprint(f'Xt.device: {Xt.device}')\n\n\nXt.shape: torch.Size([2, 3])\nXt.dtype: torch.int64\nXt.device: cpu\n\n\n\n\nMethods\n\n\nCode\nprint(f'Xt.numpy: {Xt.numpy()}') # converting\nprint(f'Xt.sum: {Xt.sum(dim=0)}') # summarizing along dimensions\nprint(f'Xt.argmax: {Xt.argmax(dim=1)}') # indices with largest value\n\n# many more attributes and methods: dir(Xt)\n\n\nXt.numpy: [[0 1 2]\n [3 4 5]]\nXt.sum: tensor([3, 5, 7])\nXt.argmax: tensor([2, 2])\n\n\n\n\nGeneration by shape\n\n\nCode\nshape=(3,4) # rows, columns\nprint('zeros: ', torch.zeros(shape))\nprint('rand: ', torch.rand(shape))\nprint('randn: ', 1 + 0.01*torch.randn(shape))\n\n\nzeros:  tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\nrand:  tensor([[0.1070, 0.6400, 0.4825, 0.7840],\n        [0.2746, 0.7469, 0.3820, 0.8065],\n        [0.4128, 0.1286, 0.0630, 0.9621]])\nrandn:  tensor([[0.9694, 0.9888, 1.0063, 1.0144],\n        [1.0208, 0.9968, 1.0060, 1.0042],\n        [1.0190, 0.9943, 0.9920, 0.9997]])\n\n\n\n\nReshaping\n\n\nCode\nXt = torch.tensor([[0,1,2], [3,4,5]] )\n\n# a special reshape = flatten\nprint(f'Xt.reshape(6): {Xt.reshape(6)}') \nprint(f'Xt.flatten: {Xt.flatten(start_dim=0)}') \n\n# more complicated reshapes\nYt = Xt.reshape(3,2)\nprint(f'Xt: {Xt}') \nprint(f'Yt: {Yt}') # \n\n\n# Warning: reshape() may not create a copy of Xt\n# Yt shares the same storage with Xt !!\nYt[0,0] = 42\nprint(f'Yt: {Yt}') #\nprint(f'Xt: {Xt}') # \n\n# to copy explicitly use clone()\n#Yt = Xt.clone().reshape(3, 2)\n\n\nXt.reshape(6): tensor([0, 1, 2, 3, 4, 5])\nXt.flatten: tensor([0, 1, 2, 3, 4, 5])\nXt: tensor([[0, 1, 2],\n        [3, 4, 5]])\nYt: tensor([[0, 1],\n        [2, 3],\n        [4, 5]])\nYt: tensor([[42,  1],\n        [ 2,  3],\n        [ 4,  5]])\nXt: tensor([[42,  1,  2],\n        [ 3,  4,  5]])\n\n\n\n\nPermuting\n\n\nCode\nXt = torch.arange(6).reshape(1,2,3) # 3D tensor with first direction 1-D\nprint(f'Xt: {Xt}')  \n\nprint(f'Xt: {Xt.shape}') # \nprint(f'Xt.squeeze: {Xt.squeeze().shape}') # squeezing\nprint(f'Xt.permute: {Xt.permute(2,0,1).shape}') # permuting\n#print(f'Xt.permute: {Xt.permute(2,0,1)}')\n\n\nXt: tensor([[[0, 1, 2],\n         [3, 4, 5]]])\nXt: torch.Size([1, 2, 3])\nXt.squeeze: torch.Size([2, 3])\nXt.permute: torch.Size([3, 1, 2])\n\n\n\n\nCombining\n\n\nCode\nXt = torch.tensor([[0,1,2], [3,4,5]] )\nprint(f'element_wise add {Xt + Xt}') \nprint(f'element_wise mult {Xt * Xt}') \nprint(f'matrix multiply {Xt.T @ Xt}') \n\n\nelement_wise add tensor([[ 0,  2,  4],\n        [ 6,  8, 10]])\nelement_wise mult tensor([[ 0,  1,  4],\n        [ 9, 16, 25]])\nmatrix multiply tensor([[ 9, 12, 15],\n        [12, 17, 22],\n        [15, 22, 29]])"
  },
  {
    "objectID": "help/03_Pytorch.html#devices",
    "href": "help/03_Pytorch.html#devices",
    "title": "Pytorch Starter",
    "section": "Devices",
    "text": "Devices\nPyTorch objects are associated with a device. If possible use GPU for speed.\n@GoogleColab or Kaggle: You may have to change the Runtime/Accelerator\n\n\nCode\ndef detect_device():\n    import torch\n    if torch.backends.mps.is_available():\n        # Mac\n        return \"mps\"\n    elif torch.cuda.is_available():\n        # NVIDIA\n        return \"cuda\"\n    else:\n        return \"cpu\"\n\ndevice = detect_device()\nprint('device: ', device)\n\n\ndevice:  mps\n\n\n\n\nCode\n# Initalize on device\nx = torch.tensor(\n    [3.0, 6.0, 9.0],\n    dtype=torch.float32,\n    device=device)\n\nprint(x)\n# transfer to CPU\nx=x.cpu()\nprint(x)\n\n# transfer to device\nx = x.to(device)\nprint(x)\n\n\ntensor([3., 6., 9.], device='mps:0')\ntensor([3., 6., 9.])\ntensor([3., 6., 9.], device='mps:0')"
  },
  {
    "objectID": "help/03_Pytorch.html#parameters",
    "href": "help/03_Pytorch.html#parameters",
    "title": "Pytorch Starter",
    "section": "Parameters",
    "text": "Parameters\n\n\nCode\nshape=(2,3)\nW = torch.rand(shape, requires_grad=True)\nprint(W.grad_fn)\n\n\nNone"
  },
  {
    "objectID": "help/03_Pytorch.html#autograd",
    "href": "help/03_Pytorch.html#autograd",
    "title": "Pytorch Starter",
    "section": "Autograd",
    "text": "Autograd\n\n\nCode\n# Lead nodes; define by used with\na = torch.tensor(2.0, requires_grad=True)\nb = torch.tensor(3.0, requires_grad=True)\nx = torch.tensor(1.0, requires_grad=True)\n\n# operations in computational graph: nodes z and L\nz = a + 2*b\nL = x*z  # L = x*(a+2b);  dL/dx = z; dL/dz = x\n\n# endows inner nodes (z and L) with gradient functions\nprint(f'z_grad = {z.grad_fn}')\nprint(f'L_grad = {L.grad_fn}')\n\n\nz_grad = &lt;AddBackward0 object at 0x332747490&gt;\nL_grad = &lt;MulBackward0 object at 0x332b2dab0&gt;\n\n\n\n\nCode\na.grad = b.grad = x.grad = None  # clear old grads if any\nz.retain_grad() # per default pytorch keeps gradients only for leave nodes\nL.backward() # fill gradients with respect to L\n\nprint('x.grad:', x.grad) # dL/dx = z  = a + b = 5\nprint('a.grad:', a.grad) # dL/da = (dL/dz) (dz/da) = x * 1\nprint('b.grad:', b.grad) # dL/db = (dL/dz) (dz/db) = x * 1\nprint('z.grad:', z.grad) # dL/dz = x\n\n\nx.grad: tensor(8.)\na.grad: tensor(1.)\nb.grad: tensor(2.)\nz.grad: tensor(1.)"
  },
  {
    "objectID": "help/03_Pytorch.html#legoblocks-torch.nn-module",
    "href": "help/03_Pytorch.html#legoblocks-torch.nn-module",
    "title": "Pytorch Starter",
    "section": "Legoblocks: torch.nn module",
    "text": "Legoblocks: torch.nn module\n\nSimple Functions\n\n\nCode\nX = torch.tensor([-2.0, 0.0, 0.5, 1.0])\nprint('X           ', X)\nprint('sigmoid(X): ', torch.nn.functional.sigmoid(X)) # sigmoid function\nprint('softmax(X): ', torch.nn.functional.softmax(X, dim=0))   # X --&gt; p, \\sum X = 1\nprint('relu(X):    ', torch.nn.functional.relu(X))    # ReLU: max(0, X)\n\n# similar with classes\nrelu = torch.nn.ReLU() # create instance of relu class \nprint('relu(X):    ', relu(X))\n\n\nX            tensor([-2.0000,  0.0000,  0.5000,  1.0000])\nsigmoid(X):  tensor([0.1192, 0.5000, 0.6225, 0.7311])\nsoftmax(X):  tensor([0.0246, 0.1817, 0.2996, 0.4940])\nrelu(X):     tensor([0.0000, 0.0000, 0.5000, 1.0000])\nrelu(X):     tensor([0.0000, 0.0000, 0.5000, 1.0000])\n\n\n\n\nLinear Layers\n\n\nCode\nD_in = 1\nD_out = 3\n# create D_out x D_in matrix of parameters (randomly initialized)\nlinear_layer = torch.nn.Linear(D_in, D_out)\nprint('weight.shape: ', linear_layer.weight.shape)\nprint('weight matrix:', linear_layer.weight)\n\n# fake data\nX = torch.arange(6, dtype=torch.float32).reshape((-1,D_in))\nprint('X.shape: ', X.shape)\n\n# linear transform: y_hat = f(X) = X @ W.T + bias\ny_hat = linear_layer(X)\n\nprint('After linear transformation = matrix multiplication')\nprint('y_hat.shape: ', y_hat.shape)\n\n\nweight.shape:  torch.Size([3, 1])\nweight matrix: Parameter containing:\ntensor([[ 0.4435],\n        [ 0.0648],\n        [-0.1440]], requires_grad=True)\nX.shape:  torch.Size([6, 1])\nAfter linear transformation = matrix multiplication\ny_hat.shape:  torch.Size([6, 3])\n\n\n\n\nEmbeddings\n\n\nCode\nnw = 10 # number of words\nk  = 3  # embedding dimension\n\nembedding_layer = torch.nn.Embedding(nw, k)\nwords_ids = torch.tensor([1, 3, 3, 0, 9]) # integer representation\nwords_vec = embedding_layer(words_ids)    # better representation ?\n\nprint('word vectors: ', words_vec)\n\n\nword vectors:  tensor([[-0.0521,  1.6451, -0.4626],\n        [-0.7369, -1.0085,  1.4893],\n        [-0.7369, -1.0085,  1.4893],\n        [ 1.4594, -0.4211,  1.2548],\n        [ 2.3137,  1.1605,  0.1914]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\n\nDropout Layer\n\n\nCode\nX = torch.ones(1,8)\n\ndropout_layer = torch.nn.Dropout(p=0.3) # set 30% to 0\ndropout_layer.train() # only use during training for robustness\n\nprint(X)\n# illustrate random drops and scaling\nfor i in range(6):\n    X_drop = dropout_layer(X)\n    print(X_drop)\n\n\ntensor([[1., 1., 1., 1., 1., 1., 1., 1.]])\ntensor([[0.0000, 1.4286, 1.4286, 1.4286, 1.4286, 0.0000, 0.0000, 1.4286]])\ntensor([[1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286]])\ntensor([[1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286]])\ntensor([[1.4286, 0.0000, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286, 1.4286]])\ntensor([[0.0000, 1.4286, 1.4286, 0.0000, 1.4286, 1.4286, 1.4286, 0.0000]])\ntensor([[1.4286, 1.4286, 0.0000, 1.4286, 0.0000, 1.4286, 0.0000, 1.4286]])\n\n\n\n\nBuilding Larger Models\nWe can define our own models from building blocks. Ultimately this will help to abstract from individual layers, their initialization, normalization, dropouts etc.\n\n\nCode\nimport torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self, n_in, n_out):\n        # inherit stuff from nn.Module\n        super().__init__()\n        # define layers: just some random examples\n        self.linear = nn.Linear(n_in, n_out)\n        self.norm = nn.Dropout(p=0.3)\n\n    # conncect layers defined above\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.norm(x)\n        x = nn.functional.sigmoid(x)\n        return x\n\nmodel = MyModel(10, 1)\nprint(model)\n\n\nMyModel(\n  (linear): Linear(in_features=10, out_features=1, bias=True)\n  (norm): Dropout(p=0.3, inplace=False)\n)"
  },
  {
    "objectID": "help/03_Pytorch.html#optimizer-and-loss-function",
    "href": "help/03_Pytorch.html#optimizer-and-loss-function",
    "title": "Pytorch Starter",
    "section": "Optimizer and Loss Function",
    "text": "Optimizer and Loss Function\n\n\nCode\nimport torch.optim as optim\n\nlearning_rate = 0.01 # hyperparameter\n\noptimizer = optim.Adam(model.parameters(), lr = learning_rate)\n\nloss_function = nn.MSELoss()"
  },
  {
    "objectID": "help/03_Pytorch.html#model.train-vs.-model.eval",
    "href": "help/03_Pytorch.html#model.train-vs.-model.eval",
    "title": "Pytorch Starter",
    "section": "model.train() vs. model.eval()",
    "text": "model.train() vs. model.eval()\ntwo different modes for training and evaluation\n\n\n\n\n\n\n\n\nFeature\nmodel.train() (Training Mode)\nmodel.eval() (Evaluation Mode)\n\n\n\n\nDropout\nRandomly disables neurons (adds noise)\nAll neurons active (no dropout)\n\n\nBatchNorm\nUses current batch stats (mean/var)\nUses running (saved) stats\n\n\nGradient Tracking\nYes\nUse with torch.no_grad() to disable\n\n\nWhen to use\nDuring training loop\nDuring validation/testing/inference\n\n\n\nTypical use:\n\n\nCode\n# train data\nn_samples, m_features = 100, 10\nx = torch.rand((n_samples, m_features))\ny = torch.bernoulli(x[:,0], 0.5).reshape(-1,1)\n\n# Training\nn_epochs = 10\nmodel.train()\nfor epoch in range(n_epochs):\n\n    output = model(x)           # forward path x -&gt; output\n\n    loss = loss_function(output, y) # loss calculation\n\n    optimizer.zero_grad() # reset gradients (do not accumulate)\n    loss.backward()       # backward path L -&gt; x (gradient calc)\n    optimizer.step()      # update parm = parm - learning_rate * grad\n\n    # print loss\n    if(epoch % 20 == 0):\n        print('epoch {}, loss {}'.format(epoch, loss.data))\n\n# Evaluation (Validation or Test or Prediction)\nx_val = torch.rand((n_samples, m_features))\ny_val = torch.bernoulli(x_val[:,0], 0.5).reshape(-1,1)\n\nmodel.eval()\n# disable grad calculation for speed\nwith torch.no_grad():\n    val_output = model(x_val)\n    val_loss = loss_function(val_output, y_val)\n\n\nepoch 0, loss 0.2729901075363159"
  },
  {
    "objectID": "help/03_Pytorch.html#saving-models",
    "href": "help/03_Pytorch.html#saving-models",
    "title": "Pytorch Starter",
    "section": "Saving models",
    "text": "Saving models\nYou may want to save (and share) your final model. But for long-running jobs it may also be good to save occasional checkpoints during training\n\n\nCode\nimport os\nsave_dir = \"checkpoints\"\nos.makedirs(save_dir, exist_ok=True)\n\n# 1. only saves state_dict, need to know model class\ntorch.save(model.state_dict(), os.path.join(save_dir, \"model_dict.pt\"))\nmodel.load_state_dict(torch.load(\"checkpoints/model_dict.pt\"))\n\n# 2. save full model. Warning: may not port across pytorch versions!!! \ntorch.save(model, os.path.join(save_dir, \"model_full.pth\"))\nmodel = torch.load(\"checkpoints/model_full.pth\", weights_only=False)"
  },
  {
    "objectID": "help/03_Pytorch.html#saving-histories",
    "href": "help/03_Pytorch.html#saving-histories",
    "title": "Pytorch Starter",
    "section": "Saving histories",
    "text": "Saving histories\n\n\nCode\n# 1. npz format (most general and robust)\nnp.savez(os.path.join(save_dir, \"loss_history.npz\"),\n         train=train_losses, val=val_losses)\n\ndata = np.load(\"checkpoints/loss_history.npz\")\ntrain_losses = data[\"train\"]\nval_losses = data[\"val\"]\n\n# 2. pickle format (more flexible less portable)\nwith open(os.path.join(save_dir, \"loss_history.pkl\"), \"wb\") as f:\n    pickle.dump({\"train\": train_losses, \"val\": val_losses}, f)\n\nwith open(\"checkpoints/loss_history.pkl\", \"rb\") as f:\n    losses = pickle.load(f)\ntrain_losses = losses[\"train\"]\nval_losses = losses[\"val\"]"
  },
  {
    "objectID": "help/03_Pytorch.html#saving-everything",
    "href": "help/03_Pytorch.html#saving-everything",
    "title": "Pytorch Starter",
    "section": "Saving Everything",
    "text": "Saving Everything\n\n\nCode\ntorch.save({\n    'model_state': model.state_dict(),\n    'optimizer_state': optimizer.state_dict(),\n    'train_losses': train_losses,\n    'val_losses': val_losses,\n}, os.path.join(save_dir, \"checkpoint_all.pt\"))\n\ncheckpoint = torch.load(\"checkpoints/checkpoint_all.pt\")\nmodel.load_state_dict(checkpoint['model_state'])\noptimizer.load_state_dict(checkpoint['optimizer_state'])\ntrain_losses = checkpoint['train_losses']\nval_losses = checkpoint['val_losses']"
  },
  {
    "objectID": "help/03_Pytorch.html#tensorboard",
    "href": "help/03_Pytorch.html#tensorboard",
    "title": "Pytorch Starter",
    "section": "Tensorboard",
    "text": "Tensorboard\ntensorboard is a popular tool to inspect neural networks and training progress\n\n\nCode\nfrom torch.utils.tensorboard import SummaryWriter\n\n# default `log_dir` is \"runs\" - we'll be more specific here\nwriter = SummaryWriter('log_dir/mnist_experiment')\n\n# typical uses\nwriter.add_graph(model)\nfor epoch in range(100):\n    # training loop\n    # ...\n    # train_loss = loss_function(logits, y)\n    writer.add_scalar(\"Loss\", train_loss, epoch)\n\nwriter.close()"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/practical_tips.html#human-level-performance",
    "href": "lectures/02_NeuralNetworks/practical_tips.html#human-level-performance",
    "title": "Practical Tips",
    "section": "Human Level Performance",
    "text": "Human Level Performance"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/practical_tips.html#error-analysis",
    "href": "lectures/02_NeuralNetworks/practical_tips.html#error-analysis",
    "title": "Practical Tips",
    "section": "Error Analysis",
    "text": "Error Analysis"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/practical_tips.html#data-mismatch-trainig-test",
    "href": "lectures/02_NeuralNetworks/practical_tips.html#data-mismatch-trainig-test",
    "title": "Practical Tips",
    "section": "Data Mismatch (Trainig, Test)",
    "text": "Data Mismatch (Trainig, Test)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/practical_tips.html#small-data-transfer-learning",
    "href": "lectures/02_NeuralNetworks/practical_tips.html#small-data-transfer-learning",
    "title": "Practical Tips",
    "section": "Small Data: Transfer Learning",
    "text": "Small Data: Transfer Learning"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/practical_tips.html#multi-task-learning",
    "href": "lectures/02_NeuralNetworks/practical_tips.html#multi-task-learning",
    "title": "Practical Tips",
    "section": "Multi-task Learning",
    "text": "Multi-task Learning"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/practical_tips.html#end-to-end-learning",
    "href": "lectures/02_NeuralNetworks/practical_tips.html#end-to-end-learning",
    "title": "Practical Tips",
    "section": "End-to-end Learning",
    "text": "End-to-end Learning"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html",
    "title": "Backpropagation",
    "section": "",
    "text": "Find the gradients with respect to parameters. How much should paramters be changed to reduce the loss?"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#goal",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#goal",
    "title": "Backpropagation",
    "section": "",
    "text": "Find the gradients with respect to parameters. How much should paramters be changed to reduce the loss?"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#a-simple-network",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#a-simple-network",
    "title": "Backpropagation",
    "section": "A simple network",
    "text": "A simple network\n\n\n\nLogistic regression with two input \\((x_1, x_2\\)) and one output activation \\(a \\in [0,1]\\)\n\n\nThe network encodes a calculation that is applied to each sample"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#calculating-forward-computational-graph",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#calculating-forward-computational-graph",
    "title": "Backpropagation",
    "section": "Calculating Forward: Computational Graph",
    "text": "Calculating Forward: Computational Graph\n\n\n\nComputational Graph shows how data flows through a graph of basic operations (nodes)\n\n\nThe loss function: \\[\nL(a,y) = -y \\log(a) - (1-y)\\log(1-a)\n\\]"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#calculating-backward",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#calculating-backward",
    "title": "Backpropagation",
    "section": "Calculating Backward",
    "text": "Calculating Backward\nThe derivatives can be calculates \\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial a} &= -\\frac{y}{a} + \\frac{1-y}{1-a}\\\\ \\\\\n\\frac{\\partial L}{\\partial z} &= \\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial z} = \\frac{\\partial L}{\\partial a}a(1-a)\\\\ \\\\\n\\frac{\\partial L}{\\partial w_1} &= \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial w_1} = \\frac{\\partial L}{\\partial z} x_1\\\\\\\\\n\\frac{\\partial L}{\\partial w_2} &= \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial w_2} = \\frac{\\partial L}{\\partial z} x_2\\\\\\\\\n\\frac{\\partial L}{\\partial b} &= \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial b} = \\frac{\\partial L}{\\partial z} 1\n\\end{aligned}\n\\]\n\n\n\n\n\n\nBackward Calculation\n\n\n\n\nderivatives = lookup and multiplication\ndecompose larger calculation into smaller local parts\nstrict computational flow ensures that all required contributions are known\nimportance of differentiable activation functions: \\(\\frac{\\partial a}{\\partial z}\\)\nattaches a gradient to each forward value: \\(a \\to da \\equiv \\frac{\\partial L}{\\partial a}\\)\nuse matrices for book-keeping\nflexibility: other loss functions, other inputs, more layers\nthis is for one sample \\((\\mathbf{x},y)\\) only"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#summing-over-all-sample",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#summing-over-all-sample",
    "title": "Backpropagation",
    "section": "Summing over all sample",
    "text": "Summing over all sample\n\\[\nL = \\frac{1}{N} \\sum_i L(\\hat y_i, y_i)\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\nfor iid samples: maximizing log-likelihood\ngradients accummulate: e.g. \\(\\frac{\\partial L}{\\partial b} = \\sum_i \\frac{\\partial L_i}{\\partial b}\\)\nin practice this is done for **batches* of training samples"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#summary",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#summary",
    "title": "Backpropagation",
    "section": "Summary",
    "text": "Summary\n\n\n\nForward and Backward Propagation through layer \\(l\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ngradients are byproduct of backpropagation\nshapes of gradients agree with shape of tensors\nmatrix notation for simplification and vectorized speedup\nlookup and matrix multiplication"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#further-reading",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#further-reading",
    "title": "Backpropagation",
    "section": "Further reading",
    "text": "Further reading\n\n3Blue1Brown. Backpropagation.\nAutomatic Differentiation.\nAndrew Ng @ 6:23h\n\nobserved data: \\(x\\) true label: \\(y\\) predicted label probability: \\(\\hat y = f(x, \\theta)\\) \\[\np(y|x) = \\hat{y}^y (1-\\hat{y})^y \\to \\mbox{maximize !}\n\\]\nLoss for one sample \\(x\\) \\[\nL(\\hat y, y) = - \\log p(y|x) =  - y \\log \\hat y - (1-y) \\log (1 - \\hat y) \\to \\mbox{minimize !}\n\\]\nLoss for all \\(N\\) samples \\((x_1,y_1),  (x_2,y_2) \\ldots (x_N, y_N)\\)\n\\[\nL(, x) = \\frac{1}{N} \\sum_i L(\\hat y_i, y_i)\n\\]"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#the-strategy-iteration",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#the-strategy-iteration",
    "title": "Backpropagation",
    "section": "The strategy: iteration",
    "text": "The strategy: iteration\n\nGradient Descent\n\n\nADAM\n\n\nA single neuron\n\n\nA network\n\nmultiple logistic regressions\ninput layers, hidden layers (representations), output layer\nactivations (values that neurons path on to next layer)\nnotation: 2 layer network (without input)\nparameters: weights W and biases b - dimensions and shapes"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#the-tool-backpropagation",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#the-tool-backpropagation",
    "title": "Backpropagation",
    "section": "The tool: Backpropagation",
    "text": "The tool: Backpropagation\n\nComputation Graph\n\n\nOverview\n\nfrom a single neuron to network\nfrom numbers to matrices\nstacked layers of computation (passing information)\nNN = matrix multiplications\nNN = complex, non-linear functions\n\n\n\nGPU and vectorized implementation\n\nmatrix multiplication for one sample x = column vector [m x 1]\nmatrix multiplication for n samples x = [m x n] matrix"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#initialistion",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#initialistion",
    "title": "Backpropagation",
    "section": "Initialistion",
    "text": "Initialistion\n\ndifferent initialization can lead to different (local) optima in non-convex landscapes\navoid vanishing/exploding gradients\nXavier Initialization (for tanh): \\(Var(w) = 1/n\\)\nKaiming Initialization (for ReLU); \\(Var(w) = 2/n\\)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#activations-functions",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#activations-functions",
    "title": "Backpropagation",
    "section": "Activations Functions",
    "text": "Activations Functions\nAim: introduce non-linearity\n\\[\n\\begin{aligned}\nz_1 & = W_1 \\cdot x + b_1 \\\\\nz_2 & = W_2 \\cdot z_1 + b_2 \\\\\n& = W_2 ( W_1 \\cdot x + b_1) + b_2 \\\\\n&= W^\\prime x + b^\\prime\n\\end{aligned}\n\\]\n\n\n\n\n\n\nUse (non-linear) activation functions\n\n\n\n\nfor hidden layers for richer expressiveness\nonly exception: final layers for regression problems\n\n\n\n\n\nCode\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nx = torch.linspace(-5, 5, steps=200)\n\n\n\nsigmoid\n\\[\n\\begin{aligned}\ng(z) &= \\frac{1}{1+\\exp(-z)}\\\\\ng'(z) &= g(z) (1 - g(z))\n\\end{aligned}\n\\]\n\n\nCode\ny_sigmoid = torch.sigmoid(x)\ny_prime = y_sigmoid * ( 1 - y_sigmoid) \nplt.figure(figsize=(8, 5))\nplt.plot(x, y_sigmoid, label='Sigmoid', linewidth=2)\nplt.plot(x, y_prime, label='Derviative', linewidth=2)\nplt.title(\"Sigmoid and Derivative\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\n\n\n\nsaturation and vanishing gradients –&gt; no learning\n\n\n\n\ntanh\n\\[\n\\begin{aligned}\ng(z) &= \\tanh(z)\\\\\ng'(z) &= 1 - g(z)^2\n\\end{aligned}\n\\]\n\n\nCode\ny_tanh = torch.tanh(x)\ny_prime = 1 - y_tanh**2\nplt.figure(figsize=(8, 5))\nplt.plot(x, y_tanh, label='tanh', linewidth=2)\nplt.plot(x, y_prime, label='Derviative', linewidth=2)\nplt.title(\"tanh and Derivative\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nzero-centered\n\n\n\nReLU\n\\[\n\\begin{aligned}\ng(z)  &= \\max(0,z)\\\\\ng'(z) &= 0 \\mbox{ for } z &lt; 0 \\\\\n      &= 1 \\mbox{ for } z \\ge 0\n\\end{aligned}\n\\]\n\n\nCode\ny_relu = F.relu(x)\ny_prime = (x &gt; 0).float()\nplt.figure(figsize=(8, 5))\nplt.plot(x, y_relu, label='ReLU', linewidth=2)\nplt.plot(x, y_prime, label='Derviative', linewidth=2)\nplt.title(\"ReLU and Derivative\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSoftmax:\n\ninterprete output as prob with \\(\\sum p_i = 1\\)\nsees all neurons\nlog-sum trick\n\n\\[\n\\log\\sum_i e^{x_i} = x^\\ast + \\log \\sum_i e^{x_i - x^\\ast}\n\\]\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\nSigmoid\nTanh\nReLU\nLeakyReLU\n\n\n\n\nMathematical Range\n(0, 1)\n(-1, 1)\n[0, ∞)\n(-∞, ∞)\n\n\nAdvantages\n• Smooth, interpretable as probability  • Historically important\n• Zero-centered  • Smooth transitions around 0\n• Simple, fast computation  • Sparse activations  • No vanishing gradient for x&gt;0\n• Avoids dying ReLU problem  • Retains advantages of ReLU\n\n\nDisadvantages\n• Vanishing gradient for large |x|  • Not zero-centered  • Saturation\n• Vanishing gradient for large |x|  • More expensive than ReLU\n• Dying ReLU: neurons stuck at 0  • Unbounded outputs → exploding activations\n• Slightly more computation than ReLU  • Still unbounded above • new hyperparameter \n\n\nTypical Use Cases\n• Binary classification output\n• Recurrent networks  • Some hidden layers\n• Default choice for deep nets’ hidden layers\n• When ReLU causes dead neurons or sparse gradients\n\n\n\n\n\n\n\n\n\n\n\n\nActivation Function\nTypical Use\nAdvantages\nDisadvantages\n\n\n\n\nSigmoid\nBinary classification output (logistic regression); hidden units (rarely used now)\n- Smooth output between 0 and 1  - Interpretable as probability\n- Vanishing gradients  - Saturates at extremes  - Not zero-centered\n\n\nTanh\nHidden units in older networks; similar to sigmoid but zero-centered\n- Outputs between -1 and 1  - Zero-centered\n- Still suffers from vanishing gradients  - Saturates for large inputs\n\n\nReLU\nDefault for hidden layers\n- Computationally efficient  - Avoids vanishing gradients for positive inputs  - Sparse activations\n- “Dying ReLU” problem: neurons can output 0 permanently  - Not zero-centered\n\n\nLeaky ReLU\nHidden layers (especially when ReLU is too brittle)\n- Fixes dying ReLU by allowing small gradient when input &lt; 0\n- Slope for negative part is a hyperparameter  - Still not zero-centered\n\n\nSoftmax\nOutput layer for multi-class classification\n- Converts raw scores to probabilities  - Highlights strongest class\n- Not useful for hidden layers  - Sensitive to outliers  - Can be numerically unstable (requires log-sum-exp trick)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#data-preprocessing",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#data-preprocessing",
    "title": "Backpropagation",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nsplitting\ntransformation & normalization"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#data-augmentation",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#data-augmentation",
    "title": "Backpropagation",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\nembedd into ML workflow\nidentify symmetries (invariances): shift, rotate, scale, crop, brightness\naugment only training data\naugment all samples of the training data in same proportions"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#regualarization",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#regualarization",
    "title": "Backpropagation",
    "section": "Regualarization",
    "text": "Regualarization\n\nL2 regularization\n\\[\n\\theta_{k+1} = \\theta_k - \\epsilon \\nabla_\\theta f(\\theta_k) - \\lambda \\theta_k\n\\]\n\npenalizes large parameters\nfavors more even distribution\nimproves generalization, every neuron contributes\n\n\n\nweight decay\n\\[\n\\theta_{k+1} = (1-\\lambda) \\theta_k - \\alpha \\nabla_\\theta f(\\theta_k)\n\\]\nlooks equivalent to L2 norm (for GD), but not for Adam optimizer (momentum) ### data augmentation\n\n\nEarly stopping\n\n\nbagging and ensemble methods\n\nmassive effort for marginal gains (1-2%)\nensemble: train \\(k\\) different models and pool/average\nbagging: use \\(k\\) different data sets\nquantify uncertainty\n\n\n\nDropout\n\ndisable random set of neurons temporarily (20-50%)\nensemble of smaller network within a big one\nreduced network capacity; makes it harder to optimize/minimize loss -&gt; increase training time\nforce all neurons to act –&gt; creates robustness\nredundant representations\nhuman neurons are not reliable!\nmodel.train vs model.eval –&gt; normalize differently\nMonte-Carlo dropout also at inference (p=0.1 - 0.2) run inference multiple times to put confidence estimates on final layer and interpret softmax really probabilistically\nreliable AI: important for doctors\n\n\\[\nz = x. \\theta^T \\to E_{train}[z] = p E_{test}[z]\n\\]\n\n\nBatch Norm\n\nbring activations closer to 0\n\n\n\nOther norms\n\n\n\nWu and He, ECCV 2018"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#interpreting-loss-curves",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#interpreting-loss-curves",
    "title": "Backpropagation",
    "section": "Interpreting Loss curves",
    "text": "Interpreting Loss curves\n–&gt; Andrew NG"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/01_Backpropagation.html#key-challenges",
    "href": "lectures/02_NeuralNetworks/01_Backpropagation.html#key-challenges",
    "title": "Backpropagation",
    "section": "Key Challenges",
    "text": "Key Challenges\n\nFinding Optimal Solutions\nVanishing/Exploding Gradients\nOverfitting\n\nWhen comparing model performance, compare data + data processing + optimization strategy + model"
  },
  {
    "objectID": "lectures/01_MachineLearning/05_MultinomialRegression.html",
    "href": "lectures/01_MachineLearning/05_MultinomialRegression.html",
    "title": "Multinomial Regression",
    "section": "",
    "text": "A neuron layer receiving multiple inputs - and calculating multiple outputs.\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport torch\nimport torch.nn as nn"
  },
  {
    "objectID": "lectures/01_MachineLearning/05_MultinomialRegression.html#generalization-more-output-variables",
    "href": "lectures/01_MachineLearning/05_MultinomialRegression.html#generalization-more-output-variables",
    "title": "Multinomial Regression",
    "section": "",
    "text": "A neuron layer receiving multiple inputs - and calculating multiple outputs.\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport torch\nimport torch.nn as nn"
  },
  {
    "objectID": "lectures/01_MachineLearning/05_MultinomialRegression.html#get-iris-data",
    "href": "lectures/01_MachineLearning/05_MultinomialRegression.html#get-iris-data",
    "title": "Multinomial Regression",
    "section": "Get Iris data",
    "text": "Get Iris data\n\n\nCode\n# get iris data from sklearn\niris = datasets.load_iris() \nX = iris.data # all four columns  \ny = iris.target  # forth column\n\n# convert to tensors\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.long)\nprint(f\"X.shape = {X.shape}, y.shape = {y.shape}\")\n\n\nX.shape = torch.Size([150, 4]), y.shape = torch.Size([150])"
  },
  {
    "objectID": "lectures/01_MachineLearning/05_MultinomialRegression.html#model-training",
    "href": "lectures/01_MachineLearning/05_MultinomialRegression.html#model-training",
    "title": "Multinomial Regression",
    "section": "Model Training",
    "text": "Model Training\n… requires minimal changes to pytorch code\n\n\nCode\nn_epochs = 10000\nmodel = nn.Linear(4, 3)  # change: n_in = 4 (features), n_out = 3 (classes)\nloss_func = nn.CrossEntropyLoss()  # Cross-Entropy\n\n# all below as before\noptimizer = torch.optim.SGD(model.parameters()) \n\nmodel.train()\nfor epoch in range(n_epochs):\n    # model predictions are interpreted as z = logits\n    z = model(X)  \n    loss = loss_func(z, y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(\"Fitted parameters:\")\nprint(\"Intercept (bias):\", model.bias.detach().numpy())\n\n## Notice more weights\nprint(\"Coefficients (weights):\", model.weight.detach().numpy())\nprint('Loss: ', loss)    \n\n\nFitted parameters:\nIntercept (bias): [ 0.4097849  -0.00864549  0.22877324]\nCoefficients (weights): [[ 0.31035995  0.61378133 -1.4084837  -0.27761978]\n [ 0.09734378 -0.3853656   0.26491988 -0.44041577]\n [-0.6314199  -0.75229895  1.0699543   0.51602477]]\nLoss:  tensor(0.3601, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "lectures/01_MachineLearning/05_MultinomialRegression.html#predict-and-plot",
    "href": "lectures/01_MachineLearning/05_MultinomialRegression.html#predict-and-plot",
    "title": "Multinomial Regression",
    "section": "Predict and Plot",
    "text": "Predict and Plot\nNow the model predictions \\(z\\) (logits) can be interpreted as probabilities, and the class with the maximal probability will be the class prediction.\n\n\n\n\n\n\nConverting logits to probabilities\n\n\n\n\n\n\\[\np_k = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}\n\\]\nThis function is also called softmax, because it basically gives the largest probability for largest logit \\(z_k\\) (for every sample)\nThe probabilities are normalized \\[\n\\sum_{k=1}^{K} p_k = 1\n\\]\n\n\n\n\n\n\n\n\n\nCross-entropy loss\n\n\n\nfor one sample \\[\n\\mathcal{L}(\\mathbf{y}, \\mathbf{p}) = -\\sum_{k=1}^{K} y_k \\log(p_k)\n\\]\nfor all samples\n\\[\nL = \\frac{1}{N} \\sum_i \\mathcal{L}(\\mathbf{y}_i, \\mathbf{p}_i)\n\\]\n\n\n\n\nCode\nmodel.eval()\nwith torch.no_grad():\n    z = model(X)\n    y_prob = z.softmax(dim=1) # for easier interpretation\n    y_pred = z.argmax(dim=1)  # class predition = largest logit\n    acc = (y_pred == y).float().mean().item()\n\nprint('y_true:     ', y[:3])\nprint('z (logits): ', z[:3])\nprint('y_prob:     ', y_prob[:3])\nprint('y_pred:     ', y_pred[:3])\n\nprint(f\"\\n\\nTraining accuracy: {acc:.3f}\")\n\n\ny_true:      tensor([0, 0, 0])\nz (logits):  tensor([[ 2.1135, -0.5782, -4.0234],\n        [ 1.7445, -0.4050, -3.5209],\n        [ 1.9460, -0.5280, -3.6521]])\ny_prob:      tensor([[0.9346, 0.0633, 0.0020],\n        [0.8915, 0.1039, 0.0046],\n        [0.9192, 0.0774, 0.0034]])\ny_pred:      tensor([0, 0, 0])\n\n\nTraining accuracy: 0.967"
  },
  {
    "objectID": "lectures/01_MachineLearning/05_MultinomialRegression.html#confusion-matrix",
    "href": "lectures/01_MachineLearning/05_MultinomialRegression.html#confusion-matrix",
    "title": "Multinomial Regression",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\n\nCode\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n\ncm = confusion_matrix(y, y_pred.numpy())\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=iris.target_names)\ndisp.plot(cmap='Blues')"
  },
  {
    "objectID": "lectures/01_MachineLearning/05_MultinomialRegression.html#classification-report",
    "href": "lectures/01_MachineLearning/05_MultinomialRegression.html#classification-report",
    "title": "Multinomial Regression",
    "section": "Classification Report",
    "text": "Classification Report\n\n\nCode\nreport = classification_report(y, y_pred.numpy(), target_names=iris.target_names)\nprint(report)\n\n\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        50\n  versicolor       1.00      0.90      0.95        50\n   virginica       0.91      1.00      0.95        50\n\n    accuracy                           0.97       150\n   macro avg       0.97      0.97      0.97       150\nweighted avg       0.97      0.97      0.97       150\n\n\n\nConfusion Matrix & Jargon (Wikipedia)"
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#models",
    "href": "lectures/01_MachineLearning/03_Models101.html#models",
    "title": "Models 101",
    "section": "Models",
    "text": "Models\nGoal: Use some variables in the data to predict others.\n\n\n\n\n\n\n\n\n\nJargon Alert: “predictors” (“independent variables”, “features”, \\(X\\)) predict “responses” (“dependent variables”, \\(Y\\)).\nCausality Alert: a good, complex, deep or even perfect model does not mean that we found a causal connection \\(X \\to Y\\).\n\nMathematical Goal\nModel expected value \\(\\hat y_i\\) as a (linear) function of \\(x_i\\)\n\\[\n\\begin{array}{ll}\ny_i &= \\hat y_i + \\epsilon_i ~~~~~ i = 1 \\ldots n \\\\ \\\\\n\\hat y_i &= f(x_i, \\theta) = \\theta_0 + \\theta_1 x_i \\\\ \\\\\n\\epsilon_i &\\sim N(0, \\sigma^2)\n\\end{array}\n\\]\nand in matrix form (also for multiple variables)\n\n\n\nMatrix form of linear regression\n\n\n\\[\nY = X \\theta + \\epsilon\n\\]\n\n\n\n\n\n\nCompact notation is useful:\n\n\n\n\njargon: model matrix \\(X\\)\nextendable to multiple predictor variables \\(y_i=f(x_i, z_i, \\ldots)\\)\nextendable to categorical variables (dummy)\nsame intersect (\\(\\theta_0\\)) for all observation\nfocus on shapes and rules of matrix multiplication\nfor each observation: same parameters, different error terms\n\n\n\n\n\nLoss functions: Residual Sum of Squares\nFor \\(n\\) data points, choose parameter vector \\(\\theta\\) by ordinary least squares:\n\\[\nRSS(\\theta) = \\sum_{i=1}^{n} (y_i - \\hat y_i)^2 = \\sum_i \\epsilon_i^2 = \\epsilon^T \\epsilon \\to min\n\\]\n\n\nCode\nset.seed(42)\nxr &lt;- seq(-3, 3, by=0.1)  \n\n# Generate parameter combinations and linear functions over xr\nresult &lt;- params_func(5, pr=c(-2,2,-2,2,0,0), xr=xr)\n\n# Create xy-plot\nplot_xy &lt;- result$func %&gt;%\n  ggplot(aes(x = x, y = y, color = point)) +\n  geom_line() +\n  labs(title = \"X-Y Space: Linear Models\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Create parameter space plot\nplot_param &lt;- result$params %&gt;%\n  ggplot(aes(x = beta0, y = beta1, color = point)) +\n  geom_point(size = 2) +\n  labs(title = \"Parameter Space: Models\", x = \"theta0\", y = \"theta1\") +\n  xlim(-5, 5) +  ylim(-5, 5) + \n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# generate data from quadratic function with one parameter combination + noise\n# for a range of x values in xr\nres &lt;- params_func(1, pr=c(1,1,2,2,0.5,0.5), xr=xr)\ndata &lt;- res$func %&gt;% mutate(yd = y + rnorm( n() ))\n\n# plot data\nplot_data &lt;- ggplot(data, aes(x = x, y = yd)) +\n  geom_point(alpha=0.3) +                                       # data\n  geom_line(aes(y = y), linetype = \"dashed\") +                  # true function\n  geom_smooth(method='lm', formula='y~x', colour=\"lightblue\") + # best linear fit\n  labs(title = \"X-Y Space: Sampled Data\", x=\"x\", y=\"y\") +\n  theme_minimal()\n\n# calculate Residual Sum of Squares for each combinations (beta0, beta1) in grid\nbeta0_seq &lt;- seq(-5, 5, length.out = 100)\nbeta1_seq &lt;- seq(-5, 5, length.out = 100)\ngrid &lt;- expand_grid(beta0 = beta0_seq, beta1 = beta1_seq) %&gt;%\n  mutate(RSS = map2_dbl(beta0, beta1, function(b0, b1) {\n    sum((data$yd - (b0 + b1 * xr))^2)\n  }))\n\n# Plot RSS + contour\nplot_rss &lt;- ggplot(grid, aes(x = beta0, y = beta1, z = RSS)) +\n  geom_tile(aes(fill = RSS)) +  geom_contour(color = \"grey\") +  \n  scale_fill_gradient(low = \"lightblue\", high = \"red\") +\n  labs(title = \"Parameter Space: RSS Contours\", x = \"theta0\", y = \"theta1\") + \n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nplot_xy + plot_param + plot_data + plot_rss + plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEach parameter combination corresponds to a specific (linear) model\nBetter models have lower RSS (= “cost” = “loss”)\nsmallest RSS corresponds to maximal likelihood: \\(P(Y|M)\\)\neven the “best” model may be wrong\n“all models are wrong, some are useful”"
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#goal-revisited",
    "href": "lectures/01_MachineLearning/03_Models101.html#goal-revisited",
    "title": "Models 101",
    "section": "Goal: revisited",
    "text": "Goal: revisited\nThe linear regression has minimzed the residual sum of squares \\(RSS\\). Below I visualize (in red) the remaining residual errors as deviations of the fitted line from the true data. I also compare it to the mean model where no dependency on X=Petal.Length is used.\n\n\nCode\nmu = iris$Petal.Width %&gt;% mean                      # simplest model\nmodel &lt;- lm(Petal.Width ~ Petal.Length, data=iris)  # linear model\n\np1 &lt;- iris %&gt;%\n  mutate(mean = mu) %&gt;%\n  ggplot(aes(x = 0, y = Petal.Width)) +\n  geom_point() +\n  geom_hline(yintercept=mu, colour=\"blue\") +\n  geom_segment(aes(xend = 0, yend = mean), color = \"red\") + \n  theme(\n    axis.title.x = element_blank(), \n    axis.text.x = element_blank(), \n    axis.ticks.x = element_blank()) +\n  ggtitle('Mean Model')\n\n\np2 &lt;- iris %&gt;%\n  mutate(predicted = predict(model, iris),\n         residuals = residuals(model)\n         ) %&gt;%\n  ggplot(aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point() +\n  geom_line(aes(y=predicted), color=\"blue\") +\n  geom_segment(aes(xend = Petal.Length, yend = predicted), color = \"red\") +  \n  ggtitle('Linear Model: Residual Sum of Squares')\n\np1 + p2 + plot_layout(widths = c(1, 3))"
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#model-fit-model-train",
    "href": "lectures/01_MachineLearning/03_Models101.html#model-fit-model-train",
    "title": "Models 101",
    "section": "Model Fit = Model Train",
    "text": "Model Fit = Model Train\n\n\n\n\n\n\nMathematical Solution\n\n\n\n\n\n\nMatrix Algebra for Expectations\nFor linear regression, \\(f(x,\\theta) =X \\theta\\), there is an explicit formula for the parameters \\(\\hat \\theta\\) and \\(\\hat \\sigma\\)\n\\[\n\\begin{array}{ll}\n\\hat {\\theta} &= (X^TX)^{-1} X^T Y\\\\\n\\hat {\\sigma}^2 &= \\epsilon^T \\epsilon / (n-p) = RSS /(n-p)\n\\end{array}\n\\]\n\n\nStatistical Analysis\nRemaining sampling fluctuation around \\(\\hat y\\) (measured by \\(\\hat \\sigma\\)) also induced uncertainties in the parameter estimate (error propagation)\n\\[\nCov(\\theta) = \\hat \\sigma (X^TX)^{-1}\n\\]\nGiven sampling fluctuations, and the corresponding uncertainty in the parameter estimate, we want to quantify our surprise for \\(\\hat {\\theta} \\ne 0\\) assuming that \\(\\theta=0\\). This can be done by a confidence interval\n\\[\n\\hat {\\theta} \\pm c \\cdot \\sigma_\\theta\n\\] There choice for \\(c\\) is arbitrary and it corresponds to our desired level of confidence that the “true” parameter \\(\\theta\\) is contained within this interval (\\(c=1.96\\) is a popular choice).\nOften we are interested if the confidence interval contains zero (no dependence). The p-value denotes the probability that zero is outside the region, even if the true value \\(\\theta=0\\)\n\n\n\n\n\n\n\n\n\n\nPractical Solutions\n\n\n\n\n\nEasy with R: lm()\n\n\nCode\n# fit a linear model\nmodel &lt;- lm(Petal.Width ~ Petal.Length, data=iris)  \n#model %&gt;% summary\n\n\nEasy with Python/sklearn: LinearRegression()\n\n\nCode\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\n\n# 1. Load iris data\niris_data = load_iris(as_frame=True)\nX = iris_data.data\n\n# 2. munge and reshape data\nx = iris['petal length (cm)']   # predictor\ny = iris['petal width (cm)']    # response \nx = x.to_numpy().reshape(-1, 1) # shape it as samples x features\n\n# 3. define model\nlm = LinearRegression()  \n\n# 4. fit / train model\nlm.fit(x, y)             \n\n# 5. report model & fit parameters\nprint(\"R² score:\", lm.score(x, y))\nprint('Fitted Parameters: ', lm.intercept_, lm.coef_)\n\nyp = lm.predict(x)                 # prediction\nMSE= mean_squared_error(y, yp)     # evaluate fit \nprint('Mean Squared Error: ', MSE)"
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#model-evaluations",
    "href": "lectures/01_MachineLearning/03_Models101.html#model-evaluations",
    "title": "Models 101",
    "section": "Model Evaluations",
    "text": "Model Evaluations\n\n\n\n\n\n\nWhat is a good model (“good fit”)\n\n\n\n\nRSS small \\(\\to\\) how small?\nslope \\(\\theta_1 \\ne 0\\) (confidence interval, small \\(P\\)-value)\ngood explanation of variance: large coefficient of determination \\(R^2\\)\ngood improvement of explained variance: large F-statistics (small \\(P\\)-value)\nvisualization\n\n\n\n\n\n\n\n\n\n\\(R^2\\) and \\(F\\)\n\n\n\n\n\nTotal Sum of Squares (Mean-only Model):\n\\[\nTSS = \\sum_i(y_i - \\bar y)^2 = ||Y - \\bar{Y}||^2\n\\]\nTwo components of variation: \\[\n\\begin{array}{ll}\n||Y - \\bar{Y}||^2 &= ||Y-\\hat{Y}||^2 + ||\\hat{Y} - \\bar{Y}||^2 \\\\\nTSS &= RSS + ESS\n\\end{array}\n\\]\nCoefficient of Determination: Fraction of variation explained by new model (\\(R^2\\)):\n\\[\nR^2 = \\frac{TSS-RSS}{TSS} = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_i(y_i - \\hat y_i)^2}{\\sum_i(y_i-\\bar{y})^2}\n\\]\nF-statistics \\[\nF = \\frac{(TSS-RSS)/(p-1)}{RSS/(n-p)}  \n\\]\n\n\n\n\n\n\n\n\n\nPoor Models \\(\\to\\) useless models"
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#model-predictions",
    "href": "lectures/01_MachineLearning/03_Models101.html#model-predictions",
    "title": "Models 101",
    "section": "Model Predictions",
    "text": "Model Predictions\n\nwith confidence intervals\nUncertainties in model parameters become uncertainties in prediction:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice\n\n\n\nWe model the expected mean and the confidence interval denotes the “standard deviation” of the mean, not the variability in the data\n\n\n\n\nwith prediction intervals\nThe variation of the data is clearly larger than suggested by the confidence interval shown above.\nIt is important to recognize that there are two components of variations:\n\\[\n||Y - \\bar{Y}||^2 = ||Y-\\hat{Y}||^2 + ||\\hat{Y} - \\bar{Y}||^2\n\\]\n\nthe variance explained by the linear model: \\(||\\hat{Y} - \\bar{Y}||^2\\) where \\(\\hat{Y} = X \\hat \\theta\\). In this context errors in \\(\\hat \\theta\\) will induce errors in Petal.Width.\nthe unexplained variance \\(||Y-\\hat{Y}||^2 = ||\\epsilon||^2\\) that remains unaccounted for, but we assume that \\(\\epsilon = (Y-\\hat{Y}) \\propto N(0,\\sigma^2)\\)\n\n\n\nCode\npred &lt;-predict(model, newdata=iris, interval = 'prediction' , level = 0.893)\n\ncbind(iris, pred) %&gt;%\n  ggplot(aes(x=Petal.Length)) + \n  geom_point(aes(y = Petal.Width, colour=Species)) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.2) +\n  geom_line(aes(y = fit)) +\n  ggtitle('Prediction Interval')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe prediction interval covers the broader variability of the data.\nBut as with the confidence interval, the level can be set at will.\nA common but arbitrary choice is 0.95\n\n\n\n\n\n\n\n\n\nPredictions with Linear Algebra\n\n\n\n\n\nThe confidence interval (CI) denotes the range within which we expect the expected means to lie\nThe prediction interval (PI) also includes the residual variance \\(\\sigma^2\\)\n\\[\n\\begin{array}{ll}\n\\hat y & = X\\theta = E[Y(X)] = \\mbox{expected mean}\\\\\nCov(\\hat \\theta) &= \\sigma^2 (X^TX)^{-1} \\\\\nCov(\\hat y) &= X \\cdot Cov(\\hat \\theta) \\cdot X^T = \\sigma^2 X(X^TX)^{-1} X^T \\\\\nSE(\\hat y) &= \\sqrt{diag(Cov(\\hat y))} \\\\\nCI:  & \\hat{y} \\pm ~t_{n-2} \\cdot SE(\\hat{y}) \\\\\nPI:  & \\hat{y}  \\pm ~t_{n-2} \\cdot \\sqrt{SE(\\hat{y}) + \\sigma^2}\n\\end{array}\n\\]\n\n\n\n\n\nPredicting Outside\nRegardless of prediction or confidence intervals, be careful when predicting (and interpreting) beyond the data range in which the model was trained/fitted\n\n\nCode\nmodel &lt;- lm(Petal.Width ~ Petal.Length, data=iris)                    # just in case we forgot\nnew_data &lt;- data.frame(Petal.Length = seq(0, 10, 0.1))                # create new data\npred &lt;- predict(model, newdata = new_data, interval = \"prediction\")   # + prediction interval (default)\npred &lt;- pred %&gt;% as.data.frame %&gt;% mutate(Petal.Length=new_data$Petal.Length) # add Petal.Length \n\nggplot(data=iris, aes(x = Petal.Length)) +\n  geom_point(aes(y = Petal.Width)) +\n  geom_smooth(aes(y = Petal.Width), method = \"lm\", formula = y ~ x) +   # CI from geom_smooth\n  geom_ribbon(data = pred, aes(ymin = lwr, ymax = upr), alpha = 0.2) +  # PI from pred\n  theme_minimal() +\n  labs(title = \"Hic sunt dracones\")"
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#factorial-variables-as-predictors",
    "href": "lectures/01_MachineLearning/03_Models101.html#factorial-variables-as-predictors",
    "title": "Models 101",
    "section": "Factorial variables as predictors",
    "text": "Factorial variables as predictors\nIn the iris example the “Species” variable is a factorial (categorical) variable with 3 levels.\nOther typical examples: different experimental conditions or treatments.\n\n\nCode\nplot(Petal.Width ~ Species, data=iris)\n\n\n\n\n\n\n\n\n\nCode\n# can also be modelled with model matrix X\n#model_cat=lm(Petal.Width ~ Species, data=iris)\n#summary(model_cat)\n\n\nEach species level (dummy variable X) has their own expected value \\(y\\).\nRequires choice of dummy encoding and reference level."
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#model-checking",
    "href": "lectures/01_MachineLearning/03_Models101.html#model-checking",
    "title": "Models 101",
    "section": "Model Checking",
    "text": "Model Checking\nLinear models \\(y_i=\\hat y_i + \\epsilon_i\\) make certain assumptions: \\(\\epsilon_i \\propto N(0,\\sigma^2)\\)\n\nresiduals \\(\\epsilon_i\\) are independent from each other (non-linear patterns?)\nresiduals are normally distributed\nhave equal variance \\(\\sigma^2\\) (“homoscedasticity”)\nno outliers (large residuals) or observations with strong influence on fit\n\nDifferent methods and software for diagnostic plots\n\n\nCode\nfit=lm(Petal.Width ~ ., data=iris)\nop=par(no.readonly=TRUE)   # safe only resettable graphical parameters, avoids many warnings\npar(mfrow=c(2,2))          # change graphical parameters: 2x2 images on device\nplot(fit,col=iris$Species) # four plots rather than one\n\n\n\n\n\n\n\n\n\nCode\npar(op)                    # reset graphical parameters"
  },
  {
    "objectID": "lectures/01_MachineLearning/03_Models101.html#review",
    "href": "lectures/01_MachineLearning/03_Models101.html#review",
    "title": "Models 101",
    "section": "Review",
    "text": "Review\n\ndependencies between variables can often be modeled\nfind best parameters and loss function (RSS)\nlinear models:\n\nfast analytical and computational solutions\nmany evaluation metrics and checking: \\(R^2, F, \\ldots\\)\n\nmay not be appropriate (complex relationships, \\(y\\) categorical)\n\ndangers:\n\ngood models not necessarily causal\nextrapolation beyond fit domains\n\nfuture\n\nbeyond linear models\nfocus on RSS, accuracy (and other loss functions)\nparameters less interesting"
  },
  {
    "objectID": "homework/04_Autoencoder.html",
    "href": "homework/04_Autoencoder.html",
    "title": "01 Autoencoder",
    "section": "",
    "text": "Data Download: Obtain both train and test data for the MNIST data set of handwritten digits from torchvision.datasets\nData Massage: For numerical efficiency use pytorch dataloaders and split the dataset into train, test and validation. Check data values and if normalization is required.\nModel Definition: Define a simple autoencoder (model1) with 4 layers to reduce dimensions: 728 -&gt; 128 -&gt; 64 -&gt; 16 -&gt; 3 count dimensions and parameters\nGoal and Strategy: Define a loss function and a suitable optimizer\nModel Training: Train the model and track the following parameters for each epochs\n\n\nloss and validation loss\ntraining images, latent representations, reconstructred images\n\n\nModel Evaluation:\n\n\nplot traing and validation loss.\nFor the best model calculate loss on. test data\ncompare several real images and compare to their reconstruction\n\n\nLatent Assessment: Compare the latent representation of images with the PCA embedding\n\n\nhow much variability is encoded by the first 3 components of PCA(X) compared to PCA(Z)\n\n\n\nData Generation: For a given image, generate new images by changing selected latent variable in PCA space or latent space.\noptional: replace the simple autoencoder model1 by a neural network with 3 convolutional layers (model2)\n\n\ncount parameters and compare to model1\ncompare learning curves with model1\n\n\nexplore what happens if you reduce the size of the data set"
  },
  {
    "objectID": "homework/04_Autoencoder.html#tasks",
    "href": "homework/04_Autoencoder.html#tasks",
    "title": "01 Autoencoder",
    "section": "",
    "text": "Data Download: Obtain both train and test data for the MNIST data set of handwritten digits from torchvision.datasets\nData Massage: For numerical efficiency use pytorch dataloaders and split the dataset into train, test and validation. Check data values and if normalization is required.\nModel Definition: Define a simple autoencoder (model1) with 4 layers to reduce dimensions: 728 -&gt; 128 -&gt; 64 -&gt; 16 -&gt; 3 count dimensions and parameters\nGoal and Strategy: Define a loss function and a suitable optimizer\nModel Training: Train the model and track the following parameters for each epochs\n\n\nloss and validation loss\ntraining images, latent representations, reconstructred images\n\n\nModel Evaluation:\n\n\nplot traing and validation loss.\nFor the best model calculate loss on. test data\ncompare several real images and compare to their reconstruction\n\n\nLatent Assessment: Compare the latent representation of images with the PCA embedding\n\n\nhow much variability is encoded by the first 3 components of PCA(X) compared to PCA(Z)\n\n\n\nData Generation: For a given image, generate new images by changing selected latent variable in PCA space or latent space.\noptional: replace the simple autoencoder model1 by a neural network with 3 convolutional layers (model2)\n\n\ncount parameters and compare to model1\ncompare learning curves with model1\n\n\nexplore what happens if you reduce the size of the data set"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#define-class-label",
    "href": "homework/traditional_ml_to_generative.html#define-class-label",
    "title": "Machine Learning cycle",
    "section": "2. Define class label",
    "text": "2. Define class label"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#visualize-px",
    "href": "homework/traditional_ml_to_generative.html#visualize-px",
    "title": "Machine Learning cycle",
    "section": "3. Visualize p(x)",
    "text": "3. Visualize p(x)\n\n\nCode\nplt.hist(x, bins=40, density=True, alpha=0.6, label=\"p(x) empirical\")\nplt.title(\"Complicated marginal p(x)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"density\")\nplt.legend()\nplt.show()\n\nplt.scatter(y, x, c=y, cmap=\"coolwarm\", alpha=0.6)\nplt.xlabel(\"x\")\nplt.ylabel(\"latent z (true)\")\nplt.title(\"GMM clustering of x\")\nplt.show()"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#unsupervised-clustering-gmm",
    "href": "homework/traditional_ml_to_generative.html#unsupervised-clustering-gmm",
    "title": "Machine Learning cycle",
    "section": "4. Unsupervised clustering (GMM)",
    "text": "4. Unsupervised clustering (GMM)\n\n\nCode\nfrom sklearn.mixture import GaussianMixture\n\ngmm = GaussianMixture(n_components=2).fit(X)\nclusters = gmm.predict(X)\n\nplt.scatter(x, z, c=clusters, cmap=\"coolwarm\", alpha=0.6)\nplt.xlabel(\"x\")\nplt.ylabel(\"latent z (true)\")\nplt.title(\"GMM clustering of x\")\nplt.show()"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#supervised-logistic-regression",
    "href": "homework/traditional_ml_to_generative.html#supervised-logistic-regression",
    "title": "Machine Learning cycle",
    "section": "5. Supervised logistic regression",
    "text": "5. Supervised logistic regression\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\nclf = LogisticRegression().fit(X_train, y_train) # claim: y ~ f(X)\ny_pred = clf.predict(X_test)\ny_prob = clf.predict_proba(X_test)[:, 1]"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#evaluation-classification-report-and-roc-curve",
    "href": "homework/traditional_ml_to_generative.html#evaluation-classification-report-and-roc-curve",
    "title": "Machine Learning cycle",
    "section": "6. Evaluation (classification report and ROC curve)",
    "text": "6. Evaluation (classification report and ROC curve)\n\n\nCode\nfrom sklearn.metrics import classification_report, roc_curve, auc\n\nprint(classification_report(y_test, y_pred))\n\nfpr, tpr, _ = roc_curve(y_test, y_prob)\nplt.plot(fpr, tpr, label=f\"AUC = {auc(fpr, tpr):.2f}\")\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC curve\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#feature-interpretation-logistic-coefficients",
    "href": "homework/traditional_ml_to_generative.html#feature-interpretation-logistic-coefficients",
    "title": "Machine Learning cycle",
    "section": "7. Feature interpretation (logistic coefficients)",
    "text": "7. Feature interpretation (logistic coefficients)\n\n\nCode\nprint(f\"Logistic coef: {clf.coef_}, intercept: {clf.intercept_}\")"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#approximate-inverse-estimate-z-from-x",
    "href": "homework/traditional_ml_to_generative.html#approximate-inverse-estimate-z-from-x",
    "title": "Machine Learning cycle",
    "section": "8. Approximate inverse: estimate z from x",
    "text": "8. Approximate inverse: estimate z from x\n\n\nCode\nfrom scipy.stats import norm, rankdata\n\n# Empirical CDF of x\nranks = rankdata(x)\nempirical_cdf = ranks / (len(x) + 1)\nz_hat = norm.ppf(empirical_cdf)\n\n# Compare distribution\nplt.hist(z_hat, bins=40, density=True, alpha=0.6, label=\"z_hat ~ N(0,1)\")\nx_vals = np.linspace(-3, 3, 200)\nplt.plot(x_vals, norm.pdf(x_vals), 'k--', label=\"True N(0,1)\")\nplt.legend()\nplt.title(\"Transformed variable z_hat from x\")\nplt.show()\n\n\n\n\nCode\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(PolynomialFeatures(degree=10), LinearRegression())\nmodel.fit(z_hat.reshape(-1, 1), x)\n\nz_test = np.linspace(-3, 3, 300)\nx_pred = model.predict(z_test.reshape(-1, 1))\n\nplt.plot(z_test, x_pred, label=\"Learned g(z)\")\nplt.scatter(z, x, alpha=0.2, s=10, label=\"True (z, x)\")\nplt.xlabel(\"z\")\nplt.ylabel(\"x\")\nplt.title(\"Learned inverse mapping z → x\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#generate-new-data-using-z-n0-1",
    "href": "homework/traditional_ml_to_generative.html#generate-new-data-using-z-n0-1",
    "title": "Machine Learning cycle",
    "section": "9. Generate new data using z ~ N(0, 1)",
    "text": "9. Generate new data using z ~ N(0, 1)\n\n\nCode\nz_new = np.random.normal(0, 1, 500)\nepsilon_new = np.random.normal(0, 0.1, 500)\nx_gen = np.sin(3 * z_new) + 0.3 * z_new + epsilon_new\n\nplt.hist(x_gen, bins=40, density=True, alpha=0.6, label=\"Generated x from z\")\nplt.hist(x, bins=40, density=True, alpha=0.4, label=\"Original x\")\nplt.legend()\nplt.title(\"Compare p(x) and generated samples\")\nplt.show()"
  },
  {
    "objectID": "homework/traditional_ml_to_generative.html#summary",
    "href": "homework/traditional_ml_to_generative.html#summary",
    "title": "Machine Learning cycle",
    "section": "10. Summary",
    "text": "10. Summary\nThis example shows how a nonlinear, complex distribution p(x) can be constructed from a simple latent variable z ~ N(0,1). Without any neural networks, we explored: - Clustering with GMM - Binary classification with logistic regression - Evaluation and interpretability - Latent space approximation and data generation\nThis prepares the stage for introducing Autoencoders, VAEs, and GANs.\n\n\n\n\n\n\n\n\nConcept\nTraditional Method\nMapping to Deep Learning\n\n\n\n\nLatent variable z\nN(0,1)\nLatent code\n\n\nComplex p(x)\nsin(3z) + noise\nGenerator network\n\n\nInverse mapping x → z\nRidge regression\nEncoder network\n\n\nDimensionality reduction\nPCA, GMM\nAutoencoder\n\n\np(z) → x\ndeterministic function\nDecoder or GAN generator\n\n\nLearning p(x)\nKDE or histograms\nLikelihood via VAE, GANs\n\n\nSupervised y\nLogistic regression\nClassifier layer"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neural Networks and Deep Learning",
    "section": "",
    "text": "Upon successful completion of this course, you will\n\nunderstand the key concepts and challenges of neural networks\nbe proficient in common software solutions for deep learning (pytorch)\nbe able to define, train and evaluate your own models\nhave completed 4 concrete data analysis projects and provide reproducible code\nhave worked in teams and presented your work\nwant to learn more and deeper … open"
  },
  {
    "objectID": "index.html#objectives",
    "href": "index.html#objectives",
    "title": "Neural Networks and Deep Learning",
    "section": "",
    "text": "Upon successful completion of this course, you will\n\nunderstand the key concepts and challenges of neural networks\nbe proficient in common software solutions for deep learning (pytorch)\nbe able to define, train and evaluate your own models\nhave completed 4 concrete data analysis projects and provide reproducible code\nhave worked in teams and presented your work\nwant to learn more and deeper … open"
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "Neural Networks and Deep Learning",
    "section": "Content",
    "text": "Content\n\nMachine Learning 101\nNeural Networks\nConvolutional Neural Networks\nGenerative Networks"
  },
  {
    "objectID": "index.html#format",
    "href": "index.html#format",
    "title": "Neural Networks and Deep Learning",
    "section": "Format",
    "text": "Format\nThe course comprises the following blocks (with the estimated time effort)\n\nLectures: (16x 2h) will provide an introduction and overview\nProjects: 4 larger assignments that are solved in teams of 2 students. Each team will submit a fully documented and executable notebook per project.\nTutorials: (16x 2h): weekly tutorials will focus on practical implementations and the review of lecture material. Up to 2 teams will present their project solutions (~30 min / team)."
  },
  {
    "objectID": "index.html#evaluation",
    "href": "index.html#evaluation",
    "title": "Neural Networks and Deep Learning",
    "section": "Evaluation",
    "text": "Evaluation\n\nProject Submissions (28 P). Completed projects are submitted as single jupyter noteboooks and are evaluated per team. Notebooks have to be fully documented to explain both the analysis goals and the code. They should clearly state software dependencies and be fully executable.\nProject Presentation (12 P). Presentation and submission of a reproducible data analysis project in an executable markdown format.\nExam (60 P): this will be a written individual exam with conceptual questions, including multiple choice and short essays."
  },
  {
    "objectID": "index.html#materials-and-references",
    "href": "index.html#materials-and-references",
    "title": "Neural Networks and Deep Learning",
    "section": "Materials and References",
    "text": "Materials and References\n\nClassic Books (Theory & Maths)\n\nGoodfellow et al (2016)\nBishop & Bishop (2024)\n\nClassic Courses (YouTube)\n\nAndrew Ng @ Coursera\nStandford CS230, 2018\n\nPytorch Tutorials (Practical)\nHuggingFace Learning (Next Steps)"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n01 Machine Learning\n\n\nAn ML starter\n\n\n\nThomas Manke\n\n\nOct 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n01 Machine Learning\n\n\nA template for Iris Data\n\n\n\nThomas Manke\n\n\nOct 28, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/01_MachineLearning.html",
    "href": "projects/01_MachineLearning.html",
    "title": "01 Machine Learning",
    "section": "",
    "text": "This project serves a introduction to the key challenges in machine learning and data analysis. Upon successful completion you should be able to conduct such analysis tasks independently and apply them to other problems.\n\nSetup Jupyter Notebook which includes\n\ndetail environment in which this nodebook was developed and tested\ninstallation of versioned packages (if necessary)\nload required software packages and report their version\nother custom functions (if necessary)\nproject-wide variables\n\nLoad Data (see examples below) and describe in detail their origin, content, structures and data types.\nData Exploration. Prepare statistical and visual data summaries. How many variables/features does the data contain, how many samples? Are there missing values? Explore correlations among variables and conduct at least one unsupervised analysis. Consider variable transformations and normalizations where applicable.\nData Model. Define what you want to model (following the table below) and choose an appropriate model framework. Define a simple baseline model and a loss function - theoretically and practically. Explain the model in terms of input-output data structures and dimensions.\nTrain Model. Define a Train Define Train and Test Data and run the training loop. Monitor the behaviour of the loss function.\nEvaluate Model. Run model predictions on the test data set. Calculate and visualize the appropriate metrics to evaluate model performance. Explain why a separate test data set should be used for such evaluations.\nImprove Model. Suggest extensions to your baseline model and repeat steps 4. - 6. Do these extensions improve the model?\n\n\n\n\n\n\n\nSummary\n\n\n\nProvide a concise summary and critical reflection of this project and all analysis steps. What are the assumptions and limits? Document and annotate all steps above carefully in a single jupyter notebook and ensure that it will execute reproducibly and without error. Upon completion, submit the project as a single notebook per team. This notebook will be the basis for your presentation and it will be shared with the whole class.\nInclude a statement of originality and contributions at the beginning"
  },
  {
    "objectID": "projects/01_MachineLearning.html#tasks",
    "href": "projects/01_MachineLearning.html#tasks",
    "title": "01 Machine Learning",
    "section": "",
    "text": "This project serves a introduction to the key challenges in machine learning and data analysis. Upon successful completion you should be able to conduct such analysis tasks independently and apply them to other problems.\n\nSetup Jupyter Notebook which includes\n\ndetail environment in which this nodebook was developed and tested\ninstallation of versioned packages (if necessary)\nload required software packages and report their version\nother custom functions (if necessary)\nproject-wide variables\n\nLoad Data (see examples below) and describe in detail their origin, content, structures and data types.\nData Exploration. Prepare statistical and visual data summaries. How many variables/features does the data contain, how many samples? Are there missing values? Explore correlations among variables and conduct at least one unsupervised analysis. Consider variable transformations and normalizations where applicable.\nData Model. Define what you want to model (following the table below) and choose an appropriate model framework. Define a simple baseline model and a loss function - theoretically and practically. Explain the model in terms of input-output data structures and dimensions.\nTrain Model. Define a Train Define Train and Test Data and run the training loop. Monitor the behaviour of the loss function.\nEvaluate Model. Run model predictions on the test data set. Calculate and visualize the appropriate metrics to evaluate model performance. Explain why a separate test data set should be used for such evaluations.\nImprove Model. Suggest extensions to your baseline model and repeat steps 4. - 6. Do these extensions improve the model?\n\n\n\n\n\n\n\nSummary\n\n\n\nProvide a concise summary and critical reflection of this project and all analysis steps. What are the assumptions and limits? Document and annotate all steps above carefully in a single jupyter notebook and ensure that it will execute reproducibly and without error. Upon completion, submit the project as a single notebook per team. This notebook will be the basis for your presentation and it will be shared with the whole class.\nInclude a statement of originality and contributions at the beginning"
  },
  {
    "objectID": "projects/01_MachineLearning.html#originality-and-contributions",
    "href": "projects/01_MachineLearning.html#originality-and-contributions",
    "title": "01 Machine Learning",
    "section": "Originality and Contributions",
    "text": "Originality and Contributions\nAuthors: X.Y., A.I.\nThis notebook is our own work. Any other sources have been clearly marked and cited.\nAll authors contributed equally. (alternative: state specific contributions explicitly)"
  },
  {
    "objectID": "projects/01_MachineLearning.html#references",
    "href": "projects/01_MachineLearning.html#references",
    "title": "01 Machine Learning",
    "section": "References",
    "text": "References\nProvide references and citations where necessary"
  },
  {
    "objectID": "projects/01_MachineLearning.html#data-sets",
    "href": "projects/01_MachineLearning.html#data-sets",
    "title": "01 Machine Learning",
    "section": "Data Sets",
    "text": "Data Sets\nThe data sets below are heavily used in machine learning. They also have the advantage of being easily accessible and highly standardized.\nEach team should work with the data set (Project ID) assigned to them.\nWhen modelling the given target variable \\(y\\), it should be modelled as a multivariate function of other variables/features \\(X\\) in the data set. You can also consider relevant subsets of features, if you can argue to exclude some features, or for the purpose of a first simple baseline model.\n\n\n\nProject ID\nLink\ntarget variable \\(y\\)\n\n\n\n\n1.1\nwine data\nwine quality\n\n\n1.2\ngapminder\nlifeExpectancy\n\n\n1.3\nbreast cancer\ncancer status\n\n\n1.4\nMNIST\ndigit label"
  },
  {
    "objectID": "projects/01_MachineLearning.html#data-loading-hints",
    "href": "projects/01_MachineLearning.html#data-loading-hints",
    "title": "01 Machine Learning",
    "section": "Data Loading Hints",
    "text": "Data Loading Hints\nThere are many ways to obtain these data sets. Below are just a few suggestions.\nPlease explore the data carefully - transform and normalize where necessary.\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n    sep=\";\"\n)\nprint(df.describe())\ny = df['quality']\n\n\n\n\nCode\nimport plotly.express as px\ndf = px.data.gapminder() \ny = df['lifeExp']\n\n\n\n\nCode\nfrom sklearn.datasets import load_breast_cancer\nimport pandas as pd\n\nbc = load_breast_cancer(as_frame=True)\nX = bc.data\ny = pd.Series(bc.target).map({0: \"malignant\", 1: \"benign\"}).rename(\"status\")\ndf = pd.concat([X, y], axis=1)\n\n\n\n\nCode\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# this dataset already comes split in train and test\ntransform = transforms.ToTensor()\ntrain = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\ntest  = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\ntrain_loader = DataLoader(train, batch_size=64, shuffle=True)"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html",
    "href": "projects/01_MachineLearningIris.html",
    "title": "01 Machine Learning",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import datasets"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#import-packages",
    "href": "projects/01_MachineLearningIris.html#import-packages",
    "title": "01 Machine Learning",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import datasets"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#obtain-iris-data-set",
    "href": "projects/01_MachineLearningIris.html#obtain-iris-data-set",
    "title": "01 Machine Learning",
    "section": "Obtain Iris data set",
    "text": "Obtain Iris data set\n\n\nCode\niris = datasets.load_iris()\nX = iris.data    # measurements\ny = iris.target  # species label\n\nprint(f\"type(X) = {type(X)}\")\nprint(f\"type(y)  = {type(y)}\")\nprint(f\"shape(X)   = {X.shape}\")\nprint(f\"shape(y) = {y.shape}\")\n\n# define number of observations and number of variables\nn_obs, n_var = iris.data.shape\n\n\ntype(X) = &lt;class 'numpy.ndarray'&gt;\ntype(y)  = &lt;class 'numpy.ndarray'&gt;\nshape(X)   = (150, 4)\nshape(y) = (150,)"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#convert-and-reshape",
    "href": "projects/01_MachineLearningIris.html#convert-and-reshape",
    "title": "01 Machine Learning",
    "section": "Convert and Reshape",
    "text": "Convert and Reshape\n… use pandas dataframe for convenience\n\n\nCode\ndf = pd.DataFrame(X, columns=iris.feature_names)\nprint(\"Numerical Summaries\")\nprint(df.describe())\n\nprint(\"Wide Format - top\")\nprint(df.head())\n\n# reshape to long format\ndf_long = df.melt(var_name=\"Feature\", value_name=\"Value\")\nprint('Long Format - top')\nprint(df_long.head())\n\n\nNumerical Summaries\n       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\ncount         150.000000        150.000000         150.000000   \nmean            5.843333          3.057333           3.758000   \nstd             0.828066          0.435866           1.765298   \nmin             4.300000          2.000000           1.000000   \n25%             5.100000          2.800000           1.600000   \n50%             5.800000          3.000000           4.350000   \n75%             6.400000          3.300000           5.100000   \nmax             7.900000          4.400000           6.900000   \n\n       petal width (cm)  \ncount        150.000000  \nmean           1.199333  \nstd            0.762238  \nmin            0.100000  \n25%            0.300000  \n50%            1.300000  \n75%            1.800000  \nmax            2.500000  \nWide Format - top\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                5.1               3.5                1.4               0.2\n1                4.9               3.0                1.4               0.2\n2                4.7               3.2                1.3               0.2\n3                4.6               3.1                1.5               0.2\n4                5.0               3.6                1.4               0.2\nLong Format - top\n             Feature  Value\n0  sepal length (cm)    5.1\n1  sepal length (cm)    4.9\n2  sepal length (cm)    4.7\n3  sepal length (cm)    4.6\n4  sepal length (cm)    5.0"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#histogram",
    "href": "projects/01_MachineLearningIris.html#histogram",
    "title": "01 Machine Learning",
    "section": "Histogram",
    "text": "Histogram\n\n\nCode\nplt.figure()\nsns.histplot(df_long[\"Value\"], bins=30)\nplt.title(\"Histogram\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#violin-plots",
    "href": "projects/01_MachineLearningIris.html#violin-plots",
    "title": "01 Machine Learning",
    "section": "Violin Plots",
    "text": "Violin Plots\n\n\nCode\nprint(f\"variable means: {np.mean(X, axis=0)})\")\nplt.figure()\nsns.violinplot(x=\"Feature\", y=\"Value\", data=df_long)\nplt.title(\"Violin plots for all variables\")\nplt.tight_layout()\nplt.show()\n\n\nvariable means: [5.84333333 3.05733333 3.758      1.19933333])"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#missing-values",
    "href": "projects/01_MachineLearningIris.html#missing-values",
    "title": "01 Machine Learning",
    "section": "Missing Values",
    "text": "Missing Values\n\n\nCode\nna_counts = df.isna().sum() # pandas\n#na_counts = np.isnan(X).sum(axis=0) # numpy\nprint(\"Missing values:\\n\", na_counts)\n\nsns.scatterplot(x=na_counts.index, y=na_counts.values)\nplt.title(\"Number of missing values per feature\")\nplt.xlabel(\"Variable\")\nplt.ylabel(\"# missing\")\nplt.tight_layout()\nplt.show()\n\n\nMissing values:\n sepal length (cm)    0\nsepal width (cm)     0\npetal length (cm)    0\npetal width (cm)     0\ndtype: int64"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#heatmap",
    "href": "projects/01_MachineLearningIris.html#heatmap",
    "title": "01 Machine Learning",
    "section": "Heatmap",
    "text": "Heatmap\n\n\nCode\n# don't use heatmaps before you know what you get ;-)\nplt.figure()\nsns.heatmap(df, cmap=\"Blues\", cbar_kws={'label': 'Value'})\nplt.xlabel(\"Variables (features)\")\nplt.ylabel(\"Samples (observations)\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#correlations",
    "href": "projects/01_MachineLearningIris.html#correlations",
    "title": "01 Machine Learning",
    "section": "Correlations",
    "text": "Correlations\n\n\nCode\nsns.pairplot(df, height=1.5, aspect=1.0)\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorr = df.corr()\n\nplt.figure()\nsns.heatmap(\n    corr,\n    vmin=-1, vmax=1,\n    cmap=\"coolwarm\",    \n    annot=True,        # show correlation values\n    fmt=\".2f\",         # format to 2 decimals\n    square=True,       # make cells square\n    cbar_kws={\"label\": \"Pearson r\"}\n)\nplt.title(\"Correlation Matrix\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#scale-transformation",
    "href": "projects/01_MachineLearningIris.html#scale-transformation",
    "title": "01 Machine Learning",
    "section": "Scale Transformation",
    "text": "Scale Transformation\n… is almost always useful (esp. when variables live on different scales)\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nXs = scaler.fit_transform(X)\n\n# numpy2pandas (mostly for plotting convenience)\ncol_names = [\"SL\", \"SW\", \"PL\", \"PW\"]\ndf_scaled = pd.DataFrame(Xs, columns=col_names)\ndf_scaled_long = df_scaled.melt(var_name=\"Feature\", value_name=\"Value\")\n\nprint(f\"variable means: \\n{df_scaled.mean()})\")\nplt.figure()\nsns.violinplot(x=\"Feature\", y=\"Value\", data=df_scaled_long)\nplt.title(\"Violin plots for scaled variables\")\nplt.tight_layout()\nplt.show()\n\n\nvariable means: \nSL   -1.690315e-15\nSW   -1.842970e-15\nPL   -1.698641e-15\nPW   -1.409243e-15\ndtype: float64)"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#unsupervised-clustering-kmeans",
    "href": "projects/01_MachineLearningIris.html#unsupervised-clustering-kmeans",
    "title": "01 Machine Learning",
    "section": "Unsupervised: Clustering (kmeans)",
    "text": "Unsupervised: Clustering (kmeans)\n\n\nCode\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3)\nlabels = kmeans.fit_predict(Xs)\n\ndf_scaled[\"cluster\"] = labels.astype(str)\nsns.pairplot(df_scaled, hue=\"cluster\", height=1.5, aspect=1.0)"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#unsupervised-principle-component-analysis",
    "href": "projects/01_MachineLearningIris.html#unsupervised-principle-component-analysis",
    "title": "01 Machine Learning",
    "section": "Unsupervised: Principle Component Analysis",
    "text": "Unsupervised: Principle Component Analysis\n…. provides very convenient low-dimensional representation of the data\n\n\nCode\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(Xs)\nvar_exp = pca.explained_variance_ratio_\n\nxlab = f\"PC1 ({var_exp[0]:.3f})\"\nylab = f\"PC2 ({var_exp[1]:.3f})\"\nsns.scatterplot(x=X_pca[:,0], y=X_pca[:,1])\nplt.title(\"PCA plot\")\nplt.xlabel(xlab)\nplt.ylabel(ylab)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#supervised-linear-model-x-to-y",
    "href": "projects/01_MachineLearningIris.html#supervised-linear-model-x-to-y",
    "title": "01 Machine Learning",
    "section": "Supervised: Linear Model \\(x \\to y\\)",
    "text": "Supervised: Linear Model \\(x \\to y\\)\nUse PyTorch to define a linear model (with one input and one output), a suitable loss function, and an optimizer.\n\n\nCode\nimport numpy as np\nimport torch\nimport torch.nn as nn\nprint('torch version:', torch.__version__)\n\n# identical to the 1st lecture (same model)\nmodel = nn.Linear(1, 1) # n_in = 1, n_out = 1\nloss_func = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters())\n\n\ntorch version: 2.7.1"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#define-data",
    "href": "projects/01_MachineLearningIris.html#define-data",
    "title": "01 Machine Learning",
    "section": "Define Data",
    "text": "Define Data\nAgain we need some reformating (numpy \\(\\to\\) pytorch tensor) to make the data fit with the pytorch framework. We also need to define the input data (x_t) and output data (y_t). Their shape and dimensionality should correspond to the model defined above.\n\n\nCode\n# select two arbitrary variables as x=X[:,2] and y=X[:,3] \n# just for illustration - feel free to change\nx_t = torch.as_tensor(X[:,2], dtype=torch.float32).view(-1, 1)\ny_t = torch.as_tensor(X[:,3], dtype=torch.float32).view(-1, 1)\nprint(f\"shape(x_t): {x_t.shape}\")\nprint(f\"shape(y_t): {y_t.shape}\")\n\n\nshape(x_t): torch.Size([150, 1])\nshape(y_t): torch.Size([150, 1])"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#train-the-model",
    "href": "projects/01_MachineLearningIris.html#train-the-model",
    "title": "01 Machine Learning",
    "section": "Train the Model",
    "text": "Train the Model\nChose a number of iterations (e.g 100 “epochs”) and keep track of the loss at each iteration.\n\n\nCode\nlosses = []\nmodel.train()\nfor epoch in range(200):\n    y_pred = model(x_t)\n    loss = loss_func(y_pred, y_t)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    losses.append(loss.item()) # keep track of losses\n\n# finished training. inspect model \nprint('Mean Squared Error (loss):    ',loss.item())\nprint('Parameters:')\nfor name, p in model.named_parameters():\n    print(name, p.detach())\n\n\nMean Squared Error (loss):     0.19705839455127716\nParameters:\nweight tensor([[0.2114]])\nbias tensor([0.5649])\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n“weight” = slope parameter\n“bias” = intercept"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#inspect-losses",
    "href": "projects/01_MachineLearningIris.html#inspect-losses",
    "title": "01 Machine Learning",
    "section": "Inspect losses",
    "text": "Inspect losses\nPlot the losses against the number of iterations an check if they are reducing\n\n\nCode\nplt.figure()\nplt.plot(losses)\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#predicitons",
    "href": "projects/01_MachineLearningIris.html#predicitons",
    "title": "01 Machine Learning",
    "section": "Predicitons",
    "text": "Predicitons\nUse the trained model to make predction for 50 new \\(x\\)-values in a range \\((-10,10)\\). Overlay the model predictions with the original data on which the model was learned.\n\n\nCode\n# pick any range of new x-values (meaningful?)\nx_new = np.arange(-10,10,0.5)\n# convert to tensor\nx_new_t = torch.as_tensor(x_new, dtype=torch.float32).view(-1, 1)\n# run predictions\nyp  = model(x_new_t)\n\n# convert from tensor to numpy and reshape\ny_pred = yp.detach().numpy().flatten()\n\nplt.scatter(x_t, y_t, label=\"Data\")\nplt.plot(x_new,y_pred, c=\"orange\", label=\"Model\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Critique\n\n\n\n\nthere is no perfect fit because\n\nlearning has not yet converged\nreal data is noisy MSE \\(\\ne 0\\)\n\nModels are ignorant: What is the Petal.Width for Petal.Length = -1cm ?\nAssessments of predicted performance should invoke some left out data (test set)\nModels are not necessarily causal\nall models are wrong, some are useful"
  },
  {
    "objectID": "projects/01_MachineLearningIris.html#multivariate-multinomial-regression",
    "href": "projects/01_MachineLearningIris.html#multivariate-multinomial-regression",
    "title": "01 Machine Learning",
    "section": "Multivariate Multinomial Regression",
    "text": "Multivariate Multinomial Regression\n… with test-train split\n\n\nCode\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n\nX = torch.tensor(iris.data, dtype=torch.float32) # numeric measurements\nY = torch.tensor(iris.target, dtype=torch.long)  # class labels\nprint(f\"X.shape = {X.shape}, Y.shape = {Y.shape}\")\n\n# define train and test data sets\nn_samples = X.shape[0] # number of all samples\nn_train = int(0.8 * n_samples) # number of training samples\nperm = torch.randperm(n_samples) # shuffled random indicies\ni_train = perm[:n_train] # train indices\ni_test = perm[n_train:] # test indices\n\nmodel = nn.Linear(4, 3) # n_in = 4, n_out = 3\nloss_func = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters())\n\nmodel.train()\nfor epoch in range(5000):\n    yp = model(X[i_train])\n    loss = loss_func(yp, Y[i_train])\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nmodel.eval()\nz = model(X[i_test])\ny_pred_class = torch.argmax(z, dim=1)\ny_true_class = Y[i_test].numpy()\nacc = accuracy_score(y_true_class, y_pred_class)\nprint('Accuracy: ', acc)   \n\ncm = confusion_matrix(y_true_class, y_pred_class)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=iris.target_names)\ndisp.plot(cmap='Blues')\n\n\nX.shape = torch.Size([150, 4]), Y.shape = torch.Size([150])\nAccuracy:  0.9333333333333333"
  },
  {
    "objectID": "homework/02_MNIST.html",
    "href": "homework/02_MNIST.html",
    "title": "01 Homework",
    "section": "",
    "text": "Reference:\n\nhttps://www.kaggle.com/code/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n\nRun a classification on the MNIST data with\n\nA simple logistic (10 classes)\nA three layer NN (128, 128, 128, 10) with ReLU\nA CNN\n\ncompare the accuracies on a test data"
  },
  {
    "objectID": "homework/02_MNIST.html#task",
    "href": "homework/02_MNIST.html#task",
    "title": "01 Homework",
    "section": "",
    "text": "Reference:\n\nhttps://www.kaggle.com/code/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n\nRun a classification on the MNIST data with\n\nA simple logistic (10 classes)\nA three layer NN (128, 128, 128, 10) with ReLU\nA CNN\n\ncompare the accuracies on a test data"
  },
  {
    "objectID": "homework/02_MNIST.html#packages",
    "href": "homework/02_MNIST.html#packages",
    "title": "01 Homework",
    "section": "Packages",
    "text": "Packages"
  },
  {
    "objectID": "homework/02_MNIST.html#data",
    "href": "homework/02_MNIST.html#data",
    "title": "01 Homework",
    "section": "Data",
    "text": "Data\nVerify mean and std"
  },
  {
    "objectID": "homework/02_MNIST.html#model-definition",
    "href": "homework/02_MNIST.html#model-definition",
    "title": "01 Homework",
    "section": "Model Definition",
    "text": "Model Definition"
  },
  {
    "objectID": "homework/02_MNIST.html#parameters",
    "href": "homework/02_MNIST.html#parameters",
    "title": "01 Homework",
    "section": "Parameters",
    "text": "Parameters"
  },
  {
    "objectID": "homework/02_MNIST.html#define-traing-loop",
    "href": "homework/02_MNIST.html#define-traing-loop",
    "title": "01 Homework",
    "section": "Define Traing Loop",
    "text": "Define Traing Loop"
  },
  {
    "objectID": "homework/02_MNIST.html#run-training",
    "href": "homework/02_MNIST.html#run-training",
    "title": "01 Homework",
    "section": "Run Training",
    "text": "Run Training"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html",
    "href": "lectures/01_MachineLearning/01_Data.html",
    "title": "Data Stories",
    "section": "",
    "text": "Domain\nOpen Formats\nProprietary Formats\nTypical Sizes\n\n\n\n\nUnstructured text\nTXT, Markdown, LaTeX\nDOCX (Microsoft), Pages (Apple)\nkB - MB\n\n\ntabular data\nCSV, TSV\nXLSX (Microsoft), Numbers (Apple)\nkB - MB\n\n\nkey-value data (nested)\nJSON YAML, XML\n???\nkB - MB\n\n\nImages\nPNG, JPEG, TIFF, SVG\nPSD (Photoshop), HEIF/HEIC (Apple)\nkB - GB\n\n\nHuman Genome\nFASTA\n-\n1 GB\n\n\nUbuntu 25.04\n*.c\n-\n5.8 GB\n\n\nWindows 11\n\n.EXE, .DLL (Microsoft)\n7 GB\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nsizes = [7, 1]   # sizes in GB\nlabels = [\"Windows 11\", \"Human Genome\"]\nexplode = [0, 0.075]\n\nfig, ax = plt.subplots()\nwedges, texts = ax.pie(sizes, \n  labels=labels,\n  explode=explode,\n  wedgeprops = dict(width=0.75),\n  startangle=45\n)  \n\nfor t in texts:\n    t.set_fontsize(16)\n\nplt.show()\n\n\n\n\n\nYour Genome Is Not Big !"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-sizes",
    "href": "lectures/01_MachineLearning/01_Data.html#data-sizes",
    "title": "Data Stories",
    "section": "",
    "text": "Domain\nOpen Formats\nProprietary Formats\nTypical Sizes\n\n\n\n\nUnstructured text\nTXT, Markdown, LaTeX\nDOCX (Microsoft), Pages (Apple)\nkB - MB\n\n\ntabular data\nCSV, TSV\nXLSX (Microsoft), Numbers (Apple)\nkB - MB\n\n\nkey-value data (nested)\nJSON YAML, XML\n???\nkB - MB\n\n\nImages\nPNG, JPEG, TIFF, SVG\nPSD (Photoshop), HEIF/HEIC (Apple)\nkB - GB\n\n\nHuman Genome\nFASTA\n-\n1 GB\n\n\nUbuntu 25.04\n*.c\n-\n5.8 GB\n\n\nWindows 11\n\n.EXE, .DLL (Microsoft)\n7 GB\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nsizes = [7, 1]   # sizes in GB\nlabels = [\"Windows 11\", \"Human Genome\"]\nexplode = [0, 0.075]\n\nfig, ax = plt.subplots()\nwedges, texts = ax.pie(sizes, \n  labels=labels,\n  explode=explode,\n  wedgeprops = dict(width=0.75),\n  startangle=45\n)  \n\nfor t in texts:\n    t.set_fontsize(16)\n\nplt.show()\n\n\n\n\n\nYour Genome Is Not Big !"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-rates",
    "href": "lectures/01_MachineLearning/01_Data.html#data-rates",
    "title": "Data Stories",
    "section": "Data Rates",
    "text": "Data Rates\n\n\n\n\n\n\n\n\n\nDomain\nOpen Formats\nProprietary Formats\nTypical Rates\n\n\n\n\nAudio\nFLAC, WAV\n(MP3), M4A (Apple), WMA (Microsoft)\n1–10 MB/min\n\n\nVideo\nWebM/VP9\n(MP4/H.264), MOV (Apple), WMV (Microsoft)\n10 MB/min (HD) → GBs/min (4K+)\n\n\nInternet (Common Crawl)\nWARC (HTTP)\n-\n100 TB/ month ~ 2GB/min\n\n\nGenomics (sequencing machines)\nFASTQ, VCF, BAM, …\nBCL (Illumina), pod5 (ONT)\n34 GB/min → 2 GB/min\n\n\nEarth Observation System (NASA)\nHDF5\n-\n?? → 2 GB/min\n\n\nParticle physics (CERN LHC)\nROOT\n-\n1 PB/s → 2000 GB/min\n\n\nSquare Kilometer Array\nHDF5, FITS\n-\n75 TB/min → 1 TB/min\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\non-the-fly processing for large data\nship metadata with data\navoid data duplication\ncompression"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-io",
    "href": "lectures/01_MachineLearning/01_Data.html#data-io",
    "title": "Data Stories",
    "section": "Data I/O",
    "text": "Data I/O\nThere are several ways to get data into RAM for analysis\n\ntabular data\n\n\nCode\nimport pandas as pd\n\n# from local paths or URL\nurl=\"https://github.com/thomasmanke/DataSets/raw/refs/heads/main/iris.csv.gz\"\ndf = pd.read_csv(url, compression=\"gzip\")\n\nprint(df.head()) # A glimpse\n\n\n   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n0           5.1          3.5           1.4          0.2  setosa\n1           4.9          3.0           1.4          0.2  setosa\n2           4.7          3.2           1.3          0.2  setosa\n3           4.6          3.1           1.5          0.2  setosa\n4           5.0          3.6           1.4          0.2  setosa"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-integrity",
    "href": "lectures/01_MachineLearning/01_Data.html#data-integrity",
    "title": "Data Stories",
    "section": "Data Integrity",
    "text": "Data Integrity\nSometimes you may have to check data integrity after downloads and transfers\n\n\nCode\nimport requests\nimport hashlib\nimport pandas as pd\nfrom io import BytesIO\n\nurl=\"https://github.com/thomasmanke/DataSets/raw/refs/heads/main/iris.csv.gz\"\nresponse = requests.get(url)\ndata = response.content\nchecksum = hashlib.sha256(data).hexdigest()\nprint(f\"Checksum (SHA256) of compressed data: {checksum}\")\n\n# treat data as compressed file and load as before\n#data_io = BytesIO(data) \n#df = pd.read_csv(data_io, compression=\"gzip\")\n\n\nChecksum (SHA256) of compressed data: cc954f855bffd5fa2a5c80194f4c527b951a69e35b8ddb88661b9abb1cf84911"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-summaries",
    "href": "lectures/01_MachineLearning/01_Data.html#data-summaries",
    "title": "Data Stories",
    "section": "Data Summaries",
    "text": "Data Summaries\n\n\nCode\nprint(\"== Iris - Missing Values ==\")\nprint(df.isna().sum())     # column-wise      \nprint(\"== Iris - Statistical Summmary ==\")\nprint(df.describe(include=\"all\"))       \n\n\n== Iris - Missing Values ==\nSepal.Length    0\nSepal.Width     0\nPetal.Length    0\nPetal.Width     0\nSpecies         0\ndtype: int64\n== Iris - Statistical Summmary ==\n        Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\ncount     150.000000   150.000000    150.000000   150.000000     150\nunique           NaN          NaN           NaN          NaN       3\ntop              NaN          NaN           NaN          NaN  setosa\nfreq             NaN          NaN           NaN          NaN      50\nmean        5.843333     3.057333      3.758000     1.199333     NaN\nstd         0.828066     0.435866      1.765298     0.762238     NaN\nmin         4.300000     2.000000      1.000000     0.100000     NaN\n25%         5.100000     2.800000      1.600000     0.300000     NaN\n50%         5.800000     3.000000      4.350000     1.300000     NaN\n75%         6.400000     3.300000      5.100000     1.800000     NaN\nmax         7.900000     4.400000      6.900000     2.500000     NaN\n\n\n\n\n\n\n\n\nData \\(\\gg\\) Numbers\n\n\n\n\nsubject experise required \\(\\to\\) information on iris can be found here\ndata have metadata: when, who, where, why\nhave a question\nhave a model\n\n\n\n\nimage data\n\n\nCode\nfrom PIL import Image\nimport numpy as np\nimport requests\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\n\nurl = 'https://img-datasets.s3.amazonaws.com/elephant.jpg'\nresponse = requests.get(url)\nfo = BytesIO(response.content) # content2fileobject\nimg_obj = Image.open(fo)       # image object \nprint(f\"img_obj type: {type(img_obj)} size={img_obj.size} mode={img_obj.mode}\")\n\n# transformations (optional)\n#img_obj = img_obj.resize((200, 200))\n#img_obj = img_obj.rotate(45) \n\n# conversion: numpy array\nimg = np.array(img_obj, dtype=np.uint8)\nprint(f\"img type: {type(img)} shape: {img.shape}\")\n\n\nimg_obj type: &lt;class 'PIL.JpegImagePlugin.JpegImageFile'&gt; size=(1440, 1440) mode=RGB\nimg type: &lt;class 'numpy.ndarray'&gt; shape: (1440, 1440, 3)\n\n\n\n\nCode\nplt.imshow(img) # imshow accepts both pillow img and np\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\npackaged data\nSome datasets are very famous and they have been packaged for easy access.\n\n\nCode\nimport pandas as pd\nfrom seaborn import load_dataset\nfrom plotly.express import data\nfrom torchvision import datasets\n\n# iris from seaborn\niris = load_dataset(\"iris\")\nprint(f\"{type(iris)} {iris.shape}\")  \n\n# gapminder from px\ngapminder = data.gapminder()\nprint(f\"{type(gapminder)} {gapminder.shape}\")\n\n# mnist from torchvision\nmnist = datasets.MNIST(root=\"./data\", train=True, download=True)\nprint(f\"{type(mnist.data)} {mnist.data.shape}\")  \n\n# Reshaping and data type conversions\nmnist_np = mnist.data.flatten(start_dim=1).numpy()\nprint(f\"{type(mnist_np)} {mnist_np.shape}\")  \n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt; (150, 5)\n&lt;class 'pandas.core.frame.DataFrame'&gt; (1704, 8)\n&lt;class 'torch.Tensor'&gt; torch.Size([60000, 28, 28])\n&lt;class 'numpy.ndarray'&gt; (60000, 784)\n\n\n\n\n\n\n\n\nTask (10 min)\n\n\n\nSearch online for more detailed descriptions of those famous datasets (Iris, Gapminder, MNIST)\nDiscuss with your neighbour:\n\nWhat is in the data?\nHow many samples?\nHow many variables/features?\nWhich questions could we ask about the data?\n\nReport back to class.\n\n\n\n\nCode\n# Sometimes the data comes with descriptions\nfrom sklearn.datasets import load_iris\niris = load_iris()\nprint(iris.DESCR)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nclean data from packages\ndata types: package-dependent, may need conversion/reshaping\npay attention to shapes and dimensions\nwatch out for metadata and descriptions"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-analysis",
    "href": "lectures/01_MachineLearning/01_Data.html#data-analysis",
    "title": "Data Stories",
    "section": "Data Analysis",
    "text": "Data Analysis\n\n\n\nData Analysis Workflow. Iterations and Repetitions (from Hadley Wickham).\n\n\n\n\n\n\n\n\nData Analysis \\(\\gg\\) Modeling\n\n\n\n\n\n\n\n\n\n\n\nStage\nExample tasks\nTypical time share\n\n\n\n\nData Acquisition\ngathering data and metadata, checking consistency, negotiating access\n10–20 %\n\n\nData Cleaning\nHandling missing values, reshaping tables, fixing types, deduplication\n20–60 %\n\n\nData Transformation\nNormalization, scaling, filtering, creating derived variables\n20–30 %\n\n\nData Exploration\nVisualization, summary statistics, feature selection, sanity checks\n10–20 %\n\n\nData Modeling\nModel selection, Model training/(deep)learning, evaluation\n5–20 %\n\n\nData Communication\nReports, dashboards, storytelling\n5–10 %\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMost of this course will focus on “Data Modeling”"
  },
  {
    "objectID": "lectures/01_MachineLearning/01_Data.html#data-science-challenges",
    "href": "lectures/01_MachineLearning/01_Data.html#data-science-challenges",
    "title": "Data Stories",
    "section": "Data Science Challenges",
    "text": "Data Science Challenges\n\nLanguages and Jargon\nDomain Expertise\nDirty Data\nFAIR data\nStatistics & Maths\n(Programming)\n\n\n\n\n“Many safe roads at land; countless doomed routes at sea.”"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html",
    "title": "What is Machine Learning ?",
    "section": "",
    "text": "“The principal difficulty … lay in the fact of there being too much evidence. What was vital was overlaid and hidden by what was irrelevant. Of all the facts which were presented to us we had to pick just those which we deemed to be essential, and then piece them together in their order, so as to reconstruct this very remarkable chain of events.” Sherlock Holmes (The Naval treaty, 1893)\n\n\n\nCan we write a program to do the same?"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#how-do-we-recognize-label-images",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#how-do-we-recognize-label-images",
    "title": "What is Machine Learning ?",
    "section": "",
    "text": "“The principal difficulty … lay in the fact of there being too much evidence. What was vital was overlaid and hidden by what was irrelevant. Of all the facts which were presented to us we had to pick just those which we deemed to be essential, and then piece them together in their order, so as to reconstruct this very remarkable chain of events.” Sherlock Holmes (The Naval treaty, 1893)\n\n\n\nCan we write a program to do the same?"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#a-paradigm-shift",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#a-paradigm-shift",
    "title": "What is Machine Learning ?",
    "section": "A paradigm shift",
    "text": "A paradigm shift\n\nClassical Programming: Rules (f) + Data (X) –&gt; Answers (Y)\n\nMachine Learning: Answers (Y) + Data (X) –&gt; Rules (f)\n\nQuiz (5 min): A simpler challenge: Find the rule \\(y = f(x)\\) for x and y below\n\n\nCode\nimport numpy as np\n\nx = np.array([-1,  0, 1, 2, 3, 4])     # Data\ny = np.array([-3, -1, 1, 3, 5, 7])     # Answer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice: a more realistic scenario would allow for errors: \\(y = f(x) + \\epsilon\\)"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#the-math-way-gauss-1809",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#the-math-way-gauss-1809",
    "title": "What is Machine Learning ?",
    "section": "The Math Way (Gauss: 1809)",
    "text": "The Math Way (Gauss: 1809)\n\nModel Definition\n\\[\n\\begin{align}\n\\mbox{Linear model:} ~~~& y=f(x_i; a,b) = a  + b x_i \\\\ \\\\\n\\mbox{2 Paramters:} ~~~& \\theta = (a,b) ~~~~\\mbox{same for all  } x_i\n\\end{align}\n\\]\n\n\nA goal\n\\[\n\\begin{align}\nL(\\theta) = \\sum_i (y_i - f(x_i;\\theta))^2  \\to \\mbox{minimize!}\\\\\n\\underset{\\theta}{\\mbox{argmin}} ~L(\\theta) \\longrightarrow \\hat{\\theta}\n\\end{align}\n\\]\n\nlinear regression = ordinary least squares (OLS)\nsynonymous terms: goal = loss function = cost function = objective function = …\nsolving the goal = fitting = training = learning = parameter estimation = (linear) regression\nfor linear models: fast and analytical solution !!\n\n\n\nOptimal Paramters\n\\[\n\\hat{\\theta} = (a,b) = (-1,2)\n\\]\n\n\nPredictions\nrun best model for new values, e.g. \\(x= (10, -40, \\ldots)\\)\n\n\nCode\nx_new = np.array([10, -40])\ny_new = -1 + 2.0*x_new\nprint('predictions: ', x_new, \"-&gt;\", y_new)\n\n\npredictions:  [ 10 -40] -&gt; [ 19. -81.]\n\n\n\n\n\n\n\n\nModeling Steps\n\n\n\n\ndefine the data\ndefine the model\nfit parameters\nevaluate model\nuse model (predict)"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#the-python-way-sklearn-2013",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#the-python-way-sklearn-2013",
    "title": "What is Machine Learning ?",
    "section": "The Python way (sklearn: 2013)",
    "text": "The Python way (sklearn: 2013)\n\n\nCode\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\n\nlm = linear_model.LinearRegression()  # define model\nxr = x.reshape(-1,1)                  # define data (+restructuring for specific tool) \n\nlm.fit(xr, y)             \n\n# report fit\nprint('Fitted Parameters       ', lm.intercept_, lm.coef_)\n\nyp = lm.predict(xr)                   # prediction\nMSE= mean_squared_error(y, yp)        # evaluate fit. other scores: R2=lm.score(xr, y)\nprint('Mean Squared Error:     ', MSE)\n\n# predict y for some new x\nx_new = np.linspace(-10, 10, 20)\ny_new = lm.predict(x_new.reshape(-1,1))\n\n\nFitted Parameters        -1.0 [2.]\nMean Squared Error:      0.0\n\n\n\n\nCode\ntitle_str = f\"Loss = {MSE:.3f}; Intercept = {lm.intercept_.item():.3f}, Slope = {lm.coef_.item():.3f}\"\nplt.figure(figsize=(6,4))\nplt.plot(x, y, 'o', label=\"Data\")\nplt.plot(x_new, y_new, label=\"Model prediction\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(title_str)\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#the-pytorch-way-2025",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#the-pytorch-way-2025",
    "title": "What is Machine Learning ?",
    "section": "The PyTorch Way (2025)",
    "text": "The PyTorch Way (2025)\nLearn parameters by iterative improvements\n\\[\n\\begin{array}{ll}\n\\textbf{Input:} & \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\} \\\\\n\\textbf{Initialize:} & \\theta \\text{ randomly} \\\\\n\\textbf{Repeat:} & \\\\\n& y_{\\mathrm{pred}} \\gets f(x; \\theta) \\\\\n& L \\gets L(y_{\\mathrm{pred}}, y) \\\\\n& \\theta \\gets \\text{update}(\\theta, L) \\\\\n\\textbf{Until:} & \\text{stopping criterion is met}\n\\end{array}\n\\]\n\n\nCode\nimport numpy as np\nimport torch\nprint('torch version:', torch.__version__)\n\n# convert to tensor, type and reshape to column vector\nx_t = torch.as_tensor(x, dtype=torch.float32).view(-1, 1)\ny_t = torch.as_tensor(y, dtype=torch.float32).view(-1, 1)\n\nmodel = torch.nn.Linear(1,1) # define black box model (1 input and 1 output)\nloss_func = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# training loop\nlosses = []\nmodel.train()\nfor epoch in range(200):\n    y_pred = model(x_t)             # model prediction\n    loss = loss_func(y_pred, y_t)   # model loss\n\n    optimizer.zero_grad()           # initalizations\n    loss.backward()                 # calculate new parameters\n    optimizer.step()                # update parameters\n\n    losses.append(loss.item())      # track loss history\n\n# finished training. inspect model \nprint(f'Mean Squared Error (loss) = {loss.item():.5f}')\nprint('Trained Parameters')\nprint(f'PyTorch: intercept =  {model.bias.item():.3f},  slope={model.weight.item():.3f}')\nprint(f'sklearn: intercept =  {lm.intercept_.item():.3f},  slope={lm.coef_.item():.3f}')\n\n\ntorch version: 2.7.1\nMean Squared Error (loss) = 0.01165\nTrained Parameters\nPyTorch: intercept =  -0.859,  slope=1.954\nsklearn: intercept =  -1.000,  slope=2.000\n\n\n\n\n\n\n\n\nDon’t Panic !!!\n\n\n\n\nlooks more complicated than sklearn, but is much more flexible and powerful\niterative approach is generic and extends to much more complex models, far beyond linear.\nPyTorch supports all modeling steps: define model, define loss function, define optimizer, fit model, predict.\nThere are alternative frameworks: TensorFlow/Keras, … \\(\\to\\) Tutorial\nI will use PyTorch, but it should be possible to transfer everything also to TF/Keras\nPyTorch has new data structures that need to get used to: torch.Tensor\npredictions less accurate (and slower) than with sklearn \\(\\to\\) iterations not yet converged\nloss (MSE) can be monitored to assess convergence"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#monitor-training",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#monitor-training",
    "title": "What is Machine Learning ?",
    "section": "Monitor Training",
    "text": "Monitor Training\n\n\nCode\nplt.figure()\nplt.plot(losses)\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.title('Loss history')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nloss \\(L\\) should decrease for sensible parameter updates\nloss histories are the most important tools to assess learning progress"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#making-predictions",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#making-predictions",
    "title": "What is Machine Learning ?",
    "section": "Making predictions",
    "text": "Making predictions\nEven though our current best model is not very good, we already have a blackbox that allows us to make prediction for any new data \\(x\\). Like so:\n\n\nCode\n# define some new input data  x_new\nx_new = np.linspace(-10, 10, 20)\nx_t = torch.as_tensor(x_new, dtype=torch.float32).view(-1, 1)\n\n# blackbox predcition\ny_new = model(x_t) \n\n\n\n\nCode\nintercept = model.bias.item()\nslope = model.weight.item()\ntitle_str = f\"Loss = {loss.item():.3f}; Intercept = {intercept:.3f}, Slope = {slope:.3f}\"\nplt.figure(figsize=(6,4))\nplt.plot(x, y, 'o', label=\"Data\")\nplt.plot(x_new, y_new.detach(), label=\"Model prediction\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(title_str)\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "lectures/01_MachineLearning/02_LinearRegression.html#summary",
    "href": "lectures/01_MachineLearning/02_LinearRegression.html#summary",
    "title": "What is Machine Learning ?",
    "section": "Summary",
    "text": "Summary\n\n3 ways to do linear regression - 1 common goal: minimize loss (MSE)\nGauss (linear algebra), scikit-learn (Gauss + Python), PyTorch (iterations & optimization)\n\n\n\n\nA simple neuron calculating a linear function."
  },
  {
    "objectID": "lectures/01_MachineLearning/index.html",
    "href": "lectures/01_MachineLearning/index.html",
    "title": "01 Machine Learning",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nData Stories\n\n\n\nData Formats\n\nData Types\n\nData Analysis Workflow\n\nData Challenges\n\nMetadata\n\n\n\nData is everywhere and in all shapes\n\n\n\nThomas Manke\n\n\nOct 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Machine Learning ?\n\n\n\nsklearn\n\npytorch\n\nmachine learning\n\nloss function\n\nepochs\n\ntraining loop\n\n\n\nCan machines learn?\n\n\n\nThomas Manke\n\n\nOct 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nModels 101\n\n\n\nlinear model\n\nRSS\n\nparameters\n\nloss function\n\nmodel matrix\n\nmodel checking\n\nexplained variance\n\n\n\nSome models are useful\n\n\n\nThomas Manke\n\n\nOct 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Linear Regression\n\n\n\nmultivariate regression\n\niris\n\n\n\nHandle multiple inputs\n\n\n\nThomas Manke\n\n\nOct 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Regression\n\n\n\nmultinomial regression\n\niris\n\n\n\nHandle multiple outputs\n\n\n\nThomas Manke\n\n\nOct 28, 2025\n\n\n\n\n\n\n\n\n\n\n\nTraining And Testing Data\n\n\n\noverfitting\n\nunderfitting\n\n\n\nuse different data set to test your model\n\n\n\nThomas Manke\n\n\nOct 28, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures/01_MachineLearning/06_TrainingAndTesting.html",
    "href": "lectures/01_MachineLearning/06_TrainingAndTesting.html",
    "title": "Training And Testing Data",
    "section": "",
    "text": "Code\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# helper function\ndef poly_features(x, degree: int):\n    # built polynomial features from x (up to degree)\n    # x -&gt; [1, x, x^2, ..., x^degree]\n    cols = [torch.ones_like(x)]\n    for p in range(1, degree + 1):\n        cols.append(x ** p)\n    return torch.cat(cols, dim=1)\n\ndef model_train(model, X, y, epochs=2000, lr=0.05):\n    loss_func = torch.nn.MSELoss()\n    opt = torch.optim.Adam(model.parameters(), lr=lr)\n#    opt = torch.optim.SGD(model.parameters(), lr=lr)\n    model.train()\n    for _ in range(epochs):\n        opt.zero_grad()\n        loss = loss_func(model(X), y)\n        loss.backward()\n        opt.step()\n    return loss.item()\n\ndef model_predict(model, X):\n    model.eval()\n    with torch.no_grad():\n        return model(X)\n\ndef model_test(model, X, y):\n    loss_func = torch.nn.MSELoss()\n    with torch.no_grad():\n        return loss_func(model(X), y).item()"
  },
  {
    "objectID": "lectures/01_MachineLearning/06_TrainingAndTesting.html#generate-synthetic-data",
    "href": "lectures/01_MachineLearning/06_TrainingAndTesting.html#generate-synthetic-data",
    "title": "Training And Testing Data",
    "section": "Generate synthetic data",
    "text": "Generate synthetic data\n\n\nCode\nseed = 0\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nn = 10\nnoise_sigma = 0.75\n\n# data range\nx = torch.linspace(-2.0, 2.0, n).unsqueeze(1) \n# synthetic data. very simple y = x + noise\ny = x + noise_sigma * torch.randn_like(x)\n\n# define plotting range (for later convenience)\nx_plot = torch.linspace(-2.1, 2.1, 200).unsqueeze(1)"
  },
  {
    "objectID": "lectures/01_MachineLearning/06_TrainingAndTesting.html#train-and-test-data",
    "href": "lectures/01_MachineLearning/06_TrainingAndTesting.html#train-and-test-data",
    "title": "Training And Testing Data",
    "section": "Train and Test Data",
    "text": "Train and Test Data\n\n\nCode\n# Train-test split (80/20)\nn_train = int(0.8 * n) # number of training data \n\n# permute indices [0,1,...,n-1] --&gt; [3, 1, 6, ....]\nperm = torch.randperm(n)  \ntrain_idx, test_idx = perm[:n_train], perm[n_train:]\nx_train, y_train = x[train_idx], y[train_idx]\nx_test, y_test = x[test_idx], y[test_idx]\n\n# plot orginal data\nplt.figure(figsize=(7,5))\nplt.scatter(x_train, y_train, color='black', label='train data', s=50)\nplt.scatter(x_test, y_test, color='red', label='test data', s=50)\nplt.title(\"Train and Test Data\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()"
  },
  {
    "objectID": "lectures/01_MachineLearning/06_TrainingAndTesting.html#fitting-under-and-over",
    "href": "lectures/01_MachineLearning/06_TrainingAndTesting.html#fitting-under-and-over",
    "title": "Training And Testing Data",
    "section": "Fitting: Under and Over",
    "text": "Fitting: Under and Over\n\n\n\n\n\n\nPolynomial Fit\n\n\n\n\n\n\n\nCode\npf = poly_features(x, 3)\nprint(pf[:5])\n\n\ntensor([[ 1.0000, -2.0000,  4.0000, -8.0000],\n        [ 1.0000, -1.5556,  2.4198, -3.7641],\n        [ 1.0000, -1.1111,  1.2346, -1.3717],\n        [ 1.0000, -0.6667,  0.4444, -0.2963],\n        [ 1.0000, -0.2222,  0.0494, -0.0110]])\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(7,5))\nplt.scatter(x_train, y_train, color='black', label='train data', s=50)\nplt.scatter(x_test, y_test, color='red', label='test data', s=50)\n\n# plot fits of different polynomial degrees\ndegrees = [0, 1, 7]\ncolors = ['blue', 'green', 'orange']\nfor d, col in zip(degrees, colors):\n    Xtr = poly_features(x_train, d)\n    Xpl = poly_features(x_plot, d)\n    model = torch.nn.Linear(d + 1, 1, bias=False)\n    model_train(model, Xtr, y_train)\n    y_pred = model_predict(model, Xpl)\n    plt.plot(x_plot, y_pred, color=col, lw=3, label=f\"d={d}\")\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Polynomial fits and overfits\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nhigh degree polynomials \\(\\to\\) complex models, many parameters, high flexibilty\ncomplex models can fit training data (black) points with high accuracy\nerror tends to be large on left-out data (test data, red)\nevaluate models on test data !\n\n\n\n\n\nCode\ndegrees = range(0, 9)\ntrain_mse, test_mse = [], []\nfor d in degrees:\n    Xtr = poly_features(x_train, d)\n    Xte = poly_features(x_test, d)\n    model = torch.nn.Linear(d + 1, 1, bias=False)\n    train_loss = model_train(model, Xtr, y_train)\n    test_loss = model_test(model, Xte, y_test)\n    train_mse.append(train_loss)\n    test_mse.append(test_loss)\n\nplt.figure(figsize=(7,5))\nplt.plot(degrees, train_mse, marker='o', label='Train MSE')\nplt.plot(degrees, test_mse, marker='o', label='Test MSE')\nplt.xlabel('Polynomial degree (model complexity)')\nplt.ylabel('Mean Squared Error')\nplt.title('Train vs Test error')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\noverfitted models do not generalize well to unseen data\nmany regularization methods"
  },
  {
    "objectID": "lectures/01_MachineLearning/06_TrainingAndTesting.html#appendix-more-splitting",
    "href": "lectures/01_MachineLearning/06_TrainingAndTesting.html#appendix-more-splitting",
    "title": "Training And Testing Data",
    "section": "Appendix: More splitting",
    "text": "Appendix: More splitting\nThere is also a powerful (and frequently used) method to conduct the train-test split with sklearn functionality. But this requires changing from torch tensors to numpy arrays (and convert back to tensors).\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\nprint(f'x.shape: {x.shape} y.shape {y.shape}')\nx_train, x_test, y_train, y_test = train_test_split(\n    x.numpy(), y.numpy(), test_size=0.2, random_state=42\n)\n\n# Convert back to torch\nx_train = torch.tensor(x_train, dtype=torch.float32)\nx_test  = torch.tensor(x_test, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32)\ny_test  = torch.tensor(y_test, dtype=torch.float32)\n\nprint(f'x_train.shape: {x_train.shape} y_train.shape {y_train.shape}')\nprint(f'x_test.shape: {x_test.shape} y_test.shape {y_test.shape}')\n\n\nx.shape: torch.Size([10, 1]) y.shape torch.Size([10, 1])\nx_train.shape: torch.Size([8, 1]) y_train.shape torch.Size([8, 1])\nx_test.shape: torch.Size([2, 1]) y_test.shape torch.Size([2, 1])"
  },
  {
    "objectID": "lectures/01_MachineLearning/04_MultivariateRegression.html",
    "href": "lectures/01_MachineLearning/04_MultivariateRegression.html",
    "title": "Multivariate Linear Regression",
    "section": "",
    "text": "A simple neuron calculating a simple linear function.\n\n\n\n\n\n\n\n\nSome Lessons from iris\n\n\n\n\nThere is no perfect fit because real data is noisy. In general: MSE \\(\\ne 0\\)\nModels are ignorant: What is the Petal.Width for Petal.Length = -1cm ?\nModels are not necessarily causal\n“All models are wrong, some are useful”\nAssessments of predicted performance should invoke some left out data (test set)"
  },
  {
    "objectID": "lectures/01_MachineLearning/04_MultivariateRegression.html#summary",
    "href": "lectures/01_MachineLearning/04_MultivariateRegression.html#summary",
    "title": "Multivariate Linear Regression",
    "section": "",
    "text": "A simple neuron calculating a simple linear function.\n\n\n\n\n\n\n\n\nSome Lessons from iris\n\n\n\n\nThere is no perfect fit because real data is noisy. In general: MSE \\(\\ne 0\\)\nModels are ignorant: What is the Petal.Width for Petal.Length = -1cm ?\nModels are not necessarily causal\n“All models are wrong, some are useful”\nAssessments of predicted performance should invoke some left out data (test set)"
  },
  {
    "objectID": "lectures/01_MachineLearning/04_MultivariateRegression.html#generalization-more-input-variables",
    "href": "lectures/01_MachineLearning/04_MultivariateRegression.html#generalization-more-input-variables",
    "title": "Multivariate Linear Regression",
    "section": "Generalization: more input variables",
    "text": "Generalization: more input variables\n\n\n\nA simple neuron receiving multiple inputs - and calculating a linear function.\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport torch\nimport torch.nn as nn\n\n\n\n\nCode\n# get iris data from sklearn\niris = datasets.load_iris() \nX = iris.data[:,:3] # first three columns  \ny = iris.data[:,3]  # forth column\n\n# convert to tensors\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\nprint(f\"X.shape = {X.shape}, y.shape = {y.shape}\")\n\n\nX.shape = torch.Size([150, 3]), y.shape = torch.Size([150, 1])"
  },
  {
    "objectID": "lectures/01_MachineLearning/04_MultivariateRegression.html#model-training",
    "href": "lectures/01_MachineLearning/04_MultivariateRegression.html#model-training",
    "title": "Multivariate Linear Regression",
    "section": "Model Training",
    "text": "Model Training\n… requires minimal changes to pytorch code\n\n\nCode\nn_epochs = 1000\nmodel = nn.Linear(3, 1)  # only change: n_in = 3, n_out = 1\n\n# all below as before\nloss_func = nn.MSELoss()  # MSE!\noptimizer = torch.optim.SGD(model.parameters()) \n\nmodel.train()\nfor epoch in range(n_epochs):\n    y_pred = model(X) \n    loss = loss_func(y_pred, y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(\"Fitted parameters:\")\nprint(\"Intercept (bias):\", model.bias.item())\n\n## Notice more weights\nprint(\"Coefficients (weights):\", model.weight.detach().numpy())\nprint('Loss: ', loss)    \n\n\nFitted parameters:\nIntercept (bias): 0.2394721806049347\nCoefficients (weights): [[ 0.0421822  -0.21746331  0.36797115]]\nLoss:  tensor(0.0560, grad_fn=&lt;MseLossBackward0&gt;)"
  },
  {
    "objectID": "lectures/01_MachineLearning/04_MultivariateRegression.html#predict-and-plot",
    "href": "lectures/01_MachineLearning/04_MultivariateRegression.html#predict-and-plot",
    "title": "Multivariate Linear Regression",
    "section": "Predict and Plot",
    "text": "Predict and Plot\nNow the predicted values y_pred depend on multiple inputs high-dimensional inputs x. It’s convenient to plot the true value y_true against y_pred = f(x).\n\n\nCode\nmodel.eval()\ny_pred = model(X)\n\n# convert to numpy (only for plotting)\nyp = y_pred.detach().numpy()\nyt = y.numpy()\n\ntitle_str = f\"Truth vs Predictions. Loss = MSE = {loss:.3f}\"\nplt.figure(figsize=(6,4))\nplt.plot(yp, yt, 'o')\nplt.xlabel(\"y_pred = f(x)\")\nplt.ylabel(\"y_true\")\nplt.title(title_str)\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n/var/folders/jh/jwym2_j128v8nn1dwc0sc10m0000gn/T/ipykernel_47611/1653731775.py:14: UserWarning:\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument."
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "neural networks: layers of neurons connected by weight\nin each neuron: weights + bias + non-linear activation function\noptimization: gradient descent, SGD , momentum, ADAM\nbackprogation: gradients with chain rule on computational graph\ntechnical: vanishing gradients –&gt; batch norm, weight initialization\noverfitting and generalization: data augmentation, regularization (L2, dropout)"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#review",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#review",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "neural networks: layers of neurons connected by weight\nin each neuron: weights + bias + non-linear activation function\noptimization: gradient descent, SGD , momentum, ADAM\nbackprogation: gradients with chain rule on computational graph\ntechnical: vanishing gradients –&gt; batch norm, weight initialization\noverfitting and generalization: data augmentation, regularization (L2, dropout)"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#why-not-add-more-layers",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#why-not-add-more-layers",
    "title": "Convolutional Neural Networks",
    "section": "Why not add more layers",
    "text": "Why not add more layers\n\nNN with one hidden layer are universal function approximators\notpimization hard: finding parameters to give optimal decision boundary (for classification)\nno structure; just brute force\nblack boxes: parameter interpretation\nperformance plateau\n\n–&gt; more structure: - MLP: no structure –&gt; permutation of input no changes - CNN: spatial structure (images) - RNN: temporal structure (sequences) Transformers –&gt; from classifiers to generators"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#problem",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#problem",
    "title": "Convolutional Neural Networks",
    "section": "Problem:",
    "text": "Problem:\nfully connected layers: input(1000 x 1000 x 3 pixels) –&gt; 1 hidden layer(1000 neurons) –&gt; 3 billion parameters\nrestrict capacity of each layer –&gt; weight sharing\ninvariance assumption: should not matter where feature is\nCNN: start point for subsequence task - classification - object localization (bounding box) - object detection (multiple bounding boxes) - instance segmentation (point-wise classification)"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#what-are-convolutions",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#what-are-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "What are convolutions?",
    "text": "What are convolutions?\n\n1D\n–&gt; i2DL\n\n\n2D\n–&gt; i2DL"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#cnn-architecture",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#cnn-architecture",
    "title": "Convolutional Neural Networks",
    "section": "CNN Architecture",
    "text": "CNN Architecture\n\nNN: pass 1-D vectors from layer to layer\nCNN: match network to spatial structure (2D images)\n\nkeep input 2-D (actually: CxWxH) –&gt; also arrange neurons in 2-D (actually DxWxH)\n\nNumber of outputs from Convolutional Layer\n\\[\nN_{out} = \\frac{N_{in} - F + 2P}{S} + 1\n\\]\nNotice\n\n\\((N_{in} - F + 2P)/S\\) has to be integer for this to work properly.\n\n\n\nNumbers and Memory\n\n2-layer conv\n\n\n\n\n\n\n\n\n\n\n\n\nLayer\nOutput Shape\nParam Formula\n#Outputs\n#Params\nParam Mem (MB)\nOut Mem (MB)\n\n\n\n\nInput\n(1, 3, 224, 224)\n—\n150,528\n0\n0.00\n0.60\n\n\nConv1\n(1, 64, 224, 224)\n3×3×3×64 + 64\n3,211,264\n1,792\n0.01\n12.84\n\n\nMaxPool\n(1, 64, 112, 112)\n—\n802,816\n0\n0.00\n3.21\n\n\nConv2\n(1, 64, 112, 112)\n3×3×64×64 + 64\n802,816\n36,928\n0.15\n3.21\n\n\nFlatten\n(1, 802816)\n—\n802,816\n0\n0.00\n3.21\n\n\nFC\n(1, 10)\n802816×10 + 10\n10\n8,028,170\n32.11\n0.00\n\n\nTotal\n—\n—\n—\n8,066,890\n32.27\n23.07\n\n\n\n\n\nVggNet (2014)\n\n\n\n\n\n\n\n\n\n\n\n\nLayer\nOutput Shape\nParam Formula\n#Outputs\n#Params\nParam Mem (MB)\nOutput Mem (MB)\n\n\n\n\nInput\n224×224×3\n—\n150,528\n0\n0.00\n0.60\n\n\nConv1_1\n224×224×64\n3×3×3×64 + 64\n3,211,264\n1,792\n0.01\n12.84\n\n\nConv1_2\n224×224×64\n3×3×64×64 + 64\n3,211,264\n36,928\n0.15\n12.84\n\n\nMaxPool1\n112×112×64\n—\n802,816\n0\n0.00\n3.21\n\n\nConv2_1\n112×112×128\n3×3×64×128 + 128\n1,605,632\n73,856\n0.30\n6.42\n\n\nConv2_2\n112×112×128\n3×3×128×128 + 128\n1,605,632\n147,584\n0.59\n6.42\n\n\nMaxPool2\n56×56×128\n—\n401,408\n0\n0.00\n1.61\n\n\nConv3_1\n56×56×256\n3×3×128×256 + 256\n802,816\n295,168\n1.18\n3.21\n\n\nConv3_2\n56×56×256\n3×3×256×256 + 256\n802,816\n590,080\n2.36\n3.21\n\n\nConv3_3\n56×56×256\n3×3×256×256 + 256\n802,816\n590,080\n2.36\n3.21\n\n\nMaxPool3\n28×28×256\n—\n200,704\n0\n0.00\n0.80\n\n\nConv4_1\n28×28×512\n3×3×256×512 + 512\n401,408\n1,180,160\n4.72\n1.61\n\n\nConv4_2\n28×28×512\n3×3×512×512 + 512\n401,408\n2,359,808\n9.44\n1.61\n\n\nConv4_3\n28×28×512\n3×3×512×512 + 512\n401,408\n2,359,808\n9.44\n1.61\n\n\nMaxPool4\n14×14×512\n—\n100,352\n0\n0.00\n0.40\n\n\nConv5_1\n14×14×512\n3×3×512×512 + 512\n100,352\n2,359,808\n9.44\n0.40\n\n\nConv5_2\n14×14×512\n3×3×512×512 + 512\n100,352\n2,359,808\n9.44\n0.40\n\n\nConv5_3\n14×14×512\n3×3×512×512 + 512\n100,352\n2,359,808\n9.44\n0.40\n\n\nMaxPool5\n7×7×512\n—\n25,088\n0\n0.00\n0.10\n\n\nFC_1\n4096\n7×7×512×4096 + 4096\n4,096\n102,764,544\n411.06\n0.02\n\n\nFC_2\n4096\n4096×4096 + 4096\n4,096\n16,781,312\n67.13\n0.02\n\n\nFC_3\n1000\n4096×1000 + 1000\n1,000\n4,097,000\n16.39\n0.00\n\n\nTotal\n—\n—\n—\n138M+\n564 MB\n~77 MB\n\n\n\nNotice\n\nmemory and compute in early layers, parameters in last layer\nnumbers can more than double when the gradients are calculated + caching\nmaintain also multiple images (batch normalization)\n\n\n\nPython Implementation\nDefine Model:\n\n\nCode\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport numpy as np\n\nclass my_model(nn.Module):\n    \"\"\" \n    My simple Convolutional Network\n    input shape: [,3,224,224] \n    output shape: [,10]\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.flatten = nn.Flatten()\n        self.fc = nn.Linear(112 * 112 * 64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.conv2(x)\n        x = self.flatten(x) \n        x = self.fc(x)\n        return x\n\n\nNotice that each model has a defined input shape (e.g. [,3,224,224]) and a defined output (e.g [,10]). The inner layers should work such that the dimensions match.\nIdeally we also define pre-processing steps and transformers to adjust those dimensions when other data is provided.\nEmploy Model:\n\n\nCode\nmodel = my_model()\n#model = models.vgg16() # precompiled model from torchvision.models\n\n\nGet parameter dimensions\n\n\nCode\n# number and shapes of parameters can be obtained directly from model\n# there is also a package that could do the same\n# pip install torchsummary --&gt; torchsummary.summary(model, input_shape) \nfor name, layer in model.named_modules():\n    num_params = sum(p.numel() for p in layer.parameters())\n    print(f\"{layer.__class__.__name__:12} {num_params}\")\n\n# shapes of parameters: notice that we have weights and biases\nfor name, parameter in model.named_parameters():\n    print(name, parameter.shape)\n\n\nmy_model     8066890\nConv2d       1792\nMaxPool2d    0\nConv2d       36928\nFlatten      0\nLinear       8028170\nconv1.weight torch.Size([64, 3, 3, 3])\nconv1.bias torch.Size([64])\nconv2.weight torch.Size([64, 64, 3, 3])\nconv2.bias torch.Size([64])\nfc.weight torch.Size([10, 802816])\nfc.bias torch.Size([10])\n\n\nNotice that the number and shapes of parameters can be obtained directly from the model.\nIn contrast, the output sizes of each layer will depend on the dimension of the input data and can only be done at execution (forward pass)\nBelow I show how to use, hooks that allow for efficient manipulation of the forward path, such that all quantities of interest can be tracked. Here I will track the shapes\nDefine Hook\n\n\nCode\ndef describe_model_forward(model, input_tensor):\n    \"\"\" \n    describe_model_forward collects information on the shapes of parameters and outputs \n    on each computational layer of a neural network\n    \"\"\"\n    layer_info = []\n    hooks = []\n\n    # define a \"hook\" function that can be passed to the model\n    # and evaluate as each layer is run in the foward path\n    def register_hook(module):\n        def hook(module, input, output):\n            name = module.__class__.__name__\n            # assumption: output of model is single tensor\n            output_shape = tuple(output.shape)\n            param_shapes = [tuple(p.shape) for p in module.parameters() if p.requires_grad]\n\n            layer_info.append((name, output_shape, param_shapes))\n\n        # add hook to computational layers (not containers Sequential, ModuleList)\n        # Warning: layer and module is ALMOST synonymous, but modules may also be container of layers\n        # here we exclude them conditionally\n        # This was trial and error. I'm not sure if this exlcusion list is exhaustive in general\n        if not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList):\n            hooks.append(module.register_forward_hook(hook))\n\n    # Recursively add register_hook() function to each computational layer\n    model.apply(register_hook)\n\n    # Run forward pass\n    with torch.no_grad():\n        _ = model(input_tensor)\n\n    # Remove hooks to clean memory\n    for h in hooks:\n        h.remove()\n\n    return layer_info\n\n\nCollect Layer Information\n\n\nCode\nmodel = models.vgg16()\n# Define Data\nx = torch.randn(16, 3, 224, 224) # fake data ~ batch of images\n\n# Run forward path and collect information\nlayer_info = describe_model_forward(model, x)\n\n# Print out information\nfor (name, out_shape, param_shape) in layer_info:\n    # assume that there is always an output shape\n    n_out = int(np.prod(out_shape))\n    n_params = 0\n    # parameters shape maybe empty (e.g ReLU --&gt; 0 params)\n    if len(param_shape) == 2:\n        n_weights = np.prod(param_shape[0])\n        n_bias = np.prod(param_shape[1])\n        n_params = int(n_weights + n_bias)\n\n    print(f\"Layer {name:&lt;14} \\\n        output: {str(out_shape):&lt;25} {n_out:&lt;10,}\\\n        Parameters: {str(param_shape):&lt;15} {n_params:,}\"\n    )\n\n\nLayer Conv2d                 output: (16, 64, 224, 224)        51,380,224        Parameters: [(64, 3, 3, 3), (64,)] 1,792\nLayer ReLU                   output: (16, 64, 224, 224)        51,380,224        Parameters: []              0\nLayer Conv2d                 output: (16, 64, 224, 224)        51,380,224        Parameters: [(64, 64, 3, 3), (64,)] 36,928\nLayer ReLU                   output: (16, 64, 224, 224)        51,380,224        Parameters: []              0\nLayer MaxPool2d              output: (16, 64, 112, 112)        12,845,056        Parameters: []              0\nLayer Conv2d                 output: (16, 128, 112, 112)       25,690,112        Parameters: [(128, 64, 3, 3), (128,)] 73,856\nLayer ReLU                   output: (16, 128, 112, 112)       25,690,112        Parameters: []              0\nLayer Conv2d                 output: (16, 128, 112, 112)       25,690,112        Parameters: [(128, 128, 3, 3), (128,)] 147,584\nLayer ReLU                   output: (16, 128, 112, 112)       25,690,112        Parameters: []              0\nLayer MaxPool2d              output: (16, 128, 56, 56)         6,422,528         Parameters: []              0\nLayer Conv2d                 output: (16, 256, 56, 56)         12,845,056        Parameters: [(256, 128, 3, 3), (256,)] 295,168\nLayer ReLU                   output: (16, 256, 56, 56)         12,845,056        Parameters: []              0\nLayer Conv2d                 output: (16, 256, 56, 56)         12,845,056        Parameters: [(256, 256, 3, 3), (256,)] 590,080\nLayer ReLU                   output: (16, 256, 56, 56)         12,845,056        Parameters: []              0\nLayer Conv2d                 output: (16, 256, 56, 56)         12,845,056        Parameters: [(256, 256, 3, 3), (256,)] 590,080\nLayer ReLU                   output: (16, 256, 56, 56)         12,845,056        Parameters: []              0\nLayer MaxPool2d              output: (16, 256, 28, 28)         3,211,264         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 28, 28)         6,422,528         Parameters: [(512, 256, 3, 3), (512,)] 1,180,160\nLayer ReLU                   output: (16, 512, 28, 28)         6,422,528         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 28, 28)         6,422,528         Parameters: [(512, 512, 3, 3), (512,)] 2,359,808\nLayer ReLU                   output: (16, 512, 28, 28)         6,422,528         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 28, 28)         6,422,528         Parameters: [(512, 512, 3, 3), (512,)] 2,359,808\nLayer ReLU                   output: (16, 512, 28, 28)         6,422,528         Parameters: []              0\nLayer MaxPool2d              output: (16, 512, 14, 14)         1,605,632         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 14, 14)         1,605,632         Parameters: [(512, 512, 3, 3), (512,)] 2,359,808\nLayer ReLU                   output: (16, 512, 14, 14)         1,605,632         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 14, 14)         1,605,632         Parameters: [(512, 512, 3, 3), (512,)] 2,359,808\nLayer ReLU                   output: (16, 512, 14, 14)         1,605,632         Parameters: []              0\nLayer Conv2d                 output: (16, 512, 14, 14)         1,605,632         Parameters: [(512, 512, 3, 3), (512,)] 2,359,808\nLayer ReLU                   output: (16, 512, 14, 14)         1,605,632         Parameters: []              0\nLayer MaxPool2d              output: (16, 512, 7, 7)           401,408           Parameters: []              0\nLayer AdaptiveAvgPool2d         output: (16, 512, 7, 7)           401,408           Parameters: []              0\nLayer Linear                 output: (16, 4096)                65,536            Parameters: [(4096, 25088), (4096,)] 102,764,544\nLayer ReLU                   output: (16, 4096)                65,536            Parameters: []              0\nLayer Dropout                output: (16, 4096)                65,536            Parameters: []              0\nLayer Linear                 output: (16, 4096)                65,536            Parameters: [(4096, 4096), (4096,)] 16,781,312\nLayer ReLU                   output: (16, 4096)                65,536            Parameters: []              0\nLayer Dropout                output: (16, 4096)                65,536            Parameters: []              0\nLayer Linear                 output: (16, 1000)                16,000            Parameters: [(1000, 4096), (1000,)] 4,097,000\nLayer VGG                    output: (16, 1000)                16,000            Parameters: [(64, 3, 3, 3), (64,), (64, 64, 3, 3), (64,), (128, 64, 3, 3), (128,), (128, 128, 3, 3), (128,), (256, 128, 3, 3), (256,), (256, 256, 3, 3), (256,), (256, 256, 3, 3), (256,), (512, 256, 3, 3), (512,), (512, 512, 3, 3), (512,), (512, 512, 3, 3), (512,), (512, 512, 3, 3), (512,), (512, 512, 3, 3), (512,), (512, 512, 3, 3), (512,), (4096, 25088), (4096,), (4096, 4096), (4096,), (1000, 4096), (1000,)] 0\n\n\n\n\n\nHabits and Recommendations\n\nuse input size \\(LxL = 2^n\\) (e.g. 512)\nuse stride \\(S=1\\)\nuse padding \\(P= (F-1)/2\\) to retain input size –&gt; multiple CONV layers\npooling: \\(F=2 S=2\\) (size reduction: \\(LxL --&gt; L/2xL/2\\) !!! - aggressive reduction,"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#illustration",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#illustration",
    "title": "Convolutional Neural Networks",
    "section": "Illustration:",
    "text": "Illustration:\nhttps://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html"
  },
  {
    "objectID": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#summary",
    "href": "lectures/03_ConvolutionalNeuralNetworks/001_CNN.html#summary",
    "title": "Convolutional Neural Networks",
    "section": "Summary",
    "text": "Summary\n\n\n\nExample from VGGNet. Image from A. Karpathy"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html",
    "title": "Activation Functions",
    "section": "",
    "text": "observed data: \\(x\\) true label: \\(y\\) predicted label probability: \\(\\hat y = f(x, \\theta)\\) \\[\np(y|x) = \\hat{y}^y (1-\\hat{y})^y \\to \\mbox{maximize !}\n\\]\nLoss for one sample \\(x\\) \\[\nL(\\hat y, y) = - \\log p(y|x) =  - y \\log \\hat y - (1-y) \\log (1 - \\hat y) \\to \\mbox{minimize !}\n\\]\nLoss for all \\(N\\) samples \\((x_1,y_1),  (x_2,y_2) \\ldots (x_N, y_N)\\)\n\\[\nL(, x) = \\frac{1}{N} \\sum_i L(\\hat y_i, y_i)\n\\]"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#the-goal-minimize-lost",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#the-goal-minimize-lost",
    "title": "Activation Functions",
    "section": "",
    "text": "observed data: \\(x\\) true label: \\(y\\) predicted label probability: \\(\\hat y = f(x, \\theta)\\) \\[\np(y|x) = \\hat{y}^y (1-\\hat{y})^y \\to \\mbox{maximize !}\n\\]\nLoss for one sample \\(x\\) \\[\nL(\\hat y, y) = - \\log p(y|x) =  - y \\log \\hat y - (1-y) \\log (1 - \\hat y) \\to \\mbox{minimize !}\n\\]\nLoss for all \\(N\\) samples \\((x_1,y_1),  (x_2,y_2) \\ldots (x_N, y_N)\\)\n\\[\nL(, x) = \\frac{1}{N} \\sum_i L(\\hat y_i, y_i)\n\\]"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#the-strategy-iteration",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#the-strategy-iteration",
    "title": "Activation Functions",
    "section": "The strategy: iteration",
    "text": "The strategy: iteration\n\nGradient Descent\n\n\nADAM\n\n\nA single neuron\n\n\nA network\n\nmultiple logistic regressions\ninput layers, hidden layers (representations), output layer\nactivations (values that neurons path on to next layer)\nnotation: 2 layer network (without input)\nparameters: weights W and biases b - dimensions and shapes"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#the-tool-backpropagation",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#the-tool-backpropagation",
    "title": "Activation Functions",
    "section": "The tool: Backpropagation",
    "text": "The tool: Backpropagation\n\nComputation Graph\n\n\nOverview\n\nfrom a single neuron to network\nfrom numbers to matrices\nstacked layers of computation (passing information)\nNN = matrix multiplications\nNN = complex, non-linear functions\n\n\n\nGPU and vectorized implementation\n\nmatrix multiplication for one sample x = column vector [m x 1]\nmatrix multiplication for n samples x = [m x n] matrix"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#initialistion",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#initialistion",
    "title": "Activation Functions",
    "section": "Initialistion",
    "text": "Initialistion\n\ndifferent initialization can lead to different (local) optima in non-convex landscapes\navoid vanishing/exploding gradients\nXavier Initialization (for tanh): \\(Var(w) = 1/n\\)\nKaiming Initialization (for ReLU); \\(Var(w) = 2/n\\)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#activations-functions",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#activations-functions",
    "title": "Activation Functions",
    "section": "Activations Functions",
    "text": "Activations Functions\nAim: introduce non-linearity\n\\[\n\\begin{aligned}\nz_1 & = W_1 \\cdot x + b_1 \\\\\nz_2 & = W_2 \\cdot z_1 + b_2 \\\\\n& = W_2 ( W_1 \\cdot x + b_1) + b_2 \\\\\n&= W^\\prime x + b^\\prime\n\\end{aligned}\n\\]\n\n\n\n\n\n\nUse (non-linear) activation functions\n\n\n\n\nfor hidden layers for richer expressiveness\nonly exception: final layers for regression problems\n\n\n\n\n\nCode\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nx = torch.linspace(-5, 5, steps=200)\n\n\n\nsigmoid\n\\[\n\\begin{aligned}\ng(z) &= \\frac{1}{1+\\exp(-z)}\\\\\ng'(z) &= g(z) (1 - g(z))\n\\end{aligned}\n\\]\n\n\nCode\ny_sigmoid = torch.sigmoid(x)\ny_prime = y_sigmoid * ( 1 - y_sigmoid) \nplt.figure(figsize=(8, 5))\nplt.plot(x, y_sigmoid, label='Sigmoid', linewidth=2)\nplt.plot(x, y_prime, label='Derviative', linewidth=2)\nplt.title(\"Sigmoid and Derivative\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\n\n\n\nsaturation and vanishing gradients –&gt; no learning\n\n\n\n\ntanh\n\\[\n\\begin{aligned}\ng(z) &= \\tanh(z)\\\\\ng'(z) &= 1 - g(z)^2\n\\end{aligned}\n\\]\n\n\nCode\ny_tanh = torch.tanh(x)\ny_prime = 1 - y_tanh**2\nplt.figure(figsize=(8, 5))\nplt.plot(x, y_tanh, label='tanh', linewidth=2)\nplt.plot(x, y_prime, label='Derviative', linewidth=2)\nplt.title(\"tanh and Derivative\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nzero-centered\n\n\n\nReLU\n\\[\n\\begin{aligned}\ng(z)  &= \\max(0,z)\\\\\ng'(z) &= 0 \\mbox{ for } z &lt; 0 \\\\\n      &= 1 \\mbox{ for } z \\ge 0\n\\end{aligned}\n\\]\n\n\nCode\ny_relu = F.relu(x)\ny_prime = (x &gt; 0).float()\nplt.figure(figsize=(8, 5))\nplt.plot(x, y_relu, label='ReLU', linewidth=2)\nplt.plot(x, y_prime, label='Derviative', linewidth=2)\nplt.title(\"ReLU and Derivative\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSoftmax:\n\ninterprete output as prob with \\(\\sum p_i = 1\\)\nsees all neurons\nlog-sum trick\n\n\\[\n\\log\\sum_i e^{x_i} = x^\\ast + \\log \\sum_i e^{x_i - x^\\ast}\n\\]\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\nSigmoid\nTanh\nReLU\nLeakyReLU\n\n\n\n\nMathematical Range\n(0, 1)\n(-1, 1)\n[0, ∞)\n(-∞, ∞)\n\n\nAdvantages\n• Smooth, interpretable as probability  • Historically important\n• Zero-centered  • Smooth transitions around 0\n• Simple, fast computation  • Sparse activations  • No vanishing gradient for x&gt;0\n• Avoids dying ReLU problem  • Retains advantages of ReLU\n\n\nDisadvantages\n• Vanishing gradient for large |x|  • Not zero-centered  • Saturation\n• Vanishing gradient for large |x|  • More expensive than ReLU\n• Dying ReLU: neurons stuck at 0  • Unbounded outputs → exploding activations\n• Slightly more computation than ReLU  • Still unbounded above • new hyperparameter \n\n\nTypical Use Cases\n• Binary classification output\n• Recurrent networks  • Some hidden layers\n• Default choice for deep nets’ hidden layers\n• When ReLU causes dead neurons or sparse gradients\n\n\n\n\n\n\n\n\n\n\n\n\nActivation Function\nTypical Use\nAdvantages\nDisadvantages\n\n\n\n\nSigmoid\nBinary classification output (logistic regression); hidden units (rarely used now)\n- Smooth output between 0 and 1  - Interpretable as probability\n- Vanishing gradients  - Saturates at extremes  - Not zero-centered\n\n\nTanh\nHidden units in older networks; similar to sigmoid but zero-centered\n- Outputs between -1 and 1  - Zero-centered\n- Still suffers from vanishing gradients  - Saturates for large inputs\n\n\nReLU\nDefault for hidden layers\n- Computationally efficient  - Avoids vanishing gradients for positive inputs  - Sparse activations\n- “Dying ReLU” problem: neurons can output 0 permanently  - Not zero-centered\n\n\nLeaky ReLU\nHidden layers (especially when ReLU is too brittle)\n- Fixes dying ReLU by allowing small gradient when input &lt; 0\n- Slope for negative part is a hyperparameter  - Still not zero-centered\n\n\nSoftmax\nOutput layer for multi-class classification\n- Converts raw scores to probabilities  - Highlights strongest class\n- Not useful for hidden layers  - Sensitive to outliers  - Can be numerically unstable (requires log-sum-exp trick)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#data-preprocessing",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#data-preprocessing",
    "title": "Activation Functions",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nsplitting\ntransformation & normalization"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#data-augmentation",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#data-augmentation",
    "title": "Activation Functions",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\nembedd into ML workflow\nidentify symmetries (invariances): shift, rotate, scale, crop, brightness\naugment only training data\naugment all samples of the training data in same proportions"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#regualarization",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#regualarization",
    "title": "Activation Functions",
    "section": "Regualarization",
    "text": "Regualarization\n\nL2 regularization\n\\[\n\\theta_{k+1} = \\theta_k - \\epsilon \\nabla_\\theta f(\\theta_k) - \\lambda \\theta_k\n\\]\n\npenalizes large parameters\nfavors more even distribution\nimproves generalization, every neuron contributes\n\n\n\nweight decay\n\\[\n\\theta_{k+1} = (1-\\lambda) \\theta_k - \\alpha \\nabla_\\theta f(\\theta_k)\n\\]\nlooks equivalent to L2 norm (for GD), but not for Adam optimizer (momentum) ### data augmentation\n\n\nEarly stopping\n\n\nbagging and ensemble methods\n\nmassive effort for marginal gains (1-2%)\nensemble: train \\(k\\) different models and pool/average\nbagging: use \\(k\\) different data sets\nquantify uncertainty\n\n\n\nDropout\n\ndisable random set of neurons temporarily (20-50%)\nensemble of smaller network within a big one\nreduced network capacity; makes it harder to optimize/minimize loss -&gt; increase training time\nforce all neurons to act –&gt; creates robustness\nredundant representations\nhuman neurons are not reliable!\nmodel.train vs model.eval –&gt; normalize differently\nMonte-Carlo dropout also at inference (p=0.1 - 0.2) run inference multiple times to put confidence estimates on final layer and interpret softmax really probabilistically\nreliable AI: important for doctors\n\n\\[\nz = x. \\theta^T \\to E_{train}[z] = p E_{test}[z]\n\\]\n\n\nBatch Norm\n\nbring activations closer to 0\n\n\n\nOther norms\n\n\n\nWu and He, ECCV 2018"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#interpreting-loss-curves",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#interpreting-loss-curves",
    "title": "Activation Functions",
    "section": "Interpreting Loss curves",
    "text": "Interpreting Loss curves\n–&gt; Andrew NG"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#key-challenges",
    "href": "lectures/02_NeuralNetworks/03_ActivationFunctions.html#key-challenges",
    "title": "Activation Functions",
    "section": "Key Challenges",
    "text": "Key Challenges\n\nFinding Optimal Solutions\nVanishing/Exploding Gradients\nOverfitting\n\nWhen comparing model performance, compare data + data processing + optimization strategy + model"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html",
    "href": "lectures/02_NeuralNetworks/neural_networks.html",
    "title": "Neural Networks (Fragments)",
    "section": "",
    "text": "early layers learn to represent lower features (images: edges, audio: sounds, …), deeper layers learn concepts (shapes, eyes, faces or phonemes, words, sentences)\nneuroscientist inspiration\ncircuit theory: computable functions, e.g. parity \\((x_1, ... , x_n)\\) = x_1 XOR x_2 .. XOR x_n$ need \\(O(log n)\\) neurons in multiple layers, or \\(O(2^n)\\) in a single layer (exponential)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#why-deep",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#why-deep",
    "title": "Neural Networks (Fragments)",
    "section": "",
    "text": "early layers learn to represent lower features (images: edges, audio: sounds, …), deeper layers learn concepts (shapes, eyes, faces or phonemes, words, sentences)\nneuroscientist inspiration\ncircuit theory: computable functions, e.g. parity \\((x_1, ... , x_n)\\) = x_1 XOR x_2 .. XOR x_n$ need \\(O(log n)\\) neurons in multiple layers, or \\(O(2^n)\\) in a single layer (exponential)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#dimesnion-counting-convertions",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#dimesnion-counting-convertions",
    "title": "Neural Networks (Fragments)",
    "section": "Dimesnion Counting & Convertions",
    "text": "Dimesnion Counting & Convertions\n\n\n\n\n\n\n\n\nAspect\nTextbook / Theory (Samples as Columns)\nTensorFlow / PyTorch (Batch-First, transposed definition)\n\n\n\n\nActivations \\(A^{(l-1)}\\)\n\\(A^{(l-1)} \\in \\mathbb{R}^{m \\times n}\\)\n\\(A^{(l-1)} \\in \\mathbb{R}^{n \\times m}\\)\n\n\nWeights \\(W^{(l)}\\)\n\\(W^{(l)} \\in \\mathbb{R}^{k \\times m}\\)\n\\(W^{(l)} \\in \\mathbb{R}^{m \\times k}\\)\n\n\nBias \\(b^{(l)}\\)\n\\(b^{(l)} \\in \\mathbb{R}^{k \\times 1}\\)\n\\(b^{(l)} \\in \\mathbb{R}^{1 \\times k}\\) (broadcast)\n\n\nPre-activation \\(Z^{(l)}\\)\n\\(Z^{(l)} = W^{(l)} A^{(l-1)} + b^{(l)}\\)\n\\(Z^{(l)} = A^{(l-1)} W^{(l)} + b^{(l)}\\)\n\n\nOutput \\(Z^{(l)}\\)\n\\(Z^{(l)} \\in \\mathbb{R}^{k \\times n}\\)\n\\(Z^{(l)} \\in \\mathbb{R}^{n \\times k}\\)\n\n\nSample orientation\nEach column = 1 sample\nEach row = 1 sample\n\n\nTypical in\nMath, statistics, early ML literature\nTensorFlow, PyTorch, NumPy, GPU computation"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#dense-nueral-networks-mlp",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#dense-nueral-networks-mlp",
    "title": "Neural Networks (Fragments)",
    "section": "Dense Nueral Networks (MLP)",
    "text": "Dense Nueral Networks (MLP)\nGeneric equation for forward propagation @ layer \\(l\\):\n\\(x=a^0\\) (input layer), \\(\\hat y = a^L\\) (output layer)\n\nForward: a^{l-1} a^l$\n\\[\n\\begin{aligned}\nz^l &= W^l a^{l-1} + b^l \\\\\na^l &= g(z^l)\n\\end{aligned}\n\\] - cache \\(z^l\\)\n\n\nBackward: da^l a^{l-1}$\n\\[\n\\begin{aligned}\ndz^{l} &= da^{l} * g'(z^l)\\\\\ndW^l &= dz^l {a^{l-1}}^T \\\\\ndb^l &= \\sum dz^l\\\\\nda^{l-1} &= {W^{l}}^T dz^l\n\\end{aligned}\n\\]\n\ncache: dW^l, db^l\n\n\\(\\boxed{\\,dZ^{[l]} \\;=\\; dA^{[l]} \\;\\odot\\; g^{[l]\\,'}(Z^{[l]})\\,} \\qquad (n_l\\times m)\\)\n\\[\n\\begin{aligned} D^{[l]} &= \\frac{\\partial J}{\\partial Z^{[l]}} = \\frac{\\partial J}{\\partial A^{[l]}} \\odot g^{[l]\\,'}(Z^{[l]}) \\quad & (n_l \\times m) \\\\ dW^{[l]} &= \\frac{\\partial J}{\\partial W^{[l]}} = \\frac{1}{m}\\, D^{[l]} (A^{[l-1]})^\\top \\quad & (n_l \\times n_{l-1}) \\\\ db^{[l]} &= \\frac{\\partial J}{\\partial b^{[l]}} = \\frac{1}{m}\\, D^{[l]} \\mathbf{1}_m \\quad & (n_l \\times 1) \\\\ dA^{[l-1]} &= \\frac{\\partial J}{\\partial A^{[l-1]}} = (W^{[l]})^\\top D^{[l]} \\quad & (n_{l-1} \\times m) \\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#a-short-history-of-nn",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#a-short-history-of-nn",
    "title": "Neural Networks (Fragments)",
    "section": "A short history of NN",
    "text": "A short history of NN\n!!!! Double-check !!!!\n\n\n\nYear\nTask Type\nTool / Model\nCompany / Author(s)\nArchitecture / Highlights\nPublication / Link\nCitations (approx.)\nParameters\n\n\n\n\n1997\nSequence Modeling (RNN)\nLSTM\nHochreiter & Schmidhuber\nGated RNN with Constant Error Carousel\nNeural Computation (1997)\n130,000\n?\n\n\n2013\nGenerative Modeling\nVAE\nKingma & Welling\nProbabilistic autoencoder with latent space\narXiv (2013)\n47,000\n?\n\n\n2014\nImage Classification\nVGG-16\nSimonyan & Zisserman (Oxford)\nDeep ConvNet (16 layers)\narXiv (2014)\n—\n138M\n\n\n2015\nImage Classification\nResNet-50\nHe et al. (Microsoft)\nResidual CNN (skip connections)\narXiv (2015)\n280,000\n?\n\n\n2017\nText-to-Speech (TTS)\nTacotron 2\nGoogle\nSeq2Seq + attention + vocoder\nTacotron 2 (2017)\n2,500\n?\n\n\n2019\nText Classification\nBERT\nGoogle\nBidirectional Transformer (encoder)\nACL (2019)\n141,000\n110M / 340M\n\n\n2020\nText Generation\nGPT-3\nOpenAI\nDecoder-only Transformer (autoregressive)\narXiv (2020)\n51,000\n175B\n\n\n2020\nTranslation (Text2Text)\nT5\nGoogle\nEncoder–Decoder Transformer\nT5 (2020)\n10,000\n220M\n\n\n2020\nImage Classification\nViT\nGoogle\nTransformer over patch embeddings\nViT (2020)\n6,000\n86M\n\n\n2020\nObject Detection\nDETR\nMeta (Facebook)\nTransformer + CNN backbone\nDETR (2020)\n2,000\n?\n\n\n2021\nImage Generation\nDALL·E\nOpenAI\nTransformer + discrete VAE + CLIP\nOpenAI (2021)\n—\n12B\n\n\n2021\nImage Segmentation\nSegFormer\nNvidia\nTransformer-based segmentation architecture\nSegFormer (2021)\n500\n?\n\n\n2022\nSpeech-to-Text (ASR)\nWhisper\nOpenAI\nTransformer-based seq2seq with CTC\nWhisper (2022)\n5,700\n?\n\n\n2022\nImage-to-Text (Caption)\nBLIP\nSalesforce\nVision–language encoder–decoder\nBLIP (2022)\n300\n?\n\n\n2022\nVideo Generation\nImagen Video\nGoogle Research\nCascade of video diffusion models with super-resolution\nImagen Video (2022)\n—\n?\n\n\n2023\nVideo Generation\nVideoPoet\nGoogle Research\nDecoder-only autoregressive Transformer (multimodal)\nVideoPoet (2023)\n—\n?\n\n\n2024\nVideo Generation\nVeo 2\nGoogle DeepMind\n4K, cinematographic control, improved physics\nVeo 2 (2024)\n—\n?\n\n\n2025\nVideo Generation\nVeo 3\nGoogle DeepMind\nAdds synchronized audio (dialogue, SFX, ambience)\nVeo 3 (2025)\n—\n?\n\n\n\n\nTrends\n\nmore parameters\nacademic \\(\\to\\) industry\nopen \\(\\to\\) closed\npapers \\(\\to\\) money\nmulti-modal"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#loss-function",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#loss-function",
    "title": "Neural Networks (Fragments)",
    "section": "Loss function",
    "text": "Loss function\n\nthe objective of neural network"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#initialistion",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#initialistion",
    "title": "Neural Networks (Fragments)",
    "section": "Initialistion",
    "text": "Initialistion\n\ndifferent initialization can lead to different (local) optima in non-convex landscapes\navoid vanishing/exploding gradients\nXavier Initialization (for tanh): \\(Var(w) = 1/n\\)\nKaiming Initialization (for ReLU); \\(Var(w) = 2/n\\)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#activations-functions",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#activations-functions",
    "title": "Neural Networks (Fragments)",
    "section": "Activations Functions",
    "text": "Activations Functions\nAim: introduce non-linearity Problem: saturation and vanishing gradients –&gt; no learning ### tanh\n\nzero-centered\n\n\nReLU\n\n\nSoftmax:\n- interprete output as prob with $\\sum p_i = 1$\n- sees all neurons\n- log-sum trick\n\n$$\n\\log\\sum_i e^{x_i} = x^\\ast + \\log \\sum_i e^{x_i - x^\\ast}\n$$\n\n\nSummary\n\n\n\n\n\n\n\n\n\nActivation Function\nTypical Use\nAdvantages\nDisadvantages\n\n\n\n\nSigmoid\nBinary classification output (logistic regression); hidden units (rarely used now)\n- Smooth output between 0 and 1  - Interpretable as probability\n- Vanishing gradients  - Saturates at extremes  - Not zero-centered\n\n\nTanh\nHidden units in older networks; similar to sigmoid but zero-centered\n- Outputs between -1 and 1  - Zero-centered\n- Still suffers from vanishing gradients  - Saturates for large inputs\n\n\nReLU\nDefault for hidden layers\n- Computationally efficient  - Avoids vanishing gradients for positive inputs  - Sparse activations\n- “Dying ReLU” problem: neurons can output 0 permanently  - Not zero-centered\n\n\nLeaky ReLU\nHidden layers (especially when ReLU is too brittle)\n- Fixes dying ReLU by allowing small gradient when input &lt; 0\n- Slope for negative part is a hyperparameter  - Still not zero-centered\n\n\nSoftmax\nOutput layer for multi-class classification\n- Converts raw scores to probabilities  - Highlights strongest class\n- Not useful for hidden layers  - Sensitive to outliers  - Can be numerically unstable (requires log-sum-exp trick)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#data-preprocessing",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#data-preprocessing",
    "title": "Neural Networks (Fragments)",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nsplitting\ntransformation & normalization"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#data-augmentation",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#data-augmentation",
    "title": "Neural Networks (Fragments)",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\nembedd into ML workflow\nidentify symmetries (invariances): shift, rotate, scale, crop, brightness\naugment only training data\naugment all samples of the training data in same proportions"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#regualarization",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#regualarization",
    "title": "Neural Networks (Fragments)",
    "section": "Regualarization",
    "text": "Regualarization\n\nL2 regularization\n\\[\n\\theta_{k+1} = \\theta_k - \\epsilon \\nabla_\\theta f(\\theta_k) - \\lambda \\theta_k\n\\]\n\npenalizes large parameters\nfavors more even distribution\nimproves generalization, every neuron contributes\n\n\n\nweight decay\n\\[\n\\theta_{k+1} = (1-\\lambda) \\theta_k - \\alpha \\nabla_\\theta f(\\theta_k)\n\\]\nlooks equivalent to L2 norm (for GD), but not for Adam optimizer (momentum) ### data augmentation\n\n\nEarly stopping\n\n\nbagging and ensemble methods\n\nmassive effort for marginal gains (1-2%)\nensemble: train \\(k\\) different models and pool/average\nbagging: use \\(k\\) different data sets\nquantify uncertainty\n\n\n\nDropout\n\ndisable random set of neurons temporarily (20-50%)\nensemble of smaller network within a big one\nreduced network capacity; makes it harder to optimize/minimize loss -&gt; increase training time\nforce all neurons to act –&gt; creates robustness\nredundant representations\nhuman neurons are not reliable!\nmodel.train vs model.eval –&gt; normalize differently\nMonte-Carlo dropout also at inference (p=0.1 - 0.2) run inference multiple times to put confidence estimates on final layer and interpret softmax really probabilistically\nreliable AI: important for doctors\n\n\\[\nz = x. \\theta^T \\to E_{train}[z] = p E_{test}[z]\n\\]\n\n\nBatch Norm\n\nbring activations closer to 0\nc.f normalizing input layer (feature normalization)\nfrom the perspective of a deep layer it’s inputs a changing all the time (while learning) - keep their mean and variance stable (“covariate shift”) –&gt; speeds up learning\nside effect: batch norm also adds a little noise while estimating \\(\\mu, \\sigma^2\\) from mini-batch -&gt; regualrization (c.f dropout)\na test time: which \\(\\mu\\), \\(\\sigma\\) to use? exp. weighted (running) average across minimataches Given \\(z [n_l, m]\\) for some layer \\(l\\) and batch size \\(m\\) \\[\n\\begin{aligned}\n\\mu_i &= \\frac{1}{m} \\sum_j z_{ij} \\\\\n\\sigma_i^2 &= \\frac{1}{m} \\sum_j (\\mu - z_{ij})^2 \\\\\nz^{norm}_{ij} &= (z_{ij} - \\mu_i) / \\sqrt{\\sigma^2_i + \\epsilon} \\\\\n\\tilde{z}_{ij} &= \\gamma z^{norm}_{ij} + \\beta\n\\end{aligned}\n\\]\n\n\n\nOther norms\n\n\n\nWu and He, ECCV 2018"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#interpreting-loss-curves",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#interpreting-loss-curves",
    "title": "Neural Networks (Fragments)",
    "section": "Interpreting Loss curves",
    "text": "Interpreting Loss curves\n–&gt; Andrew NG"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#key-challenges",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#key-challenges",
    "title": "Neural Networks (Fragments)",
    "section": "Key Challenges",
    "text": "Key Challenges\n\nFinding Optimal Solutions\nVanishing/Exploding Gradients\nOverfitting\n\nWhen comparing model performance, compare data + data processing + optimization strategy + model"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/neural_networks.html#increase-in-compute-for-training",
    "href": "lectures/02_NeuralNetworks/neural_networks.html#increase-in-compute-for-training",
    "title": "Neural Networks (Fragments)",
    "section": "Increase in compute for training",
    "text": "Increase in compute for training"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html",
    "title": "Image Classification",
    "section": "",
    "text": "Given many images and their labels, train a neural network to predict the label of a new image (c.f human learning)."
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#goal",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#goal",
    "title": "Image Classification",
    "section": "",
    "text": "Given many images and their labels, train a neural network to predict the label of a new image (c.f human learning)."
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#repetitions",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#repetitions",
    "title": "Image Classification",
    "section": "Repetitions",
    "text": "Repetitions\n\nDefine Model & Optimization Strategy\nFit Model\nMonitor Fitting\nEvaluate Training"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#new-items",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#new-items",
    "title": "Image Classification",
    "section": "New items",
    "text": "New items\n\nData Splitting: Train & Test\nHow to handle images: data structure"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#get-packages",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#get-packages",
    "title": "Image Classification",
    "section": "Get Packages",
    "text": "Get Packages\n\n\nCode\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split, Subset\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\n\nfrom lecture_utils.helper import plot_cm, detect_device\n\ndevice = detect_device()\n\nprint('torch-version: ', torch.__version__)\nprint('device: ', device)\n\n\ntorch-version:  2.7.1\ndevice:  mps\n\n\n\n\n\n\n\n\nUse GPU if available\n\n\n\n\nGoogle Colab: \\(\\to\\) Change Runtime …\nKaggle: \\(\\to\\) Accelerator (GPU T4)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#get-data-mnist",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#get-data-mnist",
    "title": "Image Classification",
    "section": "Get Data: MNIST",
    "text": "Get Data: MNIST\n\nBackground\nMany famous datasets can be found here:\n\n@pytorch datasets\n@keras datasets\n\nIn the following we will focus on squared images of handwritten digits. They have also been annotated (labeled). For historical background see here: wikipedia\n\n\n\nExamples from MNIST handwritten digits.\n\n\n\nthe challenge\nhighly structured data\ndata collection\nhuman error rate\nLeCun (Bell Labs, Facebook/Meta, Turing Award 2018)\n\n\n\nTest and Training Sets\nFor models with many parameters there is a real danger of overfitting, i.e. learning the specifics of one set of samples rather than generalizable rules.\nFor performance evaluation it is crucial to retain an independent (but representative) test data set that it is never used for fitting. For MNIST we can obtain both train and test data from torchvision.dataset (63 MB)\n\n\nCode\n# define data transformation. toTensor() also includes division by 255!  \ntransform = transforms.Compose([transforms.ToTensor()]) \nfull_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_data  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n\n\n\nData Transformations\nAlert: Usually many more steps are necessary to prepare data for analysis: reading, reformating, filtering, shuffeling, transformation, normalization.\nThis can take up a significant amount of time and it is important to do so consistently for train and test data.\nThe transformation to pytorch tensor is minimal, but notice that this also includes an implicit normalization of images \\([0, 255]\\) (integer) \\(\\to [0,1]\\) (float). This will be done automatically and systematically whenever we access full_train or test_data, but the data set still contains the row data.\n\n\nData Inspection\n\n\nCode\ndef show_ascii_arr(arr):\n  for row in arr:\n      print(\" \".join(f\"{val:3d}\" for val in row))\n\n# accessing all *unnormalized* data\nX,y = full_train.data, full_train.targets\nprint('full_train:', type(full_train))\nprint('X:', type(X), X.shape, X.min(), X.max())\nprint('y:', type(y), y.shape)\n\n# ... as ascii\nshow_ascii_arr(X[0])\n\n# ... as grey-scale image\nplt.imshow(X[0], cmap=\"grey\")\nplt.title(y[0])\nplt.show()\n\n\nfull_train: &lt;class 'torchvision.datasets.mnist.MNIST'&gt;\nX: &lt;class 'torch.Tensor'&gt; torch.Size([60000, 28, 28]) tensor(0, dtype=torch.uint8) tensor(255, dtype=torch.uint8)\ny: &lt;class 'torch.Tensor'&gt; torch.Size([60000])\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255 247 127   0   0   0   0\n  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0\n  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82  82  56  39   0   0   0   0   0\n  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253 253 207   2   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201  78   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many samples and how many features does the MNIST training set have?\n\n\nNotice: usually we access only subsets (by index or DataLoader) those will be transformed as specified in transform()\n\n\nCode\nX,y = full_train[0]\n\nplt.imshow(X[0], cmap=\"grey\")\ntitle=f\"label={y} (min={X.min()}, max={X.max()})\"\nplt.title(title)\nplt.show()\n\n\n\n\n\n\n\n\n\nAbove we obtained train and test directly with torchvision.datasets.\nBut sometimes we need addtional subsets from the train data (e.g. validation data) This can be done as follows\n\n\nCode\nfrom torch.utils.data import random_split\n\n# keep 80% for training\nfract = 0.8 \n\n# define lengths\ntrain_len = int(fract * len(full_train))\nval_len = len(full_train) - train_len\n\n# define split\ntrain_ds, val_ds = random_split(full_train, [train_len, val_len])\n\n# Inspect\nprint('train:', len(train_ds))\nprint('val:', len(val_ds))\nX, y = val_ds[0]\nprint('X:', X.shape, y)\n\n\ntrain: 48000\nval: 12000\nX: torch.Size([1, 28, 28]) 7\n\n\n\n\nStratified sampling\nA slightly more complicated scenario may occur if the classes (encoded by targets) are not well balanced.\nIn this case we need to ensure that the subsampled data respects those proportions, since the validation (and test) data should be representative This is called “stratification” and can be obtained with help of scikit-learn.\nSee here for stratified partitioning: StratifiedShuffleSplit()\n\n\nCode\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom torch.utils.data import Subset\n\n# Use targets directly from dataset\ntargets = full_train.targets.numpy()\nsss = StratifiedShuffleSplit(n_splits=1, train_size=fract, random_state=42)\n\n# sss.split() returns an iterator\ntrain_idx, val_idx = next(sss.split(np.zeros(len(targets)), targets))\n\ntrain_ds = Subset(full_train, train_idx)\nval_ds   = Subset(full_train, val_idx)\nprint('train:', len(train_ds))\nprint('val:', len(val_ds))\n\n# Sanity check - label frequencies\ntrain_labels = train_ds.dataset.targets[train_ds.indices]\ntrain_counts = torch.bincount(train_labels)\ntrain_freq = train_counts / train_counts.sum()\n\nval_labels = val_ds.dataset.targets[val_ds.indices]\nval_counts = torch.bincount(val_labels)\nval_freq = val_counts / val_counts.sum()\n\nprint(train_freq)\nprint(val_freq)\n\n\ntrain: 48000\nval: 12000\ntensor([0.0987, 0.1124, 0.0993, 0.1022, 0.0974, 0.0904, 0.0986, 0.1044, 0.0975,\n        0.0991])\ntensor([0.0988, 0.1123, 0.0993, 0.1022, 0.0973, 0.0903, 0.0987, 0.1044, 0.0975,\n        0.0992])\n\n\n\n\ntrain_test_split alternative\nBelow is a frequently used method from scikit-learn. It is shown here only for reference. But notice that it requires several conversions and normalizations that we’ll need track carefully.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset\nfract = 0.8\n\n# convert from tensors to numpy\nX = full_train.data.numpy()\ny = full_train.targets.numpy()\n\n# convenient implementation of stratification\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=fract, stratify=y)\n\n# convert back to tensors\ntrain_ds = TensorDataset(torch.tensor(X_train[:, None]/255., dtype=torch.float32),\n                                          torch.tensor(y_train, dtype=torch.long))\nval_ds   = TensorDataset(torch.tensor(X_val[:, None]/255., dtype=torch.float32),\n                                          torch.tensor(y_val, dtype=torch.long))\ntest_ds  = TensorDataset(test_data.data[:, None]/255., test_data.targets)\n\n\n\n\nSummary\nIn most machine learinng application we want to have a data split\n\n\n\nTrain-Validation-Test Split. The fractions shown are common, but arbitrary.\n\n\n\n\nGoal Repeat\nBuild a predictor of labels for hand-written digits.\n\n\nCode\nfrom sklearn.decomposition import PCA\n\nn_sub = 500 # define a subset of digits for speed\n\n# flattening a tensor to get [samples x features]\nX_sub = full_train.data[:n_sub].flatten(start_dim=1)\ny_sub = full_train.targets[:n_sub]\nprint('X: ', X_sub.shape)\n\nX_pca = PCA(n_components = 2).fit_transform(X_sub)\nprint('Scores: ',X_pca.shape)\n\ncm = plt.get_cmap('tab10')\nplt.scatter( X_pca[:,0], X_pca[:,1] , c=y_sub, cmap=cm)\nplt.title('PCA of MNIST')\nplt.colorbar()\nplt.show()\n\n\nX:  torch.Size([500, 784])\nScores:  (500, 2)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#data-loading",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#data-loading",
    "title": "Image Classification",
    "section": "Data Loading",
    "text": "Data Loading\nRather than accessing samples per index we frequently want to load batches of a certain sizes into memory for further analysis. Really large data sets may not even fit into memory, so it is useful to define DataLoaders that get data only in batches inot memory.\n\n\nCode\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader   = DataLoader(val_ds, batch_size=64)\ntest_loader  = DataLoader(test_data, batch_size=64)\n\nprint(\"number of train_batches: \", len(train_loader))\nprint(\"number of test_batches:  \", len(test_loader))\n\n# Data Loaders are used a iterator - they return batches of X,y tuples\nX,y = next(iter(train_loader))\nprint('X batch: ', X.shape)\nprint('y batch: ', y.shape)\n\n\nnumber of train_batches:  750\nnumber of test_batches:   157\nX batch:  torch.Size([64, 1, 28, 28])\ny batch:  torch.Size([64])"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#define-model",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#define-model",
    "title": "Image Classification",
    "section": "Define Model",
    "text": "Define Model\n\nRectified Linear Unit\nBasic Non-linearity\n\n\nCode\nclass MNISTModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(28*28, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = F.relu(x)\n        logits = self.fc2(x)\n        return logits\n\n\n\n\nCode\nmodel = MNISTModel().to(device)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#summarize-model",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#summarize-model",
    "title": "Image Classification",
    "section": "Summarize Model",
    "text": "Summarize Model\n\n\nCode\nfrom torchinfo import summary\n\nsummary(model, input_size=(1000, 1, 28, 28), device=device)\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMNISTModel                               [1000, 10]                --\n├─Flatten: 1-1                           [1000, 784]               --\n├─Linear: 1-2                            [1000, 128]               100,480\n├─Linear: 1-3                            [1000, 10]                1,290\n==========================================================================================\nTotal params: 101,770\nTrainable params: 101,770\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 101.77\n==========================================================================================\nInput size (MB): 3.14\nForward/backward pass size (MB): 1.10\nParams size (MB): 0.41\nEstimated Total Size (MB): 4.65\n=========================================================================================="
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#define-loss-function-and-optimizer",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#define-loss-function-and-optimizer",
    "title": "Image Classification",
    "section": "Define Loss Function and Optimizer",
    "text": "Define Loss Function and Optimizer\n\nAdam Optimizer\nReference: DP Kingma et al. 2014. Adam: A method for Stochasitic Optimization. 200k+ citations !!!\nDon’t get stuck in local minima \\(\\to\\) adaptive learning rates\n\n\nCode\n# return sum losses over batch\nloss_function = nn.CrossEntropyLoss(reduction=\"sum\") \noptimizer = torch.optim.Adam(model.parameters())"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#train-model",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#train-model",
    "title": "Image Classification",
    "section": "Train Model",
    "text": "Train Model\n\n\nCode\ndef train_model(model, train_loader, val_loader, epochs=25):\n    history = {'loss': [], 'val_loss': []}\n    for epoch in range(epochs):\n        # training\n        model.train()\n        total_loss = 0\n        # loop over all train data (in batches given by train_loader)\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            logits = model(xb)\n            loss = loss_function(logits, yb) # total loss for batch\n\n            optimizer.zero_grad()            # set gradients to 0\n            loss.backward()                  # calculate gradients with backprop\n            optimizer.step()                 # parameter update\n\n            total_loss += loss.item()        # cumulative loss over all batches\n            \n        # average train_loss per sample\n        train_loss = total_loss / len(train_loader.dataset)\n        \n        # Validation\n        model.eval()\n        total_loss = 0\n        # switch off gradient calculation\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits = model(xb)\n                loss = loss_function(logits, yb)\n                total_loss += loss.item() \n        val_loss = total_loss / len(val_loader.dataset)\n        \n        history['loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        \n        print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n    return history\n\nhist = train_model(model, train_loader, val_loader, epochs=20)\n\n\nEpoch 1: train_loss=0.3853, val_loss=0.2311\nEpoch 2: train_loss=0.1840, val_loss=0.1639\nEpoch 3: train_loss=0.1263, val_loss=0.1325\nEpoch 4: train_loss=0.0968, val_loss=0.1099\nEpoch 5: train_loss=0.0760, val_loss=0.1035\nEpoch 6: train_loss=0.0619, val_loss=0.0976\nEpoch 7: train_loss=0.0511, val_loss=0.0917\nEpoch 8: train_loss=0.0423, val_loss=0.0918\nEpoch 9: train_loss=0.0338, val_loss=0.0896\nEpoch 10: train_loss=0.0277, val_loss=0.0880\nEpoch 11: train_loss=0.0239, val_loss=0.0869\nEpoch 12: train_loss=0.0188, val_loss=0.0955\nEpoch 13: train_loss=0.0152, val_loss=0.0916\nEpoch 14: train_loss=0.0136, val_loss=0.0939\nEpoch 15: train_loss=0.0116, val_loss=0.0967\nEpoch 16: train_loss=0.0091, val_loss=0.0949\nEpoch 17: train_loss=0.0084, val_loss=0.1144\nEpoch 18: train_loss=0.0075, val_loss=0.0992\nEpoch 19: train_loss=0.0057, val_loss=0.1231\nEpoch 20: train_loss=0.0060, val_loss=0.1042"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#save-model",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#save-model",
    "title": "Image Classification",
    "section": "Save Model",
    "text": "Save Model\n\n\nCode\noutdir = \"output\"\nos.makedirs(outdir, exist_ok=True)\n\ntorch.save(model.state_dict(), f'{outdir}/mnist_model.pt')\ntorch.save(hist, f\"{outdir}/mnist_history.pt\")"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#load-model",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#load-model",
    "title": "Image Classification",
    "section": "Load Model",
    "text": "Load Model\n\n\nCode\noutdir = \"output\"\nmodel = MNISTModel().to(device)\ntdict = torch.load(f\"{outdir}/mnist_model.pt\", map_location=device)\nmodel.load_state_dict(tdict)\nhist = torch.load(f\"{outdir}/mnist_history.pt\")"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#evaluate-training",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#evaluate-training",
    "title": "Image Classification",
    "section": "Evaluate Training",
    "text": "Evaluate Training\n\n\nCode\ndef plot_fit_history(hist, name='loss', test_score=None):\n    if name not in hist:\n        print(f\"{name} not found in history\")\n        return\n    plt.plot(hist[name], label='train')\n    val_name = 'val_' + name\n    if val_name in hist:\n        plt.plot(hist[val_name], label='valid')\n    if test_score is not None:\n        plt.axhline(test_score, color='green', linestyle='-.', label='test')\n    plt.xlabel('Epoch')\n    plt.ylabel(name)\n    plt.legend()\n    plt.show()\n\nplot_fit_history(hist, 'loss')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepeat\n\n\n\nIn principle the steps can be repeated to improve the model (decrease the training loss) or to speedup the training.\n\nincreasing number of neurons or layers\nwork with regularization techniques or data augmentation\nchange learning algorithm or learning rate\nadjust hyperparameters\n\n! But never show the model the test data until the end !"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#test-model",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#test-model",
    "title": "Image Classification",
    "section": "Test Model",
    "text": "Test Model\nIn principle we can test all test samples (10,000) at once. This is conceptually simpler, but it does requires more care with proper transformations (normalizations) which are run when accessing batches. So in this case we have to normalize by hand\n\n\nCode\n# notice that we need to normalize explicitly\nX = test_data.data.to(device) / 255.0\ny = test_data.targets.to(device)\nprint(X.shape)\nwith torch.no_grad():\n  all_logits = model(X)\n  loss = loss_function(all_logits, y)\n\nall_true = y.detach().cpu()\ntest_loss = loss.cpu() / len(X)\nplot_fit_history(hist, 'loss', test_loss)\n\n\ntorch.Size([10000, 28, 28])\n\n\n\n\n\n\n\n\n\n… more commonly batches are also used over test samples, but this requires some collection\n\n\nCode\n# Evaluate on test\nmodel.eval()\ntotal_loss = 0\nall_logits = []\nall_true = []\nwith torch.no_grad():\n    for xb, yb in test_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        logits = model(xb)\n        loss = loss_function(logits, yb)\n\n        # collect total_loss, predicted logits and true labels yb\n        total_loss += loss.item()\n        all_logits.append(logits.cpu())\n        all_true.append(yb.cpu())\n\ntest_loss = total_loss / len(test_loader.dataset) # mean test loss\nall_logits = torch.cat(all_logits)\nall_true = torch.cat(all_true)\n\nplot_fit_history(hist, 'loss', test_loss)\n\n\n\n\nCode\nall_preds = all_logits.argmax(dim=1).detach().cpu()\ncm = confusion_matrix(all_true.numpy(), all_preds.numpy())\n#print(cm)\nplot_cm(cm)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#task-for-home",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#task-for-home",
    "title": "Image Classification",
    "section": "Task for home",
    "text": "Task for home\n\nRepeat the above, but with the ReLU switch off. Tru to increase the number of nodes or layers\nReality check: scan your own handwritten digit and submit it to your trained model. Pay attention to proper normalization and black/white encoding. Does it work?"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#load-own-image",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#load-own-image",
    "title": "Image Classification",
    "section": "Load own image",
    "text": "Load own image\n\n\nCode\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import TensorDataset\n\nimg_size = test_data.data[0].shape\njpg_path = \"images/ANN_Digit3.jpg\"\n\n# convert to greyscale (L=luminance)\nimg = Image.open(jpg_path).convert(\"L\").resize(img_size)\n\nimg_tensor = transform(img) # transform to tensor\nimg_tensor = 1 - img_tensor # swap black and white\nimg_tensor = img_tensor.unsqueeze(0)\n\nprint(img_tensor.shape)\nplt.imshow(img_tensor[0][0], cmap=\"grey\")\nplt.show()\n\n\ntorch.Size([1, 1, 28, 28])\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport torch.nn.functional as F\n\n# predict logits\nlogits = model(img_tensor.to(device))\nprint('logits = ', logits.detach().cpu().numpy())\n\n# convert to probabilities\nprobs = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n\nplt.figure(figsize=(8,4))\nplt.bar(range(10), probs)\nplt.xlabel(\"Digit\")\nplt.ylabel(\"Probability\")\nplt.xticks(range(10))\nplt.title(\"Model probabilities for each digit\")\nplt.show()\n\n\nlogits =  [[-9.476369  -2.5490894 -2.6994314  3.277071  -3.6264172 -0.784245\n  -6.6809206 -6.64236    0.3123123 -1.4218341]]"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/05_ImageClassification.html#summary-1",
    "href": "lectures/02_NeuralNetworks/05_ImageClassification.html#summary-1",
    "title": "Image Classification",
    "section": "Summary",
    "text": "Summary\n\nnew data: images as flattened feature vectors\nhighly structured data (unrealistic)\nConsistent use of Data Transformation & Data Loading\nTrain-Validation-Test split (with balancing)\nImportance of non-linearity (\\(\\to\\) RELU)\nrepetiton: model definition and parameter count"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html",
    "title": "Perceptron",
    "section": "",
    "text": "A neuron cell receives electrical signals (dendrites) and passes it to other neurons (axon). Source: wikipedia"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#real-neurons",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#real-neurons",
    "title": "Perceptron",
    "section": "",
    "text": "A neuron cell receives electrical signals (dendrites) and passes it to other neurons (axon). Source: wikipedia"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#neuron-models",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#neuron-models",
    "title": "Perceptron",
    "section": "Neuron Models",
    "text": "Neuron Models\n\n\n\nMcCulloch and Pitts (1943)\n\n\n\n\n\n\n\n\nA brief history of neuron++ models\n\n\n\n\nMcCulloch and Pitts (1943): Neurons as Boolean Gates (synaptic weights and activation thresholds)\nHebbs (1949): Neurons can learn weights; “fire together & wire together”\nRosenblatt (1957): A mathematical learning rule + a machine"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#an-application-binary-classification",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#an-application-binary-classification",
    "title": "Perceptron",
    "section": "An Application: Binary Classification",
    "text": "An Application: Binary Classification\n\n\n\nMark I Perceptron (1958). Distinguish pairs of letters (20x20 pixels) - with 80% accuracy!"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#perceptions-and-citations",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#perceptions-and-citations",
    "title": "Perceptron",
    "section": "Perceptions and Citations",
    "text": "Perceptions and Citations\n– Mechanisation of Thought Processes –\nRosenblatt (1957): “Devices of this sort are expected ultimately to be capable of concept formation, language translation, collation of military intelligence, and the solution of problems through inductive logic.”\n\n\n\nNew York Times, July 7 1958"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#boolean-gates-ai-winter",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#boolean-gates-ai-winter",
    "title": "Perceptron",
    "section": "Boolean Gates & AI Winter",
    "text": "Boolean Gates & AI Winter\nHow to adjust weights and thresholds to calculate Boolean functions?\n\n\n\nBoolean Gates with weights and thresholds. \\(x_1, x_2, y \\in \\{0,1\\}\\)."
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#an-algorithm",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#an-algorithm",
    "title": "Perceptron",
    "section": "An Algorithm",
    "text": "An Algorithm\nGoal given a Boolean truth table \\((x,y) \\to\\) find weights \\(w_l\\) and thresholds \\(b_l\\)\n\n\n\nPerceptron Calculation. A linear collection function + a non-linear activation function.\n\n\n\n\n\n\n\n\n\nLearning = updating weights\n\n\n\n\\[\nw_{l} \\to w_{l} + \\alpha (y - \\hat y)*x_l\n\\]\n\nall Boolean \\(x_l, y, \\hat y \\in \\{0, 1\\}\\)\nmodified Hebbs learning: weights are updated only if target \\(y\\) is not yet reached: \\(y \\ne \\hat y\\)\nonline learning: one sample at a time\nBUT: solution only for linearly separated data (not for XOR)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#classification",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#classification",
    "title": "Perceptron",
    "section": "Classification",
    "text": "Classification\nFor continuous \\(x\\), each neuron in a perceptron defines a (linear) decision boundary based on the value of \\(z\\).\n\\[\nz = w_1 x_1 + w_2 x_2 + b\n\\]\n\n\n\nDecision Boundary for continous \\(x\\)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#universality-with-multiple-layers",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#universality-with-multiple-layers",
    "title": "Perceptron",
    "section": "Universality with Multiple Layers",
    "text": "Universality with Multiple Layers\nWith a multilayer perceptron (MLP) it is possible to calculate XOR, or any Boolean function\n\n\n\nMulti-Layer-Perceptron to calculate XOR. Source: wikipedia\n\n\n\n\n\n\n\n\nBUT\n\n\n\n\nsimple perceptron algorithm only works for single layer and linearly separaberable data.\nneed new algorithm: Backpropagation (Rumelhart, Hinton, Williams; Nature 1986)"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#complex-classification",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#complex-classification",
    "title": "Perceptron",
    "section": "Complex Classification",
    "text": "Complex Classification\nBelow I have hand-selected weights and thresholds for an MLP (with 3 hidden nodes) to create 3 decision boundaries that form a triangle.\nBased on this I have sampled points \\((x_1, x_2)\\) and assigned 2 labels (0=outside triangle, 1=inside triangle)\nLet’s explore if we can learn the known boundaries from just the data \\((x,y)\\).\nWe will discuss the algorithm later, but here I just want to illustrate that it seems to work.\n\n\nCode\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# some convenience functions\nfrom lecture_utils.helper import detect_device\nfrom lecture_utils.perceptron_utils import  plot_data_and_boundaries, plot_decision_boundary, get_data, train_model\n\n# define 3 decision boundaries with W and b\nW = np.array([[2,-1],[-2,-1],[0,1]])\nb = np.array([1,1,1]).reshape(1,-1)\n\n# sample data and assign labels based on known boundaries\nns = 10000\nX, labels = get_data(W, b, n_samples = ns)\n\n# plot data and boundaries\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\ntitle_str = f'True Boundaries and {ns} data points'\nplot_data_and_boundaries(ax, X.cpu(), labels.cpu(), W, b, title=title_str)\nplt.show()\n\n\n\n\n\nboundary parameters (w1, w2, b) specify boundary equation: w1 x1 + w2 x2 + b = 0\n\n\n\n\n\n\nCode\n# a simple model with minimal number of layers (see data generation)\nclass MinimalModel(nn.Module):\n    def __init__(self):\n        super(MinimalModel, self).__init__()\n\n        self.hidden_layer = nn.Linear(2, 3)\n        self.output_layer = nn.Linear(3, 1)\n\n    def forward(self, x):\n        x = torch.sigmoid(self.hidden_layer(x)) # sigmoid just for positivity\n        x = torch.sigmoid(self.output_layer(x)) # Sigmoid activation for classification\n        return x\n\n# a complex model with 2 hidden layers\nclass ComplexModel(nn.Module):\n    def __init__(self):\n        super(ComplexModel, self).__init__()\n\n        self.hidden1 = nn.Linear(2, 16)\n        self.hidden2 = nn.Linear(16,32)\n        self.output_layer = nn.Linear(32, 1)\n\n    def forward(self, x):\n        x = torch.sigmoid(self.hidden1(x))\n        x = torch.sigmoid(self.hidden2(x))\n        x = torch.sigmoid(self.output_layer(x)) # Sigmoid activation for classification\n        return x\n\n\n\n\nCode\ndevice = detect_device()\nn_epochs = 9000\nmodel = MinimalModel().to(device)\nX = X.to(device)\nlabels = labels.to(device)\nminimal_model, minimal_loss = train_model(model, X, labels, n_epochs=n_epochs, lr=0.001)\n\n# for the minimal model, we can actually interprete the parameters\n# so I store them here for later\nWp = minimal_model.state_dict()['hidden_layer.weight'].cpu().numpy()\nbp = minimal_model.state_dict()['hidden_layer.bias'].cpu().numpy().reshape(1,-1)\n\nmodel = ComplexModel().to(device)\ncomplex_model, complex_loss = train_model(model, X, labels, n_epochs=n_epochs, lr=0.001) \n\n\nEpoch [9000/9000], Loss: 0.1131\nEpoch [9000/9000], Loss: 0.0078\n\n\n\n\nCode\nfig, axes = plt.subplots(2, 2, figsize=(10, 5))\n\ntitle_str = f'True Boundaries and {ns} data points'\nplot_data_and_boundaries(axes[0][0], X.cpu(), labels.cpu(), W, b, title=title_str)\n\ntitle_str = f'Predicted Boundaries from Minimal Model'\nplot_data_and_boundaries(axes[0][1], X.cpu(), labels.cpu(), Wp, bp, title=title_str)\n\ntitle_str = f\"Minimal Model. Loss = {minimal_loss:.4f}\"\nplot_decision_boundary(axes[1][0], minimal_model, -2, 2, -2, 2, X, labels, title_str)\n\ntitle_str = f\"Complex Model. Loss = {complex_loss:.4f}\"\nplot_decision_boundary(axes[1][1], complex_model, -2, 2, -2, 2, X, labels, title_str)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nTrue Decision Boundaries and Predicted Decision Boundaries for 2 Models\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nfor the simple model (1 hidden layer with 3 neurons) we can still interprete the parameters as the linear boundaries.\ncomplex model (2 hidden layers with 16 + 32 neurons) is difficult to interpret: “black box”\ncomplex model “better”: smaller loss, higher confidence\nthe number of training iterations was quite high for both models (10,000)\nparameterization not unique (scaling factors and order)\n\n\n\nYuval Harari: “intelligence is not about the truth, it’s about the ability to solve problems”"
  },
  {
    "objectID": "lectures/02_NeuralNetworks/00_Perceptron.html#universalities",
    "href": "lectures/02_NeuralNetworks/00_Perceptron.html#universalities",
    "title": "Perceptron",
    "section": "Universalities",
    "text": "Universalities\nA multi-layered perceptron can\n\ncalculate any Boolean function\ncalculate any decision boundary (not just XOR and triangles)\napproximate any continuous function with arbitrary accuracy"
  },
  {
    "objectID": "help/03_BigData.html",
    "href": "help/03_BigData.html",
    "title": "Big Data",
    "section": "",
    "text": "… and common starters for deep learning.\n\n\n\nDataset\nYear\n# Samples\nFeatures per sample\nApprox. Size\nAuthors / Creators\n\n\n\n\n\nMNIST\n~1998‑1999 (released)\n60 000 train + 10 000 test = 70 000 images\n28×28 gray‑scale pixels (784 features)\n~50 MB\nYann LeCun, C. Cortes, C. Burges (via NIST)\n\n\n\nCIFAR‑10\n2009\n50 000 train + 10 000 test = 60 000 images\n32×32 color images (3×32×32 = 3 072 features)\n~170 MB\nAlex Krizhevsky, Vinod Nair, Geoffrey Hinton\n\n\n\nImageNet‑1K\n2009\n1 281 167 train + 50 000 validation + 100 000 test = ~1.43 M images\nHigh‑res RGB images (often resized to 256×256)\n~150 GB\nJia Deng, Li Fei‑Fei, et al. (ImageNet team)"
  },
  {
    "objectID": "help/03_BigData.html#famous-image-datasets",
    "href": "help/03_BigData.html#famous-image-datasets",
    "title": "Big Data",
    "section": "",
    "text": "… and common starters for deep learning.\n\n\n\nDataset\nYear\n# Samples\nFeatures per sample\nApprox. Size\nAuthors / Creators\n\n\n\n\n\nMNIST\n~1998‑1999 (released)\n60 000 train + 10 000 test = 70 000 images\n28×28 gray‑scale pixels (784 features)\n~50 MB\nYann LeCun, C. Cortes, C. Burges (via NIST)\n\n\n\nCIFAR‑10\n2009\n50 000 train + 10 000 test = 60 000 images\n32×32 color images (3×32×32 = 3 072 features)\n~170 MB\nAlex Krizhevsky, Vinod Nair, Geoffrey Hinton\n\n\n\nImageNet‑1K\n2009\n1 281 167 train + 50 000 validation + 100 000 test = ~1.43 M images\nHigh‑res RGB images (often resized to 256×256)\n~150 GB\nJia Deng, Li Fei‑Fei, et al. (ImageNet team)"
  },
  {
    "objectID": "help/03_BigData.html#data-resources",
    "href": "help/03_BigData.html#data-resources",
    "title": "Big Data",
    "section": "Data Resources",
    "text": "Data Resources\nSome popular links - personal bias and far from complete\n\n\n\nRepository / Platform\nCategories Covered\n# of Datasets (approx.)\nTotal Size (approx.)\n\n\n\n\n\nHugging Face Datasets\nNLP, vision, audio, multimodal\nThousands\nTens of TB\n\n\n\nRegistry of Open Data on AWS\nGenomics, earth science, satellite, healthcare\nMany hundreds\nHundreds of TB\n\n\n\nAwesome Public Datasets (GitHub)\nCurated lists across topics—including genomics, medical, climate\nHundreds (topics)\nVariable\n\n\n\nEuropean Genome‑phenome Archive (EGA)\nHuman genomics & phenotypic clinical data\n~4,500 studies from 1,000+ institutions\nTens to hundreds of PB\n\n\n\nThe Cancer Imaging Archive (TCIA)\nMedical imaging (cancer CT, MRI, PET)\nHundreds of collections; millions of images\nTens/hundreds of TB\n\n\n\nMedMNIST v2\nBiomedical images (2D/3D small‑scale)\n~718K images across 18 tasks\nTens of GB"
  },
  {
    "objectID": "help/01_Frameworks.html",
    "href": "help/01_Frameworks.html",
    "title": "Frameworks",
    "section": "",
    "text": "Machine Learning and Deep-Learning have been powered by a number of open frameworks to simplify all modeling steps:\nHere we review 3 popular frameworks: scikit-learn, pytorch, keras/tensorflow (not an exhaustive list)"
  },
  {
    "objectID": "help/01_Frameworks.html#goal-x-to-y-fx",
    "href": "help/01_Frameworks.html#goal-x-to-y-fx",
    "title": "Frameworks",
    "section": "Goal: \\(X \\to Y = f(X)\\)",
    "text": "Goal: \\(X \\to Y = f(X)\\)\nRemember the simple example from the lecture.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.array([-1,  0, 1, 2, 3, 4])     # Data x\ny = np.array([-3, -1, 1, 3, 5, 7])     # Answer y\n\nplt.plot(x,y,'o')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()"
  },
  {
    "objectID": "help/01_Frameworks.html#scikit-learn-2007",
    "href": "help/01_Frameworks.html#scikit-learn-2007",
    "title": "Frameworks",
    "section": "scikit-learn (2007++)",
    "text": "scikit-learn (2007++)\n\nPythonic ML-alternative to R\ndesigned for small datasets (in-memory)\nno GPU support\n\n\n\nCode\nimport sklearn\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nprint('sklearn version:', sklearn.__version__)\n\nlm = linear_model.LinearRegression()  # define model\nxr = x.reshape(-1,1)                  # reshape data for tool (samples as rows) --&gt; x[:, np.newaxis]\nlm.fit(xr, y)                         # fit c.f. R: lm(Y ~ X)\n\n# report fit\nprint(f\"Fitted Parameters {lm.intercept_}, {lm.coef_}\")\n\nyp = lm.predict(xr)                   # prediction\nMSE= mean_squared_error(y, yp)        # evaluate fit. other scores: R2=lm.score(xr, y)\nprint(f\"Mean Squared Error: {MSE}\")\n\n# predict y for some new x\nx_new=np.array([10, -40])\ny_new = lm.predict(x_new.reshape(-1,1))\n\nprint(f\"predictions:  {y_new}\")\n\n\nsklearn version: 1.7.1\nFitted Parameters -1.0, [2.]\nMean Squared Error: 0.0\npredictions:  [ 19. -81.]"
  },
  {
    "objectID": "help/01_Frameworks.html#tensorflowkeras-2015",
    "href": "help/01_Frameworks.html#tensorflowkeras-2015",
    "title": "Frameworks",
    "section": "TensorFlow/Keras (2015)",
    "text": "TensorFlow/Keras (2015)\n\nTensorFlow (Google Brain 2015): very complex\nKeras (F. Chollet 2015): user-friendly frontend to TF & other frameworks\nTF2: merged TF & Keras\nstrength: enterprise solutions & integration with Google Cloud/TPU\n\n\n\nCode\nimport tensorflow as tf\nprint('tf version:',tf.__version__)\n\n# define model - the \"black box\"\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.Input(shape=(1,))) # define input shape\nmodel.add(tf.keras.layers.Dense(units=1))\n\n# define optimization and loss\nmodel.compile(optimizer='sgd', loss='mean_squared_error') \n\n# fit model\nfit_history = model.fit(x,y, epochs=100, verbose=0)       \n\n# report fit\nprint('Fitted Parameters             ', model.trainable_variables)\nprint('Mean Squared Error (loss):    ', fit_history.history['loss'][-1])\n\n# make predictions\nx_new = np.array([ 10.0 , -40.0 ])\ny_new = model.predict(x_new)\ny_ana = -1 + 2.0*np.array(x_new)\n\nprint('analytical: ', y_ana)\nprint('numerical:  ', y_new.flatten())\n\n\n\ntf version: 2.18.0\n\nFitted Parameters              [&lt;Variable path=sequential/dense/kernel, shape=(1, 1), dtype=float32, value=[[1.8115696]]&gt;, &lt;Variable path=sequential/dense/bias, shape=(1,), dtype=float32, value=[-0.41584674]&gt;]\n\nMean Squared Error (loss):     0.19854427874088287\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step\n\nanalytical:  [ 19. -81.]\n\nnumerical:   [ 17.69985 -72.87863]"
  },
  {
    "objectID": "help/01_Frameworks.html#pytorch-facebook-2017",
    "href": "help/01_Frameworks.html#pytorch-facebook-2017",
    "title": "Frameworks",
    "section": "PyTorch (Facebook 2017)",
    "text": "PyTorch (Facebook 2017)\n\nPythonic alternative to Tensorflow 1.x\ndefault choice in AI research and community\nbacked by Linux Foundation\n\n\n\nCode\nimport torch\n#import torch.nn as nn\n#import torch.optim as optim\nprint('torch version:', torch.__version__)\n\n# Convert data to PyTorch tensors \n# define dtype and reshape to column vectors (c.f np.reshape())\nx_tensor = torch.tensor(x, dtype=torch.float32).view(-1, 1)\ny_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n\n# Define Model\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, 1)  # input dim = 1, output dim = 1\n)\n\n# Define Loss function\nloss_function = torch.nn.MSELoss()\n# Choose optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Fit model (explicit loop over epochs)\nloss_history = []\nmodel.train()             # put model in train mode \nfor epoch in range(100):\n\n    outputs = model(x_tensor)               # forward calculation\n    loss = loss_function(outputs, y_tensor) # loss calculation\n    loss_history.append(loss.item())        # append loss for tracking\n\n    # Backward pass\n    optimizer.zero_grad() # set .grad attribute in nn.Parameter to zero\n    loss.backward()       # backpropagation: calculate gradients and store in .grad\n    optimizer.step()      # update parameters: parm = parm - lr*parm.grad\n\n# Report best fit - convert to torch tensor to numpy\nfor name, param in model.named_parameters():\n    print(f\"{name}: {param.data.numpy().flatten()}\")\n\n# report last (=best?) loss from history\nprint(\"Mean Squared Error (loss):\", loss_history[-1])\n\n# make predictions\nx_new = np.array([10.0, -40.0])\nx_new_tensor = torch.tensor(x_new, dtype=torch.float32).view(-1, 1)\ny_new = model(x_new_tensor).detach().numpy()\n\ny_ana = -1 + 2.0 * np.array(x_new)\n\nprint(\"analytical: \", y_ana)\nprint(\"numerical:  \", y_new.flatten())\n\n\ntorch version: 2.6.0\n0.weight: [1.748867]\n0.bias: [-0.22144231]\nMean Squared Error (loss): 0.3526820242404938\nanalytical:  [ 19. -81.]\nnumerical:   [ 17.267227 -70.176125]\n\n\n\n\nCode\nplt.figure()\nplt.plot(fit_history.history['loss'][1:], label=\"TensorFlow\")\nplt.plot(loss_history, label=\"PyTorch\")\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "help/01_Frameworks.html#summary-table",
    "href": "help/01_Frameworks.html#summary-table",
    "title": "Frameworks",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\n\nFeature\nScikit-learn\nTensorFlow (TF)\nPyTorch\n\n\n\n\nInitial Release\n2010\n2015\n2017\n\n\nDeveloper\nINRIA, open-source community\nGoogle Brain\nFacebook AI Research (FAIR)\n\n\nLanguage\nPython (NumPy, SciPy)\nPython + C++ backend\nPython + C++ backend\n\n\nPrimary Use\nClassical ML\nDeep Learning + Production Pipelines\nDeep Learning + R&D\n\n\nModel Types\nTrees, SVMs, Linear Models, etc.\nNeural Networks\nNeural Networks\n\n\nGPU Support\n❌ No\n✅ Yes\n✅ Yes\n\n\nEase of Use\n✅ Very simple API\n⚠️ Steep TF1.x; better in TF2.x\n✅ Pythonic and intuitive\n\n\nCommunity Focus\nData Science / ML practitioners\nCloud, deployment, industry\nResearch, academia, experimental models\n\n\nIntegration\nPandas, NumPy, matplotlib\nTF Hub, TF Lite, Google Cloud\nHuggingFace\n\n\nGovernance\nCommunity-led (INRIA)\nGoogle\nLinux Foundation (since 2022)"
  },
  {
    "objectID": "help/index.html",
    "href": "help/index.html",
    "title": "Help",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nFrameworks\n\n\n\nsklearn\n\npytorch\n\ntensorflow\n\n\n\npowerful tools for neural networks\n\n\n\nThomas Manke\n\n\nOct 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nBig Data\n\n\n\nbig data\n\n\n\nFamous image sets and other Big Data\n\n\n\nThomas Manke\n\n\nOct 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytorch Starter\n\n\na basic intro\n\n\n\nThomas Manke\n\n\nOct 21, 2025\n\n\n\n\n\n\nNo matching items"
  }
]